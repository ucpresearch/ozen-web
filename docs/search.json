[
  {
    "objectID": "embedding/quarto-integration.html",
    "href": "embedding/quarto-integration.html",
    "title": "Quarto Integration",
    "section": "",
    "text": "Quarto and R Markdown documents can embed the Ozen-web viewer using the helper script create-iframe.R. This enables interactive acoustic examples in research papers, teaching materials, and blog posts.",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#overview",
    "href": "embedding/quarto-integration.html#overview",
    "title": "Quarto Integration",
    "section": "",
    "text": "Quarto and R Markdown documents can embed the Ozen-web viewer using the helper script create-iframe.R. This enables interactive acoustic examples in research papers, teaching materials, and blog posts.",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#setup",
    "href": "embedding/quarto-integration.html#setup",
    "title": "Quarto Integration",
    "section": "Setup",
    "text": "Setup\n\n1. Install Dependencies\ninstall.packages(c(\"htmltools\", \"base64enc\"))\n\n\n2. Copy Helper Script\nThe script is located at scripts/create-iframe.R in the Ozen-web repository:\n# Copy to your project\ncp path/to/ozen-web/scripts/create-iframe.R scripts/\nOr download directly:\ncurl -O https://raw.githubusercontent.com/ucpresearch/ozen-web/main/scripts/create-iframe.R",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#basic-usage",
    "href": "embedding/quarto-integration.html#basic-usage",
    "title": "Quarto Integration",
    "section": "Basic Usage",
    "text": "Basic Usage\n\nIn Quarto Document\n---\ntitle: \"Phonetic Analysis Example\"\nformat: html\n---\n\n## Vowel Formants\n\nFigure 1 shows F1 and F2 formants for the vowel /a/:\n\n```{r}\n#| echo: false\nsource(\"scripts/create-iframe.R\")\nhtml &lt;- create_embedded_viewer(\n  \"data/vowel-a.wav\",\n  overlays = \"formants\",\n  height = 600\n)\nhtmltools::HTML(html)\n```\n\n\nIn R Markdown\n---\ntitle: \"Pitch Analysis\"\noutput: html_document\n---\n\n## Intonation Pattern\n\n```{r echo=FALSE}\nsource(\"scripts/create-iframe.R\")\nhtml &lt;- create_embedded_viewer(\n  \"audio/question.wav\",\n  overlays = \"pitch,intensity\"\n)\nhtmltools::HTML(html)\n```",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#function-reference",
    "href": "embedding/quarto-integration.html#function-reference",
    "title": "Quarto Integration",
    "section": "Function Reference",
    "text": "Function Reference\n\ncreate_embedded_viewer()\ncreate_embedded_viewer(\n  audio_path,\n  overlays = NULL,\n  viewer_url = \"./ozen-web/viewer.html\",\n  height = 600\n)\nParameters:\n\naudio_path (string, required) - Path to audio file relative to output HTML\noverlays (string, optional) - Comma-separated overlays: \"pitch,formants,intensity\"\nviewer_url (string, default: \"./ozen-web/viewer.html\") - Path to viewer\nheight (number/string, default: 600) - Height in pixels or percentage (\"80%\")\n\nReturns: HTML string for iframe",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#deployment-strategies",
    "href": "embedding/quarto-integration.html#deployment-strategies",
    "title": "Quarto Integration",
    "section": "Deployment Strategies",
    "text": "Deployment Strategies\n\nStrategy 1: Use Hosted Viewer (Easiest)\nPoint viewer_url to the hosted version:\nhtml &lt;- create_embedded_viewer(\n  \"audio/example.wav\",\n  overlays = \"pitch\",\n  viewer_url = \"https://ucpresearch.github.io/ozen-web/viewer\"\n)\nPros: - No local Ozen-web build needed - Always up-to-date - Smaller project size\nCons: - Requires internet connection - External dependency\n\n\nStrategy 2: Local Viewer Build\nBuild Ozen-web locally and include in project:\n# Build Ozen-web\ncd path/to/Ozen-web\nnpm run build\n\n# Copy build to your project\ncp -r build/ ../my-project/ozen-web/\nIn Quarto document:\nhtml &lt;- create_embedded_viewer(\n  \"audio/example.wav\",\n  overlays = \"formants\",\n  viewer_url = \"./ozen-web/viewer.html\"  # Local build\n)\nPros: - Works offline - No external dependencies - Full control over version\nCons: - Larger project size (~2-3 MB) - Must update manually\n\n\nStrategy 3: Data URL Embedding (Most Portable)\nFor very short audio files, encode as data URL:\nlibrary(base64enc)\n\n# Read and encode audio\naudio_data &lt;- readBin(\"audio/short.wav\", \"raw\", file.info(\"audio/short.wav\")$size)\ndata_url &lt;- paste0(\"data:audio/wav;base64,\", base64encode(audio_data))\n\n# Create iframe with data URL\nhtml &lt;- create_embedded_viewer(\n  data_url,  # Self-contained\n  overlays = \"pitch\",\n  viewer_url = \"https://ucpresearch.github.io/ozen-web/viewer\"\n)\nPros: - Completely self-contained - Single HTML file with everything - No audio file hosting needed\nCons: - Only suitable for short files (&lt;30 seconds) - Increases HTML file size",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#complete-examples",
    "href": "embedding/quarto-integration.html#complete-examples",
    "title": "Quarto Integration",
    "section": "Complete Examples",
    "text": "Complete Examples\n\nExample 1: Tutorial With Multiple Figures\n---\ntitle: \"Phonetic Tutorial\"\nformat: html\n---\n\n```{r setup, include=FALSE}\nsource(\"scripts/create-iframe.R\")\n```\n\n## Vowel Analysis\n\n### High Front Vowel /i/\n\n```{r echo=FALSE}\nhtmltools::HTML(create_embedded_viewer(\n  \"vowels/i.wav\",\n  overlays = \"formants\",\n  height = 500\n))\n```\n\n### Low Back Vowel /ɑ/\n\n```{r echo=FALSE}\nhtmltools::HTML(create_embedded_viewer(\n  \"vowels/a.wav\",\n  overlays = \"formants\",\n  height = 500\n))\n```\n\n\nExample 2: Research Paper Figure\n---\ntitle: \"Tone Production in Mandarin\"\nformat: pdf\n---\n\n## Results\n\nFigure 1 shows pitch contours for Tone 2 (rising):\n\n```{r fig-tone2, echo=FALSE, fig.cap=\"Tone 2 pitch contour\"}\n#| echo: false\nhtmltools::HTML(create_embedded_viewer(\n  \"data/tone2-example.wav\",\n  overlays = \"pitch,intensity\",\n  viewer_url = \"https://ucpresearch.github.io/ozen-web/viewer\",\n  height = 600\n))\n```\n\n\nExample 3: Course Materials\n---\ntitle: \"Week 3: Formant Analysis Lab\"\nformat: html\n---\n\n## Exercise 1: Identify the Vowel\n\nListen to the audio and observe the formant tracks:\n\n```{r echo=FALSE}\nsource(\"scripts/create-iframe.R\")\nhtmltools::HTML(create_embedded_viewer(\n  \"exercises/mystery-vowel-1.wav\",\n  overlays = \"formants\",\n  viewer_url = \"./ozen-web/viewer.html\",\n  height = 700\n))\n```\n\n**Questions:**\n1. What are the approximate F1 and F2 values?\n2. Based on formants, which vowel is this?\n3. Is it front or back? High or low?",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#local-development-workflow",
    "href": "embedding/quarto-integration.html#local-development-workflow",
    "title": "Quarto Integration",
    "section": "Local Development Workflow",
    "text": "Local Development Workflow\n\nDirectory Structure\nmy-quarto-project/\n├── _quarto.yml\n├── index.html\n├── analysis.html\n├── scripts/\n│   └── create-iframe.R        # Helper script\n├── audio/                      # Audio files\n│   ├── example1.wav\n│   └── example2.wav\n└── ozen-web/                   # Local viewer build (optional)\n    ├── viewer.html\n    ├── _app/\n    └── ...\n\n\nPreview Locally\nquarto preview\nThis starts a local server (typically http://localhost:XXXX) where iframes will work correctly.\n\n\n\n\n\n\nWarningFile:// Protocol Doesn’t Work\n\n\n\nOpening the rendered HTML directly (double-clicking) won’t load iframes due to browser security. Always use quarto preview or a local HTTP server.\n\n\n\n\nBuild for Deployment\nquarto render\nOutput goes to _site/ directory, ready for hosting.",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#github-pages-deployment",
    "href": "embedding/quarto-integration.html#github-pages-deployment",
    "title": "Quarto Integration",
    "section": "GitHub Pages Deployment",
    "text": "GitHub Pages Deployment\n\n1. Configure Quarto\nIn _quarto.yml:\nproject:\n  type: website\n  output-dir: _site\n\nwebsite:\n  title: \"My Research\"\n  navbar:\n    left:\n      - index.html\n      - analysis.html\n\n\n2. Include Audio Files\nEnsure audio files are in the project and will be copied to output:\nresources:\n  - \"audio/*.wav\"\n  - \"ozen-web/**\"  # If using local build\n\n\n3. Deploy to GitHub Pages\nOption A: GitHub Actions (Recommended)\nCreate .github/workflows/quarto-publish.yml:\non:\n  push:\n    branches: main\n\nname: Render and Publish\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Render Quarto Project\n        uses: quarto-dev/quarto-actions/render@v2\n\n      - name: Publish to GitHub Pages\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\nOption B: Manual Publish\nquarto publish gh-pages\n\n\n4. Enable GitHub Pages\n\nGo to repository Settings → Pages\nSource: gh-pages branch\nVisit https://username.github.io/repo-name/",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#responsive-design",
    "href": "embedding/quarto-integration.html#responsive-design",
    "title": "Quarto Integration",
    "section": "Responsive Design",
    "text": "Responsive Design\nMake viewers adapt to screen size:\n# Use percentage height\nhtml &lt;- create_embedded_viewer(\n  \"audio/example.wav\",\n  height = \"80%\"  # Percentage of container\n)\nOr wrap in responsive container:\n```{r echo=FALSE}\ncat('\n&lt;div style=\"position: relative; width: 100%; padding-bottom: 56.25%;\"&gt;\n')\nhtmltools::HTML(create_embedded_viewer(\n  \"audio/example.wav\",\n  overlays = \"pitch\",\n  viewer_url = \"https://ucpresearch.github.io/ozen-web/viewer\"\n))\ncat('&lt;/div&gt;')\n```",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#troubleshooting",
    "href": "embedding/quarto-integration.html#troubleshooting",
    "title": "Quarto Integration",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\niframes Don’t Load\nProblem: Blank or broken iframes\nSolution: - Use quarto preview (not file://) - Check audio paths are relative to output HTML - Verify viewer URL is correct\n\n\nAudio Files Not Found\nProblem: 404 errors for audio files\nSolution: - Add audio files to resources: in _quarto.yml - Check relative paths (relative to rendered HTML location) - Test paths: ls _site/audio/\n\n\nScript Not Found\nProblem: source(\"scripts/create-iframe.R\") fails\nSolution: - Check script path relative to .html file - Use absolute path if needed: source(here::here(\"scripts/create-iframe.R\")) - Ensure script is in repository",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/quarto-integration.html#see-also",
    "href": "embedding/quarto-integration.html#see-also",
    "title": "Quarto Integration",
    "section": "See Also",
    "text": "See Also\n\nBasic Embedding - iframe fundamentals\nURL Parameters - Customization options\nExamples - Real-world examples\nQuarto Documentation - Official Quarto docs",
    "crumbs": [
      "Embedding",
      "Quarto Integration"
    ]
  },
  {
    "objectID": "embedding/overview.html",
    "href": "embedding/overview.html",
    "title": "Embedding Guide",
    "section": "",
    "text": "Ozen-web’s mobile viewer can be embedded in websites, Quarto documents, R Markdown, Jupyter notebooks, and any web page via iframes. This allows creating interactive acoustic visualizations in:\n\nResearch papers and presentations\nCourse materials and textbooks\nBlog posts and tutorials\nData analysis notebooks",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#introduction",
    "href": "embedding/overview.html#introduction",
    "title": "Embedding Guide",
    "section": "",
    "text": "Ozen-web’s mobile viewer can be embedded in websites, Quarto documents, R Markdown, Jupyter notebooks, and any web page via iframes. This allows creating interactive acoustic visualizations in:\n\nResearch papers and presentations\nCourse materials and textbooks\nBlog posts and tutorials\nData analysis notebooks",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#quick-start",
    "href": "embedding/overview.html#quick-start",
    "title": "Embedding Guide",
    "section": "Quick Start",
    "text": "Quick Start\nBasic embedding:\n&lt;iframe\n  data-external=\"1\"\n  src=\"./ozen-web/viewer.html?audio=audio.wav&overlays=pitch,formants\"\n  width=\"100%\"\n  height=\"600\"\n  frameborder=\"0\"&gt;\n&lt;/iframe&gt;\nKey points:\n\ndata-external=\"1\" is required for Quarto with embed-resources: true\n?audio= parameter specifies audio file path\n?overlays= parameter configures which features to display\nViewer works from any subdirectory\n\nSee: URL Parameters Reference for all available options",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#directory-structure",
    "href": "embedding/overview.html#directory-structure",
    "title": "Embedding Guide",
    "section": "Directory Structure",
    "text": "Directory Structure\nRecommended setup for Quarto/R Markdown projects:\nmy-project/\n├── document.html                # Your Quarto document\n├── document.html               # Rendered output\n├── audio/                      # Your audio files\n│   ├── sample1.wav\n│   └── sample2.wav\n├── ozen-web/                   # Copy of build/ directory\n│   ├── viewer.html\n│   ├── _app/\n│   ├── wasm/\n│   └── ... (all build contents)\n└── scripts/\n    ├── create-iframe.R         # Helper script (R)\n    └── create-iframe.py        # Helper script (Python)",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#helper-scripts",
    "href": "embedding/overview.html#helper-scripts",
    "title": "Embedding Guide",
    "section": "Helper Scripts",
    "text": "Helper Scripts\nUse helper scripts for automatic path calculation:\n\nPython\nCommand line:\npython scripts/create-iframe.py audio/sample.wav\npython scripts/create-iframe.py audio/sample.wav --overlays pitch,formants,hnr\npython scripts/create-iframe.py audio/sample.wav --height 800\nIn Jupyter/Quarto:\nimport sys\nsys.path.append('scripts')\nfrom create_iframe import create_embedded_viewer\n\n# Generate iframe HTML\nhtml = create_embedded_viewer(\"audio/sample.wav\",\n                              overlays=\"pitch,formants,hnr\",\n                              height=600)\nprint(html)\n\n\nR\nCommand line:\nRscript scripts/create-iframe.R audio/sample.wav\nRscript scripts/create-iframe.R audio/sample.wav \"pitch,formants,hnr\"\nRscript scripts/create-iframe.R audio/sample.wav \"pitch,formants\" \"./ozen-web/viewer.html\" 800\nIn R Markdown/Quarto:\nsource(\"scripts/create-iframe.R\")\n\n# Generate iframe HTML\nhtml &lt;- create_embedded_viewer(\"audio/sample.wav\",\n                               overlays = \"pitch,formants,hnr\",\n                               height = 600)\nhtmltools::HTML(html)\nSee: Helper Scripts Documentation",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#use-cases",
    "href": "embedding/overview.html#use-cases",
    "title": "Embedding Guide",
    "section": "Use Cases",
    "text": "Use Cases\n\nResearch Papers (Quarto)\nEmbed interactive spectrograms in academic papers:\n---\ntitle: \"Vowel Duration in Stress-Timed Languages\"\nformat:\n  html:\n    embed-resources: true\n---\n\n## Results\n\nFigure 1 shows the acoustic properties of the target vowel:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"../scripts/create-iframe.R\")\nhtml &lt;- create_embedded_viewer(\"data/vowel-example.wav\",\n                               overlays = \"pitch,formants,intensity\")\nhtmltools::HTML(html)\n```\n:::\n\n\nReaders can zoom, play, and explore the audio interactively.\n\n\nCourse Materials\nCreate interactive phonetics exercises:\n&lt;h3&gt;Exercise 1: Identify the vowel&lt;/h3&gt;\n&lt;p&gt;Listen to the audio and examine F1 and F2:&lt;/p&gt;\n\n&lt;iframe\n  data-external=\"1\"\n  src=\"./ozen-web/viewer.html?audio=exercises/vowel1.wav&overlays=formants\"\n  width=\"100%\"\n  height=\"500\"&gt;\n&lt;/iframe&gt;\n\n&lt;details&gt;\n&lt;summary&gt;Answer&lt;/summary&gt;\nThe vowel is [i] (high front), with F1≈300 Hz and F2≈2300 Hz.\n&lt;/details&gt;\n\n\nBlog Posts\nShare audio examples with readers:\n&lt;p&gt;Here's what a rising intonation pattern looks like:&lt;/p&gt;\n\n&lt;iframe\n  data-external=\"1\"\n  src=\"https://yoursite.com/ozen-web/viewer.html?audio=https://cdn.example.com/question.wav&overlays=pitch\"\n  width=\"100%\"\n  height=\"600\"&gt;\n&lt;/iframe&gt;\n\n\nJupyter Notebooks\nAnalyze data with embedded visualizations:\nfrom IPython.display import IFrame\n\n# Display embedded viewer\nIFrame(src='./ozen-web/viewer.html?audio=data/recording.wav&overlays=all',\n       width='100%',\n       height=600)\nSee: Examples Page for more use cases",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#deployment",
    "href": "embedding/overview.html#deployment",
    "title": "Embedding Guide",
    "section": "Deployment",
    "text": "Deployment\n\nGitHub Pages\n\nBuild Ozen-web: npm run build\nCopy build/ to your project as ozen-web/\nPush to GitHub\nEnable GitHub Pages in repository settings\nYour documents with embedded viewers are live!\n\n\n\nNetlify/Vercel\nSame as GitHub Pages — just include the ozen-web/ directory in your project.\n\n\nStatic Site Generators\nOzen-web works with all static site generators:\n\nHugo\nJekyll\nGatsby\nEleventy\nDocusaurus\n\nJust include the ozen-web/ directory in your static files.\nSee: Getting Started: Deployment",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#important-considerations",
    "href": "embedding/overview.html#important-considerations",
    "title": "Embedding Guide",
    "section": "Important Considerations",
    "text": "Important Considerations\n\nQuarto embed-resources: true\nCRITICAL: When using Quarto with embed-resources: true, you must include data-external=\"1\" in all iframe tags:\n&lt;iframe\n  data-external=\"1\"    &lt;!-- REQUIRED --&gt;\n  src=\"./ozen-web/viewer.html?audio=audio.wav\"&gt;\n&lt;/iframe&gt;\nWhy? Quarto’s embed-resources: true converts iframe sources to data URLs. However, the viewer uses ES6 module imports which cannot resolve in data URL contexts (browser limitation).\nWithout data-external=\"1\", you’ll see console errors:\nFailed to resolve module specifier './_app/immutable/entry/start.js'\nThe helper scripts automatically include this attribute.\n\n\nFile Protocol Restrictions\nBrowsers block file:// URLs from loading iframes for security. When you open a rendered HTML file directly (double-click), iframes fail with:\nNot allowed to load local resource: file:///...\nSolution: Serve over HTTP:\n# Python\npython -m http.server 8000\n\n# R\nservr::httd(port = 8000)\n\n# Then open: http://localhost:8000/document.html\nOr deploy to a web server where HTTP is automatic.\nSee: Basic Usage: Serving Documents\n\n\nCORS for Remote Audio\nRemote audio URLs must send Access-Control-Allow-Origin header.\nApache (.htaccess):\n&lt;FilesMatch \"\\.(wav|mp3|ogg|flac)$\"&gt;\n  Header set Access-Control-Allow-Origin \"*\"\n&lt;/FilesMatch&gt;\nNginx:\nlocation ~* \\.(wav|mp3|ogg|flac)$ {\n    add_header Access-Control-Allow-Origin *;\n}\nSee: URL Parameters: CORS",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#next-steps",
    "href": "embedding/overview.html#next-steps",
    "title": "Embedding Guide",
    "section": "Next Steps",
    "text": "Next Steps\n\nBasic Usage — Detailed embedding instructions\nQuarto Integration — Quarto-specific guide\nURL Parameters — Complete parameter reference\nExamples — Working examples for all use cases",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/overview.html#faq",
    "href": "embedding/overview.html#faq",
    "title": "Embedding Guide",
    "section": "FAQ",
    "text": "FAQ\nQ: Can I embed the main app (not just viewer)?\nA: The main app works in iframes but is designed for desktop use. The /viewer route is optimized for embedding and mobile.\nQ: Does the embedded viewer support annotations?\nA: The embedded viewer is view-only. For editing, link to the full app or deploy both versions.\nQ: Can I customize the appearance?\nA: Yes, via config.yaml file in the ozen-web/ directory. See Configuration Reference.\nQ: How large can embedded audio files be?\nA: Recommended &lt;100MB for browser memory limits. The viewer handles long files via on-demand analysis.\nQ: Can I embed on Medium, WordPress, etc.?\nA: Yes, if the platform allows custom HTML/iframes. Some platforms block iframes for security.",
    "crumbs": [
      "Embedding",
      "Embedding Guide"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html",
    "href": "embedding/url-parameters.html",
    "title": "URL Parameters",
    "section": "",
    "text": "The Ozen-web viewer supports URL parameters for pre-loading audio and configuring the interface. This enables creation of shareable links, embedded examples, and pre-configured analysis views.",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#overview",
    "href": "embedding/url-parameters.html#overview",
    "title": "URL Parameters",
    "section": "",
    "text": "The Ozen-web viewer supports URL parameters for pre-loading audio and configuring the interface. This enables creation of shareable links, embedded examples, and pre-configured analysis views.",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#base-url",
    "href": "embedding/url-parameters.html#base-url",
    "title": "URL Parameters",
    "section": "Base URL",
    "text": "Base URL\nhttps://ucpresearch.github.io/ozen-web/viewer\nor for local deployment:\nhttp://localhost:4173/viewer",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#parameter-reference",
    "href": "embedding/url-parameters.html#parameter-reference",
    "title": "URL Parameters",
    "section": "Parameter Reference",
    "text": "Parameter Reference\n\naudio\nType: String (URL or data URL) Required: No Default: Empty (shows file drop zone)\nLoad audio file automatically from URL.\nExamples:\n# Remote HTTPS URL\n?audio=https://example.com/recording.wav\n\n# GitHub raw content\n?audio=https://raw.githubusercontent.com/user/repo/main/audio.wav\n\n# Data URL (Base64-encoded)\n?audio=data:audio/wav;base64,UklGRiQAAABXQVZFZm10...\n\n# Multiple parameters\n?audio=https://example.com/audio.wav&overlays=pitch\nSupported formats: - WAV (recommended, no compression artifacts) - MP3 (smaller file size) - OGG (open format)\nRequirements: - Must be HTTPS (or data URL) - Server must enable CORS headers - File must be publicly accessible\n\n\n\n\n\n\nWarningCORS Required\n\n\n\nThe audio server must send Access-Control-Allow-Origin: * header. See Basic Embedding for configuration details.\n\n\n\n\noverlays\nType: Comma-separated string Required: No Default: None (all overlays disabled)\nPre-enable acoustic overlays.\nValid values:\n\n\n\nValue\nOverlay\nDescription\n\n\n\n\npitch\nPitch (F0)\nFundamental frequency track (blue)\n\n\nformants\nFormants\nF1-F4 formant frequencies (red dots)\n\n\nintensity\nIntensity\nSound pressure level (green)\n\n\nhnr\nHNR\nHarmonics-to-noise ratio (cyan)\n\n\ncog\nCoG\nCenter of gravity (orange)\n\n\nspectral_tilt\nSpectral Tilt\nSpectral slope (purple)\n\n\na1_p0\nA1-P0\nNasal measure (pink)\n\n\n\nExamples:\n# Single overlay\n?overlays=pitch\n\n# Multiple overlays (comma-separated, no spaces)\n?overlays=pitch,formants,intensity\n\n# All acoustic overlays\n?overlays=pitch,formants,intensity,hnr,cog,spectral_tilt,a1_p0\n\n# With audio\n?audio=https://example.com/audio.wav&overlays=formants\nCase sensitivity: Values are case-insensitive (pitch = Pitch = PITCH)\nInvalid values: Silently ignored (viewer loads without that overlay)\n\n\nmaxFreq\nType: Number Required: No Default: 5000 Valid values: 5000, 7500, 10000\nSet spectrogram maximum frequency (Hz).\nExamples:\n# 5 kHz (default, suitable for male speech)\n?maxFreq=5000\n\n# 7.5 kHz (suitable for female/child speech)\n?maxFreq=7500\n\n# 10 kHz (maximum detail for fricatives, bird calls)\n?maxFreq=10000\n\n# Combined with overlays\n?audio=...&overlays=formants&maxFreq=7500\nUse cases: - 5000 — Adult male speech (default) - 7500 — Adult female, children’s voices - 10000 — High-frequency analysis (fricatives, sibilants)\n\n\nbackend\nType: String Required: No Default: praatfan-local (if available), else praatfan\nSelect WASM analysis backend.\nValid values:\n\n\n\n\n\n\n\n\n\nValue\nSource\nLicense\nDescription\n\n\n\n\npraatfan-local\nLocal static/wasm/praatfan/\nMIT/Apache-2.0\nFastest (no download)\n\n\npraatfan\nGitHub Pages CDN\nMIT/Apache-2.0\nNo local setup required\n\n\npraatfan-gpl\nGitHub Pages CDN\nGPL\nGPL-licensed version\n\n\n\nExamples:\n# Use local backend (fastest)\n?backend=praatfan-local\n\n# Use CDN backend (no setup)\n?backend=praatfan\n\n# Use GPL backend\n?backend=praatfan-gpl\nNotes: - praatfan-local only works if WASM files are in static/wasm/praatfan/ - CDN backends download ~5 MB on first load - All backends produce identical results\nSee Backends Reference for details.",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#url-encoding",
    "href": "embedding/url-parameters.html#url-encoding",
    "title": "URL Parameters",
    "section": "URL Encoding",
    "text": "URL Encoding\nURL parameters must be properly encoded:\n\nSpecial Characters\n\n\n\nCharacter\nEncoded\nExample\n\n\n\n\nSpace\n%20 or +\nmy%20audio.wav\n\n\nComma\n%2C\npitch%2Cformants\n\n\nColon\n%3A\ndata%3Aaudio/wav\n\n\nSlash\n%2F\npath%2Fto%2Faudio.wav\n\n\n\nNote: Commas in overlay lists should NOT be encoded:\n✅ Correct:   ?overlays=pitch,formants\n❌ Incorrect: ?overlays=pitch%2Cformants\n\n\nJavaScript Encoding\nconst params = new URLSearchParams({\n  audio: 'https://example.com/my audio.wav',\n  overlays: 'pitch,formants'\n});\n\nconst url = `https://ucpresearch.github.io/ozen-web/viewer?${params}`;\n// Result: ...viewer?audio=https%3A%2F%2Fexample.com%2Fmy+audio.wav&overlays=pitch%2Cformants\n\n\nR Encoding\nlibrary(urltools)\n\nparams &lt;- list(\n  audio = \"https://example.com/my audio.wav\",\n  overlays = \"pitch,formants\"\n)\n\nurl &lt;- paste0(\n  \"https://ucpresearch.github.io/ozen-web/viewer?\",\n  paste(names(params), sapply(params, url_encode), sep = \"=\", collapse = \"&\")\n)\n\n\nPython Encoding\nfrom urllib.parse import urlencode, quote\n\nparams = {\n    'audio': 'https://example.com/my audio.wav',\n    'overlays': 'pitch,formants'\n}\n\nurl = f\"https://ucpresearch.github.io/ozen-web/viewer?{urlencode(params, quote_via=quote)}\"",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#data-url-format",
    "href": "embedding/url-parameters.html#data-url-format",
    "title": "URL Parameters",
    "section": "Data URL Format",
    "text": "Data URL Format\nFor self-contained embeds, encode audio as Base64 data URL:\n\nStructure\ndata:[&lt;mediatype&gt;][;base64],&lt;data&gt;\nFor audio files:\ndata:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQAAAAA=\n\n\nComponents\n\ndata: — Data URL prefix\naudio/wav — MIME type\n;base64 — Encoding indicator\n,&lt;data&gt; — Base64-encoded audio data\n\n\n\nMIME Types\n\n\n\nFormat\nMIME Type\n\n\n\n\nWAV\naudio/wav or audio/x-wav\n\n\nMP3\naudio/mpeg\n\n\nOGG\naudio/ogg\n\n\n\n\n\nGenerating Data URLs\nCommand line (Linux/Mac):\necho -n \"data:audio/wav;base64,\" && base64 -w 0 audio.wav\nNode.js:\nconst fs = require('fs');\nconst data = fs.readFileSync('audio.wav');\nconst dataURL = `data:audio/wav;base64,${data.toString('base64')}`;\nR:\nlibrary(base64enc)\naudio_data &lt;- readBin(\"audio.wav\", \"raw\", file.info(\"audio.wav\")$size)\ndata_url &lt;- paste0(\"data:audio/wav;base64,\", base64encode(audio_data))\nPython:\nimport base64\n\nwith open('audio.wav', 'rb') as f:\n    audio_data = f.read()\n    data_url = f\"data:audio/wav;base64,{base64.b64encode(audio_data).decode()}\"",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#complete-examples",
    "href": "embedding/url-parameters.html#complete-examples",
    "title": "URL Parameters",
    "section": "Complete Examples",
    "text": "Complete Examples\n\nBasic Audio Playback\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/speech.wav\n\n\nPitch Analysis\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/speech.wav&overlays=pitch,intensity\n\n\nVowel Formant Analysis\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/vowel.wav&overlays=formants&maxFreq=7500\n\n\nVoice Quality Analysis\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/voice.wav&overlays=pitch,hnr,spectral_tilt\n\n\nFricative Analysis\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/sibilant.wav&overlays=cog&maxFreq=10000\n\n\nSelf-Contained Example (Data URL)\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=data:audio/wav;base64,UklGRiQAAABXQVZF...&overlays=pitch",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#building-urls-programmatically",
    "href": "embedding/url-parameters.html#building-urls-programmatically",
    "title": "URL Parameters",
    "section": "Building URLs Programmatically",
    "text": "Building URLs Programmatically\n\nJavaScript Helper Function\nfunction buildViewerURL(audioURL, options = {}) {\n  const baseURL = 'https://ucpresearch.github.io/ozen-web/viewer';\n  const params = new URLSearchParams();\n\n  if (audioURL) params.append('audio', audioURL);\n  if (options.overlays) params.append('overlays', options.overlays);\n  if (options.maxFreq) params.append('maxFreq', options.maxFreq);\n  if (options.backend) params.append('backend', options.backend);\n\n  return `${baseURL}?${params}`;\n}\n\n// Usage\nconst url = buildViewerURL('https://example.com/audio.wav', {\n  overlays: 'pitch,formants',\n  maxFreq: 7500\n});\n\n\nR Helper Function\nbuild_viewer_url &lt;- function(audio_url, overlays = NULL, max_freq = NULL, backend = NULL) {\n  base_url &lt;- \"https://ucpresearch.github.io/ozen-web/viewer\"\n\n  params &lt;- list()\n  if (!is.null(audio_url)) params$audio &lt;- audio_url\n  if (!is.null(overlays)) params$overlays &lt;- overlays\n  if (!is.null(max_freq)) params$maxFreq &lt;- max_freq\n  if (!is.null(backend)) params$backend &lt;- backend\n\n  if (length(params) &gt; 0) {\n    query &lt;- paste(names(params), unlist(params), sep = \"=\", collapse = \"&\")\n    paste0(base_url, \"?\", query)\n  } else {\n    base_url\n  }\n}\n\n# Usage\nurl &lt;- build_viewer_url(\n  \"https://example.com/audio.wav\",\n  overlays = \"pitch,formants\",\n  max_freq = 7500\n)\n\n\nPython Helper Function\nfrom urllib.parse import urlencode\n\ndef build_viewer_url(audio_url, overlays=None, max_freq=None, backend=None):\n    base_url = 'https://ucpresearch.github.io/ozen-web/viewer'\n\n    params = {}\n    if audio_url:\n        params['audio'] = audio_url\n    if overlays:\n        params['overlays'] = overlays\n    if max_freq:\n        params['maxFreq'] = max_freq\n    if backend:\n        params['backend'] = backend\n\n    if params:\n        return f\"{base_url}?{urlencode(params)}\"\n    return base_url\n\n# Usage\nurl = build_viewer_url(\n    'https://example.com/audio.wav',\n    overlays='pitch,formants',\n    max_freq=7500\n)",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#troubleshooting",
    "href": "embedding/url-parameters.html#troubleshooting",
    "title": "URL Parameters",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nParameter Not Working\nProblem: URL parameter ignored by viewer\nPossible causes: - Typo in parameter name (overlay instead of overlays) - Invalid parameter value - URL encoding issues\nSolution: - Check parameter names (case-insensitive but must be exact) - Verify values against reference above - Test URL in browser address bar - Check browser console for errors\n\n\nAudio Won’t Load\nProblem: Audio parameter present but file doesn’t load\nCauses: - CORS not enabled - Invalid audio URL - Mixed content (HTTP audio on HTTPS viewer) - Data URL too long\nSolution: - Test audio URL directly in browser - Check server CORS headers - Use HTTPS for both viewer and audio - Keep data URLs under 2 MB\n\n\nOverlays Not Appearing\nProblem: Overlays parameter set but no visualization\nCauses: - Invalid overlay name - URL encoding broke comma separation - WASM not loaded yet\nSolution: - Check overlay names spelling - Don’t encode commas in overlay list - Wait for WASM initialization (status indicator)",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "embedding/url-parameters.html#see-also",
    "href": "embedding/url-parameters.html#see-also",
    "title": "URL Parameters",
    "section": "See Also",
    "text": "See Also\n\nBasic Embedding - iframe integration guide\nExamples - Real-world URL examples\nBackends Reference - WASM backend details\nConfiguration - Advanced settings",
    "crumbs": [
      "Embedding",
      "URL Parameters"
    ]
  },
  {
    "objectID": "reference/backends.html",
    "href": "reference/backends.html",
    "title": "Analysis Backends",
    "section": "",
    "text": "Ozen-web supports multiple WebAssembly (WASM) backends for acoustic analysis, all based on the praatfan library. Different backends offer trade-offs between licensing, deployment complexity, and load time.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#overview",
    "href": "reference/backends.html#overview",
    "title": "Analysis Backends",
    "section": "",
    "text": "Ozen-web supports multiple WebAssembly (WASM) backends for acoustic analysis, all based on the praatfan library. Different backends offer trade-offs between licensing, deployment complexity, and load time.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#available-backends",
    "href": "reference/backends.html#available-backends",
    "title": "Analysis Backends",
    "section": "Available Backends",
    "text": "Available Backends\n\npraatfan-local\nSource: Local static/wasm/praatfan/ directory License: MIT OR Apache-2.0 (dual-licensed) Size: ~5 MB Load time: Instant (no download)\nDescription:\nThe local backend uses WASM files bundled with your deployment. This is the fastest option since files are served from the same origin as the app.\nSetup:\n# Copy WASM package to static directory\nmkdir -p static/wasm/praatfan\ncp -r path/to/praatfan-core-rs/rust/pkg/* static/wasm/praatfan/\nAdvantages: - Fastest load time (no CDN download) - Works offline - No external dependencies - Full control over version\nDisadvantages: - Requires manual setup - Adds ~5 MB to deployment size - Must update manually for bug fixes\nUse when: - Deploying your own instance - Offline use required - Optimal performance needed\n\n\npraatfan\nSource: GitHub Pages CDN (https://ucpresearch.github.io/praatfan/) License: MIT OR Apache-2.0 (dual-licensed) Size: ~5 MB download Load time: 1-3 seconds (depends on connection)\nDescription:\nThe CDN backend downloads WASM files from GitHub Pages on first load. This is the default for the hosted viewer.\nSetup:\nNo setup required — works out of the box.\nAdvantages: - No local setup needed - Automatically gets updates - Smaller deployment (no WASM in bundle) - Same permissive license as local version\nDisadvantages: - Requires internet connection - Initial load delay (~2 seconds) - External dependency\nUse when: - Using hosted viewer at ucpresearch.github.io - No local WASM setup available - Deployment size needs to be minimal\n\n\npraatfan-gpl\nSource: GitHub Pages CDN License: GPL (copyleft) Size: ~5 MB download Load time: 1-3 seconds\nDescription:\nGPL-licensed version of praatfan with additional features that require GPL dependencies.\nImportant licensing note:\n\n\n\n\n\n\nWarningGPL Licensing Implications\n\n\n\nThe GPL license is “copyleft” — if you distribute modified versions or link to GPL code, your code must also be GPL-licensed. For most users embedding Ozen-web in websites or documents, this doesn’t apply. Consult legal counsel if unsure.\n\n\nAdvantages: - Same as praatfan CDN backend - Access to GPL-only features (if any)\nDisadvantages: - GPL licensing restrictions - Same external dependency as praatfan\nUse when: - You specifically need GPL features - Your project is already GPL-compatible",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#backend-comparison",
    "href": "reference/backends.html#backend-comparison",
    "title": "Analysis Backends",
    "section": "Backend Comparison",
    "text": "Backend Comparison\n\n\n\nFeature\npraatfan-local\npraatfan (CDN)\npraatfan-gpl\n\n\n\n\nLicense\nMIT/Apache-2.0\nMIT/Apache-2.0\nGPL\n\n\nSetup required\nYes\nNo\nNo\n\n\nLoad time\nInstant\n1-3s\n1-3s\n\n\nOffline use\n✅ Yes\n❌ No\n❌ No\n\n\nDeployment size\n+5 MB\nMinimal\nMinimal\n\n\nUpdates\nManual\nAutomatic\nAutomatic\n\n\nCommercial use\n✅ Yes\n✅ Yes\n⚠️ GPL terms",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#selecting-a-backend",
    "href": "reference/backends.html#selecting-a-backend",
    "title": "Analysis Backends",
    "section": "Selecting a Backend",
    "text": "Selecting a Backend\n\nVia UI\nIn the main application:\n\nClick backend selector dropdown\nChoose: praatfan-local, praatfan, or praatfan-gpl\nWait for WASM to initialize\n\nThe selection is saved to browser localStorage.\n\n\nVia URL Parameter\nWhen embedding the viewer:\n?backend=praatfan-local\n?backend=praatfan\n?backend=praatfan-gpl\nExample:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=...&backend=praatfan-local\"&gt;\n&lt;/iframe&gt;\n\n\nVia Configuration File\nIn static/config.yaml:\nbackend: praatfan-local  # or praatfan, praatfan-gpl\n\n\nProgrammatic Selection\nIn code (for custom deployments):\nimport { initWasm } from '$lib/wasm/acoustic';\n\n// Initialize specific backend\nawait initWasm('praatfan-local');",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#algorithm-compatibility",
    "href": "reference/backends.html#algorithm-compatibility",
    "title": "Analysis Backends",
    "section": "Algorithm Compatibility",
    "text": "Algorithm Compatibility\nAll backends produce identical results for standard acoustic analyses:\n\nPitch (F0)\nFormants (F1-F4) with bandwidths\nIntensity\nHNR (Harmonics-to-Noise Ratio)\nCenter of Gravity (CoG)\nSpectral Tilt\nA1-P0\nSpectrograms\n\nThe backends differ only in licensing and delivery method, not in computational accuracy.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#performance-characteristics",
    "href": "reference/backends.html#performance-characteristics",
    "title": "Analysis Backends",
    "section": "Performance Characteristics",
    "text": "Performance Characteristics\n\nInitialization Time\n\n\n\nBackend\nFirst Load\nSubsequent Loads\n\n\n\n\npraatfan-local\n~200 ms\n~200 ms\n\n\npraatfan (CDN)\n~2000 ms\n~200 ms*\n\n\npraatfan-gpl\n~2000 ms\n~200 ms*\n\n\n\n*If browser-cached\n\n\nAnalysis Speed\nAll backends have identical analysis performance once loaded:\n\n\n\nAnalysis\n30s Audio\nNotes\n\n\n\n\nPitch\n~100 ms\n\n\n\nFormants\n~200 ms\n\n\n\nIntensity\n~80 ms\n\n\n\nSpectrogram\n~300 ms\n5 kHz max freq\n\n\n\nPerformance depends on device CPU, not backend choice.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#setup-guide-local-backend",
    "href": "reference/backends.html#setup-guide-local-backend",
    "title": "Analysis Backends",
    "section": "Setup Guide: Local Backend",
    "text": "Setup Guide: Local Backend\n\nPrerequisites\n\nAccess to praatfan-core-rs repository\nOR pre-built WASM package\n\n\n\nStep 1: Obtain WASM Files\nOption A: Build from source\ncd praatfan-core-rs/rust\nwasm-pack build --target web\n# Output in pkg/ directory\nOption B: Download release\n# Download from GitHub releases\nwget https://github.com/ucpresearch/praatfan-core-rs/releases/latest/download/praatfan-wasm.tar.gz\ntar -xzf praatfan-wasm.tar.gz\n\n\nStep 2: Copy to Static Directory\n# In Ozen-web project root\nmkdir -p static/wasm/praatfan\ncp -r path/to/pkg/* static/wasm/praatfan/\n\n\nStep 3: Verify Files\nls static/wasm/praatfan/\n# Should contain:\n# - praatfan_bg.wasm\n# - praatfan.js\n# - package.json\n\n\nStep 4: Configure Default Backend\nIn static/config.yaml:\nbackend: praatfan-local\n\n\nStep 5: Test\nnpm run dev\n# Open http://localhost:5173\n# Check backend selector shows \"praatfan-local\"\n# Load audio and verify analysis works",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#troubleshooting",
    "href": "reference/backends.html#troubleshooting",
    "title": "Analysis Backends",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\n“Failed to initialize WASM”\nProblem: Backend won’t load\nPossible causes: - praatfan-local: Files missing from static/wasm/praatfan/ - praatfan/praatfan-gpl: Network connection issue - All: Browser doesn’t support WASM\nSolution: - Check browser console for specific error - Verify files exist: ls static/wasm/praatfan/ - Test network: curl https://ucpresearch.github.io/praatfan/praatfan_bg.wasm - Update browser (WASM support required)\n\n\nSlow Initial Load\nProblem: App takes 5+ seconds to initialize\nLikely cause: CDN backend with slow connection\nSolution: - Use praatfan-local for faster load - OR accept initial delay (subsequent loads are fast) - Check network speed\n\n\nCORS Errors with Local Files\nProblem: “CORS policy blocked” when using praatfan-local\nCause: Serving app via file:// protocol\nSolution: - Use HTTP server (not file://) - npm run dev or npm run preview",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#migration-guide",
    "href": "reference/backends.html#migration-guide",
    "title": "Analysis Backends",
    "section": "Migration Guide",
    "text": "Migration Guide\n\nSwitching from CDN to Local\n\nSet up local backend (see setup guide above)\nUpdate config: backend: praatfan-local\nDeploy updated app\nUsers will use local files automatically\n\n\n\nSwitching from Local to CDN\n\nUpdate config: backend: praatfan\nDeploy\nRemove static/wasm/praatfan/ to save space (optional)\n\nNo data migration needed — backends are fully compatible.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#licensing-summary",
    "href": "reference/backends.html#licensing-summary",
    "title": "Analysis Backends",
    "section": "Licensing Summary",
    "text": "Licensing Summary\n\nMIT/Apache-2.0 (praatfan-local, praatfan)\nPermissions: - ✅ Commercial use - ✅ Modification - ✅ Distribution - ✅ Private use\nConditions: - Include copyright notice - Include license text\nIdeal for: - Commercial projects - Proprietary software - Embedding in closed-source tools\n\n\nGPL (praatfan-gpl)\nPermissions: - ✅ Commercial use - ✅ Modification - ✅ Distribution - ✅ Private use\nConditions: - Disclose source code - License under GPL - State changes - Include copyright\nIdeal for: - Open-source projects - GPL-compatible software - When GPL features are required\n\n\n\n\n\n\nNoteEmbedding and GPL\n\n\n\nSimply embedding the Ozen-web viewer in a webpage via iframe typically does NOT require your website to be GPL-licensed, as this is considered “mere aggregation.” However, consult legal counsel for specific situations.",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#future-backends",
    "href": "reference/backends.html#future-backends",
    "title": "Analysis Backends",
    "section": "Future Backends",
    "text": "Future Backends\nPotential future backend options:\n\npraatfan-minimal - Smaller WASM (pitch + formants only)\npraatfan-gpu - GPU-accelerated spectrogram computation\npraatfan-python - Pyodide-based backend for Jupyter notebooks",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/backends.html#see-also",
    "href": "reference/backends.html#see-also",
    "title": "Analysis Backends",
    "section": "See Also",
    "text": "See Also\n\nConfiguration - Configuring default backend\nURL Parameters - Backend parameter\nWASM Integration - Developer guide",
    "crumbs": [
      "Analysis Backends"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html",
    "href": "reference/keyboard-shortcuts.html",
    "title": "Keyboard Shortcuts",
    "section": "",
    "text": "Keyboard shortcuts dramatically speed up your workflow in Ozen-web. This page lists all available shortcuts organized by category.\n\n\n\n\n\n\nTip\n\n\n\nTip: Print this page or keep it open in a second tab while learning Ozen-web.",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#overview",
    "href": "reference/keyboard-shortcuts.html#overview",
    "title": "Keyboard Shortcuts",
    "section": "",
    "text": "Keyboard shortcuts dramatically speed up your workflow in Ozen-web. This page lists all available shortcuts organized by category.\n\n\n\n\n\n\nTip\n\n\n\nTip: Print this page or keep it open in a second tab while learning Ozen-web.",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#audio-playback",
    "href": "reference/keyboard-shortcuts.html#audio-playback",
    "title": "Keyboard Shortcuts",
    "section": "Audio Playback",
    "text": "Audio Playback\n\n\n\n\n\n\n\n\nKey\nAction\nNotes\n\n\n\n\nSpace\nPlay/pause selection\nIf no selection, plays from cursor to end of visible window\n\n\nTab\nPlay visible window\nPlays entire time range currently shown\n\n\nEscape\nStop playback & clear selection\nStops audio and removes selection highlight",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#navigation",
    "href": "reference/keyboard-shortcuts.html#navigation",
    "title": "Keyboard Shortcuts",
    "section": "Navigation",
    "text": "Navigation\n\n\n\n\n\n\n\n\nKey\nAction\nNotes\n\n\n\n\n← →\nPan left/right\nPan by 10% of visible window\n\n\n↑ ↓\nZoom in/out\nCentered on visible window center\n\n\nMouse Wheel\nZoom in/out\nCentered on mouse position\n\n\nHorizontal Scroll\nPan view\nTwo-finger swipe on trackpads\n\n\nShift+Scroll\nPan view (maybe)\nBrowser/OS dependent",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#editing-annotations-data-points",
    "href": "reference/keyboard-shortcuts.html#editing-annotations-data-points",
    "title": "Keyboard Shortcuts",
    "section": "Editing (Annotations & Data Points)",
    "text": "Editing (Annotations & Data Points)\n\n\n\n\n\n\n\n\nKey\nAction\nNotes\n\n\n\n\nCtrl+Z\nUndo\nMac: Cmd+Z\n\n\nCtrl+Y\nRedo\nMac: Cmd+Shift+Z\n\n\nCtrl+Shift+Z\nRedo (alternative)\nCross-platform consistency\n\n\nEnter\nSave edited label\nWhen editing interval text\n\n\nEscape\nCancel label edit\nDiscards changes to interval label\n\n\nDelete\nRemove selected item\nWorks on selected boundary or data point (if implemented)",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#annotation-tiers",
    "href": "reference/keyboard-shortcuts.html#annotation-tiers",
    "title": "Keyboard Shortcuts",
    "section": "Annotation Tiers",
    "text": "Annotation Tiers\n\n\n\n\n\n\n\n\nKey\nAction\nNotes\n\n\n\n\n1\nSelect tier 1\nMakes tier 1 active for editing\n\n\n2\nSelect tier 2\nMakes tier 2 active for editing\n\n\n3\nSelect tier 3\nMakes tier 3 active for editing\n\n\n4\nSelect tier 4\nMakes tier 4 active for editing\n\n\n5\nSelect tier 5\nMakes tier 5 active for editing\n\n\nDouble-click on tier\nAdd boundary\nAt cursor time position\n\n\nDouble-click on interval\nEdit label\nOpens text input for interval\n\n\nRight-click on boundary\nRemove boundary menu\nShows context menu\n\n\nDrag boundary\nMove boundary\nAdjust boundary time position",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#data-points",
    "href": "reference/keyboard-shortcuts.html#data-points",
    "title": "Keyboard Shortcuts",
    "section": "Data Points",
    "text": "Data Points\n\n\n\n\n\n\n\n\nKey\nAction\nNotes\n\n\n\n\nDouble-click on spectrogram\nAdd data point\nAt clicked time/frequency\n\n\nDrag data point\nMove data point\nAdjust time and/or frequency position\n\n\nRight-click on data point\nRemove data point\nShows context menu\n\n\nClick data point\nSelect & view values\nValues panel updates",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#mouse-click-actions",
    "href": "reference/keyboard-shortcuts.html#mouse-click-actions",
    "title": "Keyboard Shortcuts",
    "section": "Mouse & Click Actions",
    "text": "Mouse & Click Actions\n\n\n\nAction\nResult\nLocation\n\n\n\n\nSingle Click\nPlace cursor\nWaveform or spectrogram\n\n\nClick & Drag\nSelect time region\nWaveform or spectrogram\n\n\nDouble-click\nAdd boundary\nAnnotation tier\n\n\nDouble-click\nEdit interval label\nAnnotation interval\n\n\nDouble-click\nAdd data point\nSpectrogram (outside tiers)\n\n\nRight-click\nContext menu\nBoundary or data point\n\n\nDrag\nMove boundary/point\nBoundary line or data point\n\n\nScroll Wheel\nZoom\nWaveform or spectrogram",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#platform-specific-shortcuts",
    "href": "reference/keyboard-shortcuts.html#platform-specific-shortcuts",
    "title": "Keyboard Shortcuts",
    "section": "Platform-Specific Shortcuts",
    "text": "Platform-Specific Shortcuts\n\nWindows/Linux\n\n\n\nKey\nMac Equivalent\n\n\n\n\nCtrl\nCmd\n\n\nAlt\nOption\n\n\n\n\n\nmacOS\nMost shortcuts use Cmd instead of Ctrl:\n\nUndo: Cmd+Z\nRedo: Cmd+Shift+Z\nPan: Cmd+Drag",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#tips-for-efficient-workflow",
    "href": "reference/keyboard-shortcuts.html#tips-for-efficient-workflow",
    "title": "Keyboard Shortcuts",
    "section": "Tips for Efficient Workflow",
    "text": "Tips for Efficient Workflow\n\n\n\n\n\n\nTip\n\n\n\nMaster these shortcuts first:\n\nSpace — Play selection (most used)\nScroll Wheel — Zoom (essential for navigation)\nCtrl+Z — Undo (safety net)\nDouble-click — Add boundaries and data points\n1-5 — Switch between annotation tiers\n\n\n\n\nTypical Annotation Workflow\n\nScroll Wheel — Zoom to see target region\nClick — Place cursor at boundary location\nSpace — Play selection to verify\nDouble-click tier — Add boundary\nDouble-click interval — Edit label\nEnter — Save label\nCtrl+Z — Undo if needed\n\n\n\nTypical Data Collection Workflow\n\nScroll Wheel — Zoom to vowel\nClick — Place cursor at vowel midpoint\nSpace — Play to verify location\nDouble-click spectrogram — Add data point\nCheck values panel for measurements\nRepeat for all target segments",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#customization",
    "href": "reference/keyboard-shortcuts.html#customization",
    "title": "Keyboard Shortcuts",
    "section": "Customization",
    "text": "Customization\nCurrently, keyboard shortcuts are not customizable. This may change in future versions.\nIf you need different key bindings, please open an issue on GitHub.",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#accessibility",
    "href": "reference/keyboard-shortcuts.html#accessibility",
    "title": "Keyboard Shortcuts",
    "section": "Accessibility",
    "text": "Accessibility\nOzen-web aims to be keyboard-accessible for users who cannot use a mouse:\n\nAll major functions have keyboard shortcuts\nTab navigation works for UI controls\nScreen reader support is limited (this is an area for improvement)\n\nSuggestions for accessibility improvements are welcome! Please contribute feedback.",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#quick-reference-card",
    "href": "reference/keyboard-shortcuts.html#quick-reference-card",
    "title": "Keyboard Shortcuts",
    "section": "Quick Reference Card",
    "text": "Quick Reference Card\nPrint-friendly quick reference:\n\n\n\n\nCategory\nKey\nAction\n\n\n\n\nPlayback\nSpace\nPlay selection\n\n\n\nTab\nPlay window\n\n\n\nEsc\nStop\n\n\nNavigate\n←→\nPan left/right\n\n\n\n↑↓\nZoom in/out\n\n\n\nWheel\nZoom (centered on mouse)\n\n\n\nHorizontal scroll\nPan (trackpad)\n\n\nEdit\nCtrl+Z\nUndo\n\n\n\nCtrl+Y\nRedo\n\n\n\nCtrl+C\nCopy data points\n\n\n\nEnter\nSave label\n\n\nTiers\n1-5\nSelect tier\n\n\n\nDbl-click tier\nAdd boundary\n\n\n\nDbl-click interval\nEdit label\n\n\nData\nDbl-click spectrogram\nAdd point\n\n\n\nRight-click\nRemove\n\n\n\n\n\n\n\n\n\n\nTipArrow Keys Work Everywhere\n\n\n\nThe arrow keys (←→ for panning, ↑↓ for zooming) work consistently across all browsers and operating systems, making them the most reliable navigation method.",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#browser-specific-notes",
    "href": "reference/keyboard-shortcuts.html#browser-specific-notes",
    "title": "Keyboard Shortcuts",
    "section": "Browser-Specific Notes",
    "text": "Browser-Specific Notes\n\nChrome/Edge\n\nAll shortcuts work as expected\nBest overall experience\n\n\n\nFirefox\n\nAll shortcuts work\nSlightly different scroll behavior on some platforms\n\n\n\nSafari\n\nAll shortcuts work\nCmd key on macOS instead of Ctrl",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#mobiletouch",
    "href": "reference/keyboard-shortcuts.html#mobiletouch",
    "title": "Keyboard Shortcuts",
    "section": "Mobile/Touch",
    "text": "Mobile/Touch\nThe mobile viewer (/viewer route) uses touch gestures instead of keyboard shortcuts:\n\nTap → Place cursor\nDrag → Select region\nTwo-finger drag → Pan\nPinch → Zoom\n\nSee: Mobile Viewer Documentation",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "reference/keyboard-shortcuts.html#next-steps",
    "href": "reference/keyboard-shortcuts.html#next-steps",
    "title": "Keyboard Shortcuts",
    "section": "Next Steps",
    "text": "Next Steps\n\nPractice: Try the Tutorial to practice shortcuts\nWorkflow: Read Features Overview for efficient workflows\nCustomize: See Configuration Reference for other settings",
    "crumbs": [
      "Keyboard Shortcuts"
    ]
  },
  {
    "objectID": "development/setup.html",
    "href": "development/setup.html",
    "title": "Development Setup",
    "section": "",
    "text": "Node.js 18 or later\nnpm 9 or later\nGit\nText editor (VS Code recommended)",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#prerequisites",
    "href": "development/setup.html#prerequisites",
    "title": "Development Setup",
    "section": "",
    "text": "Node.js 18 or later\nnpm 9 or later\nGit\nText editor (VS Code recommended)",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#quick-start",
    "href": "development/setup.html#quick-start",
    "title": "Development Setup",
    "section": "Quick Start",
    "text": "Quick Start\n# Clone repository\ngit clone https://github.com/ucpresearch/ozen-web.git\ncd ozen-web\n\n# Install dependencies\nnpm install\n\n# Copy WASM backend (optional, for local backend)\nmkdir -p static/wasm/praatfan\ncp -r ../praatfan-core-rs/rust/pkg/* static/wasm/praatfan/\n\n# Start development server\nnpm run dev\n\n# Open http://localhost:5173",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#repository-structure",
    "href": "development/setup.html#repository-structure",
    "title": "Development Setup",
    "section": "Repository Structure",
    "text": "Repository Structure\nozen-web/\n├── src/\n│   ├── lib/                      # Library code\n│   │   ├── stores/               # Svelte stores (state management)\n│   │   ├── wasm/                 # WASM integration\n│   │   ├── audio/                # Web Audio playback\n│   │   ├── textgrid/             # TextGrid parser\n│   │   ├── touch/                # Touch gesture handling\n│   │   ├── components/           # Svelte components\n│   │   └── types.ts              # TypeScript types\n│   ├── routes/\n│   │   ├── +page.svelte          # Main app\n│   │   ├── +layout.svelte        # App shell\n│   │   └── viewer/               # Mobile viewer route\n│   └── app.html                  # HTML template\n├── static/                       # Static assets\n│   ├── wasm/                     # WASM backends (gitignored)\n│   ├── favicon*.png              # Icons\n│   └── config.yaml               # Optional config\n├── scripts/                      # Helper scripts\n│   ├── create-iframe.R           # R embedding helper\n│   ├── create-iframe.py          # Python embedding helper\n│   └── fix-relative-paths.js     # Build script\n├── docs/                         # Documentation (Quarto)\n├── package.json\n├── svelte.config.js\n├── vite.config.ts\n└── tsconfig.json",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#development-workflow",
    "href": "development/setup.html#development-workflow",
    "title": "Development Setup",
    "section": "Development Workflow",
    "text": "Development Workflow\n\n1. Start Dev Server\nnpm run dev\n\nOpens at http://localhost:5173\nHot module replacement enabled\nChanges reload automatically\n\n\n\n2. Make Changes\nEdit files in src/:\nComponents:\n&lt;!-- src/lib/components/MyComponent.svelte --&gt;\n&lt;script lang=\"ts\"&gt;\n  export let prop: string;\n&lt;/script&gt;\n\n&lt;div&gt;{prop}&lt;/div&gt;\nStores:\n// src/lib/stores/myStore.ts\nimport { writable } from 'svelte/store';\n\nexport const myStore = writable&lt;string&gt;('initial value');\nTypes:\n// src/lib/types.ts\nexport interface MyType {\n  field: string;\n}\n\n\n3. Test Changes\n\nLoad audio file\nTest features\nCheck browser console for errors\nTest on different browsers\n\n\n\n4. Build for Production\nnpm run build\nOutput in build/ directory.\n\n\n5. Preview Production Build\nnpm run preview\nOpens at http://localhost:4173",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#npm-scripts",
    "href": "development/setup.html#npm-scripts",
    "title": "Development Setup",
    "section": "npm Scripts",
    "text": "npm Scripts\n\n\n\nCommand\nDescription\n\n\n\n\nnpm run dev\nStart development server\n\n\nnpm run build\nBuild for production\n\n\nnpm run preview\nPreview production build\n\n\nnpm run check\nType-check without building\n\n\nnpm run check:watch\nType-check in watch mode\n\n\nnpm run lint\nLint code (if configured)\n\n\nnpm run format\nFormat code (if configured)",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#vs-code-setup",
    "href": "development/setup.html#vs-code-setup",
    "title": "Development Setup",
    "section": "VS Code Setup",
    "text": "VS Code Setup\n\nRecommended Extensions\n\nSvelte for VS Code (svelte.svelte-vscode)\nTypeScript and JavaScript Language Features (built-in)\nESLint (if using)\nPrettier (if using)\n\n\n\nSettings\nCreate .vscode/settings.json:\n{\n  \"editor.formatOnSave\": true,\n  \"editor.defaultFormatter\": \"svelte.svelte-vscode\",\n  \"[svelte]\": {\n    \"editor.defaultFormatter\": \"svelte.svelte-vscode\"\n  },\n  \"[typescript]\": {\n    \"editor.defaultFormatter\": \"esbenp.prettier-vscode\"\n  }\n}",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#typescript-configuration",
    "href": "development/setup.html#typescript-configuration",
    "title": "Development Setup",
    "section": "TypeScript Configuration",
    "text": "TypeScript Configuration\ntsconfig.json is pre-configured for:\n\nSvelteKit integration\nStrict type checking\nPath aliases ($lib/*)\n\nNo changes needed for typical development.",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#environment-variables",
    "href": "development/setup.html#environment-variables",
    "title": "Development Setup",
    "section": "Environment Variables",
    "text": "Environment Variables\nOzen-web doesn’t use environment variables by default. Configuration is via static/config.yaml.",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#building-wasm-backend",
    "href": "development/setup.html#building-wasm-backend",
    "title": "Development Setup",
    "section": "Building WASM Backend",
    "text": "Building WASM Backend\nIf developing WASM integration:\n# Navigate to praatfan-core-rs\ncd ../praatfan-core-rs/rust\n\n# Build WASM package\nwasm-pack build --target web\n\n# Copy to Ozen-web\ncp -r pkg/* ../ozen-web/static/wasm/praatfan/",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#debugging",
    "href": "development/setup.html#debugging",
    "title": "Development Setup",
    "section": "Debugging",
    "text": "Debugging\n\nBrowser DevTools\nChrome DevTools: 1. Open DevTools (F12) 2. Sources tab → filesystem → add workspace 3. Set breakpoints in source files 4. Use console for logging\nVue DevTools for Svelte: Not applicable — use browser console and Svelte DevTools (community extension)\n\n\nConsole Logging\nimport { dev } from '$app/environment';\n\nif (dev) {\n  console.log('Debug info:', variable);\n}\n\n\nType Checking\n# Check types without building\nnpm run check\n\n# Watch mode\nnpm run check:watch",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#common-issues",
    "href": "development/setup.html#common-issues",
    "title": "Development Setup",
    "section": "Common Issues",
    "text": "Common Issues\n\nPort Already in Use\nError: Port 5173 is already in use\nSolution:\n# Kill process on port 5173\nlsof -ti:5173 | xargs kill\n\n# Or use different port\nnpm run dev -- --port 3000\n\n\nWASM Not Loading\nError: Failed to initialize WASM\nSolution: - Check static/wasm/praatfan/ exists and has files - Use praatfan CDN backend instead - Check browser console for specific error\n\n\nType Errors\nError: TypeScript errors in imports\nSolution:\n# Regenerate types\nnpm run check\n\n# Check tsconfig.json paths",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#testing",
    "href": "development/setup.html#testing",
    "title": "Development Setup",
    "section": "Testing",
    "text": "Testing\nCurrently no automated tests. Manual testing workflow:\n\nLoad various audio formats (WAV, MP3, OGG)\nTest all overlay toggles\nCreate annotations, test undo/redo\nAdd data points, export TSV\nTest on Chrome, Firefox, Safari\nTest mobile viewer on phone",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#contributing",
    "href": "development/setup.html#contributing",
    "title": "Development Setup",
    "section": "Contributing",
    "text": "Contributing\nSee Contributing Guide for: - Code style guidelines - Pull request process - Issue reporting",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/setup.html#see-also",
    "href": "development/setup.html#see-also",
    "title": "Development Setup",
    "section": "See Also",
    "text": "See Also\n\nArchitecture - System design overview\nStores - State management guide\nWASM Integration - Backend development",
    "crumbs": [
      "Development",
      "Development Setup"
    ]
  },
  {
    "objectID": "development/contributing.html",
    "href": "development/contributing.html",
    "title": "Contributing",
    "section": "",
    "text": "Thank you for considering contributing to Ozen-web! This guide will help you get set up and understand our development workflow.\n\n\nBefore contributing, ensure you have:\n\nNode.js 18 or later\nnpm 9 or later\nGit\nText editor (VS Code recommended with Svelte extension)\nBasic familiarity with TypeScript, Svelte, and Canvas API\n\n\n\n\n\nFork the repository on GitHub\nClone your fork:\ngit clone https://github.com/YOUR-USERNAME/Ozen-web.git\ncd ozen-web\nAdd upstream remote:\ngit remote add upstream https://github.com/ucpresearch/ozen-web.git\nInstall dependencies:\nnpm install\nCopy WASM backend (optional, for local backend):\nmkdir -p static/wasm/praatfan\n# Copy from praatfan-core-clean or use CDN backend\nStart development server:\nnpm run dev\nOpen http://localhost:5173 and verify the app works\n\nFor detailed setup instructions, see Development Setup.",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#getting-started",
    "href": "development/contributing.html#getting-started",
    "title": "Contributing",
    "section": "",
    "text": "Thank you for considering contributing to Ozen-web! This guide will help you get set up and understand our development workflow.\n\n\nBefore contributing, ensure you have:\n\nNode.js 18 or later\nnpm 9 or later\nGit\nText editor (VS Code recommended with Svelte extension)\nBasic familiarity with TypeScript, Svelte, and Canvas API\n\n\n\n\n\nFork the repository on GitHub\nClone your fork:\ngit clone https://github.com/YOUR-USERNAME/Ozen-web.git\ncd ozen-web\nAdd upstream remote:\ngit remote add upstream https://github.com/ucpresearch/ozen-web.git\nInstall dependencies:\nnpm install\nCopy WASM backend (optional, for local backend):\nmkdir -p static/wasm/praatfan\n# Copy from praatfan-core-clean or use CDN backend\nStart development server:\nnpm run dev\nOpen http://localhost:5173 and verify the app works\n\nFor detailed setup instructions, see Development Setup.",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#development-workflow",
    "href": "development/contributing.html#development-workflow",
    "title": "Contributing",
    "section": "Development Workflow",
    "text": "Development Workflow\n\nCreating a Feature Branch\n# Update your fork\ngit checkout master\ngit pull upstream master\n\n# Create feature branch\ngit checkout -b feature/your-feature-name\nBranch naming conventions: - feature/description - New features - fix/description - Bug fixes - refactor/description - Code refactoring - docs/description - Documentation changes\n\n\nMaking Changes\n\nWrite code following our style guidelines (see below)\nTest thoroughly (manual testing checklist below)\nCommit with clear messages:\ngit add file1.ts file2.svelte\ngit commit -m \"Add pitch smoothing toggle to settings panel\"\nPush to your fork:\ngit push origin feature/your-feature-name\nOpen a pull request on GitHub\n\n\n\nCommit Message Guidelines\nFormat:\n&lt;type&gt;: &lt;description&gt;\n\n[optional body]\n\n[optional footer]\nTypes: - feat: New feature - fix: Bug fix - refactor: Code refactoring - docs: Documentation changes - style: Code style changes (formatting, no logic change) - test: Adding or updating tests - chore: Maintenance tasks\nExamples:\nGood:\nfeat: Add keyboard shortcut (S) for toggling spectrogram\n\nAdd S key to show/hide spectrogram overlay. Updates keyboard\nshortcuts documentation and adds visual feedback on toggle.\n\nCloses #123\nBad:\nfixed stuff",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#code-style-guidelines",
    "href": "development/contributing.html#code-style-guidelines",
    "title": "Contributing",
    "section": "Code Style Guidelines",
    "text": "Code Style Guidelines\n\nTypeScript\nUse strict typing:\n// ✅ GOOD: Explicit types\nfunction computePitch(sound: any, timeStep: number, floor: number, ceiling: number): any {\n  return sound.to_pitch_ac(timeStep, floor, ceiling);\n}\n\n// ❌ BAD: Implicit any everywhere\nfunction computePitch(sound, timeStep, floor, ceiling) {\n  return sound.to_pitch_ac(timeStep, floor, ceiling);\n}\nPrefer interfaces over types for objects:\n// ✅ GOOD\nexport interface DataPoint {\n  id: number;\n  time: number;\n  frequency: number;\n}\n\n// ⚠️ OK but prefer interface\nexport type DataPoint = {\n  id: number;\n  time: number;\n  frequency: number;\n};\nUse const for immutable values:\n// ✅ GOOD\nconst MAX_FREQUENCY = 10000;\nconst defaultColors = {  cursor: '#ff0000' };\n\n// ❌ BAD\nlet MAX_FREQUENCY = 10000;\n\n\nSvelte Components\nComponent structure:\n&lt;script lang=\"ts\"&gt;\n  // 1. Imports\n  import { onMount } from 'svelte';\n  import { audioBuffer } from '$lib/stores/audio';\n\n  // 2. Props\n  export let height: number = 600;\n  export let showCursor: boolean = true;\n\n  // 3. Local state\n  let canvas: HTMLCanvasElement;\n  let ctx: CanvasRenderingContext2D | null = null;\n\n  // 4. Reactive statements\n  $: if ($audioBuffer && ctx) {\n    redraw();\n  }\n\n  // 5. Functions\n  function redraw() {\n    // ...\n  }\n\n  // 6. Lifecycle\n  onMount(() =&gt; {\n    ctx = canvas.getContext('2d');\n  });\n&lt;/script&gt;\n\n&lt;!-- 7. Template --&gt;\n&lt;canvas bind:this={canvas} {height} /&gt;\n\n&lt;!-- 8. Styles --&gt;\n&lt;style&gt;\n  canvas {\n    cursor: crosshair;\n  }\n&lt;/style&gt;\nNaming conventions:\n\nComponents: PascalCase.svelte\nProps: camelCase\nLocal variables: camelCase\nConstants: UPPER_SNAKE_CASE\n\nPrefer reactive statements over direct subscriptions:\n&lt;!-- ✅ GOOD: Reactive statement --&gt;\n&lt;script&gt;\n  import { cursorPosition } from '$lib/stores/view';\n\n  $: console.log('Cursor at', $cursorPosition);\n&lt;/script&gt;\n\n&lt;!-- ❌ BAD: Manual subscription --&gt;\n&lt;script&gt;\n  import { cursorPosition } from '$lib/stores/view';\n  import { onMount, onDestroy } from 'svelte';\n\n  let position = 0;\n  let unsubscribe;\n\n  onMount(() =&gt; {\n    unsubscribe = cursorPosition.subscribe(p =&gt; position = p);\n  });\n\n  onDestroy(() =&gt; {\n    unsubscribe();\n  });\n\n  $: console.log('Cursor at', position);\n&lt;/script&gt;\n\n\nFile Organization\nStore functions:\n// src/lib/stores/myStore.ts\n\n/**\n * Module docstring explaining what this store manages.\n */\n\nimport { writable } from 'svelte/store';\n\n// 1. Type definitions\nexport interface MyData {\n  field: string;\n}\n\n// 2. Store exports\nexport const myStore = writable&lt;MyData[]&gt;([]);\n\n// 3. Helper functions (non-exported)\nfunction helperFunction() {\n  // ...\n}\n\n// 4. Public API functions\nexport function addItem(item: MyData): void {\n  myStore.update(items =&gt; [...items, item]);\n}\n\n\nComments and Documentation\nUse JSDoc for public APIs:\n/**\n * Compute pitch track from audio samples.\n *\n * @param sound - WASM Sound object\n * @param timeStep - Analysis time step in seconds\n * @param pitchFloor - Minimum pitch in Hz\n * @param pitchCeiling - Maximum pitch in Hz\n * @returns Pitch object (must be freed with .free())\n *\n * @example\n * const sound = createSound(samples, sampleRate);\n * const pitch = computePitch(sound, 0.01, 75, 600);\n * // ... use pitch\n * pitch.free();\n * sound.free();\n */\nexport function computePitch(\n  sound: any,\n  timeStep: number,\n  pitchFloor: number,\n  pitchCeiling: number\n): any {\n  // Implementation...\n}\nInline comments for complex logic:\n// ✅ GOOD: Explain non-obvious logic\n// Convert pixel x-coordinate to time using visible time range\nconst time = $timeRange.start + (x / canvasWidth) * ($timeRange.end - $timeRange.start);\n\n// ❌ BAD: State the obvious\n// Set cursor position to time\ncursorPosition.set(time);\n\n\nFormatting\nUse Prettier defaults (if configured):\n\n2 spaces for indentation\nSingle quotes for strings (except avoid escaping)\nNo semicolons (Prettier will add them)\nTrailing commas where valid\n\nManual formatting (if no Prettier):\n// ✅ GOOD: Consistent spacing\nconst obj = { a: 1, b: 2 };\nif (condition) {\n  doSomething();\n}\n\n// ❌ BAD: Inconsistent spacing\nconst obj={a:1,b:2};\nif(condition){doSomething();}",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#component-patterns",
    "href": "development/contributing.html#component-patterns",
    "title": "Contributing",
    "section": "Component Patterns",
    "text": "Component Patterns\n\nStore Usage\n&lt;script lang=\"ts\"&gt;\n  import { audioBuffer } from '$lib/stores/audio';\n  import { cursorPosition } from '$lib/stores/view';\n\n  // ✅ GOOD: Use reactive statements\n  $: if ($audioBuffer) {\n    console.log('Audio loaded');\n  }\n\n  // ✅ GOOD: Direct binding in template\n&lt;/script&gt;\n\n&lt;div&gt;Cursor: {$cursorPosition.toFixed(3)}s&lt;/div&gt;\n\n\nCanvas Rendering\n&lt;script lang=\"ts\"&gt;\n  import { onMount } from 'svelte';\n  import { audioBuffer } from '$lib/stores/audio';\n\n  let canvas: HTMLCanvasElement;\n  let ctx: CanvasRenderingContext2D | null = null;\n\n  // Reactive redraw\n  $: if ($audioBuffer && ctx) {\n    renderWaveform();\n  }\n\n  function renderWaveform() {\n    if (!ctx || !$audioBuffer) return;\n\n    // Clear canvas\n    ctx.clearRect(0, 0, canvas.width, canvas.height);\n\n    // Draw waveform\n    // ...\n  }\n\n  onMount(() =&gt; {\n    ctx = canvas.getContext('2d');\n  });\n&lt;/script&gt;\n\n&lt;canvas bind:this={canvas} width={800} height={200} /&gt;\n\n\nWASM Memory Management\n// ✅ GOOD: Always free WASM objects\nexport async function analyzeAudio() {\n  const sound = createSound($audioBuffer, $sampleRate);\n  try {\n    const pitch = computePitch(sound, 0.01, 75, 600);\n    try {\n      // Use pitch...\n    } finally {\n      pitch.free();\n    }\n  } finally {\n    sound.free();\n  }\n}\n\n// ❌ BAD: Memory leak\nexport async function analyzeAudio() {\n  const sound = createSound($audioBuffer, $sampleRate);\n  const pitch = computePitch(sound, 0.01, 75, 600);\n  // Objects never freed\n}",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#testing",
    "href": "development/contributing.html#testing",
    "title": "Contributing",
    "section": "Testing",
    "text": "Testing\n\nManual Testing Checklist\nBefore submitting a pull request, test the following:\nAudio Loading: - [ ] Drag & drop WAV file - [ ] Drag & drop MP3 file - [ ] File picker - [ ] Microphone recording - [ ] URL parameter loading (?audio=...)\nVisualization: - [ ] Waveform displays correctly - [ ] Spectrogram renders - [ ] Zoom in/out with scroll wheel - [ ] Pan left/right - [ ] Cursor updates on click - [ ] Selection via drag\nOverlays: - [ ] Toggle pitch overlay - [ ] Toggle formants overlay - [ ] Toggle intensity overlay - [ ] Toggle HNR, CoG, spectral tilt, A1-P0\nAnnotations: - [ ] Add annotation tier - [ ] Remove tier - [ ] Add boundary (double-click) - [ ] Remove boundary (right-click) - [ ] Move boundary (drag) - [ ] Edit interval text - [ ] Undo/redo (Ctrl+Z / Ctrl+Y)\nData Points: - [ ] Add data point (double-click spectrogram) - [ ] Move data point (drag) - [ ] Remove data point (right-click) - [ ] Export TSV\nPlayback: - [ ] Play selection (Space) - [ ] Play visible window (Tab) - [ ] Pause (Space) - [ ] Stop (Escape) - [ ] Cursor tracks during playback\nFile I/O: - [ ] Export TextGrid - [ ] Import TextGrid - [ ] Save audio as WAV - [ ] Export data points TSV\nBrowser Compatibility: - [ ] Chrome - [ ] Firefox - [ ] Safari - [ ] Edge\nMobile Viewer: - [ ] URL loading works - [ ] Touch gestures (tap, drag, pinch, pan) - [ ] Settings drawer - [ ] Play button\n\n\nTesting Long Audio\nIf your change affects long audio handling (&gt;60s):\n\nLoad 2-minute audio file\nVerify spectrogram shows “Zoom in…” message\nZoom to &lt;60s window\nVerify analysis runs\nPan around\nVerify no UI freezing\n\n\n\nRegression Testing\nBefore submitting, verify you didn’t break:\n\nExisting keyboard shortcuts\nExisting mouse interactions\nStore reactivity\nWASM memory management (no leaks)",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#pull-request-process",
    "href": "development/contributing.html#pull-request-process",
    "title": "Contributing",
    "section": "Pull Request Process",
    "text": "Pull Request Process\n\nBefore Submitting\n\nUpdate your branch:\ngit checkout master\ngit pull upstream master\ngit checkout feature/your-feature\ngit rebase master\nRun build:\nnpm run build\nEnsure it builds without errors.\nRun type checking:\nnpm run check\nFix all TypeScript errors.\nTest manually using checklist above\nWrite good commit messages following guidelines\n\n\n\nOpening the PR\n\nGo to https://github.com/ucpresearch/ozen-web\nClick “New Pull Request”\nSelect your fork and branch\nFill out the PR template:\n\nTitle: Clear, concise description\nDescription:\n## What\n\nBrief description of what this PR does.\n\n## Why\n\nExplain the motivation or fix.\n\n## How\n\nTechnical details if complex.\n\n## Testing\n\n- [x] Tested on Chrome\n- [x] Tested on Firefox\n- [x] Ran manual test checklist\n- [x] Build passes\n- [x] Type check passes\n\n## Screenshots\n\n(If UI change, include before/after screenshots)\n\nCloses #123\n\n\nCode Review\nExpect feedback: - Reviewers may request changes - Respond to comments promptly - Push changes to the same branch (PR updates automatically)\nBe respectful: - Accept constructive criticism - Explain your reasoning if you disagree - Don’t take feedback personally\nAfter approval: - Maintainer will merge your PR - Delete your feature branch: bash   git branch -d feature/your-feature   git push origin --delete feature/your-feature",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#reporting-issues",
    "href": "development/contributing.html#reporting-issues",
    "title": "Contributing",
    "section": "Reporting Issues",
    "text": "Reporting Issues\n\nBug Reports\nUse the GitHub issue template and include:\n\nDescription: What’s wrong?\nSteps to reproduce:\n1. Load audio file\n2. Click on spectrogram\n3. Observe error in console\nExpected behavior: What should happen?\nActual behavior: What actually happens?\nEnvironment:\n\nBrowser: Chrome 120\nOS: Windows 11\nAudio file: short.wav (WAV, 16-bit, 16 kHz, 5s)\n\nScreenshots: If applicable\nConsole errors: Copy full error messages\n\n\n\nFeature Requests\nInclude:\n\nUse case: Why is this needed?\nProposed solution: How might it work?\nAlternatives: Other approaches considered?\nAdditional context: Examples, mockups, etc.",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#documentation",
    "href": "development/contributing.html#documentation",
    "title": "Contributing",
    "section": "Documentation",
    "text": "Documentation\n\nWhen to Update Docs\nUpdate documentation when you:\n\nAdd a new feature\nChange existing behavior\nAdd/remove keyboard shortcuts\nModify API functions\nFix significant bugs\n\n\n\nDocumentation Structure\n\nUser docs: docs/ (Quarto .html files)\n\nTutorial: Step-by-step guides\nFeatures: Feature descriptions\nReference: Technical details\n\nDeveloper docs: docs/development/ (this section)\nCode comments: Inline JSDoc and comments\n\n\n\nWriting Documentation\nUse clear, concise language:\n&lt;!-- ✅ GOOD --&gt;\n## Adding Data Points\n\nDouble-click on the spectrogram to add a data point.\n\n&lt;!-- ❌ BAD --&gt;\n## Data Point Addition Methodology\n\nIn order to facilitate the instantiation of a data collection\npoint entity, the user should utilize a double-click interaction\nmodality upon the spectrogram visualization interface.\nInclude code examples:\n## Usage\n\n\\`\\`\\`typescript\nimport { computePitch } from '$lib/wasm/acoustic';\n\nconst sound = createSound(samples, sampleRate);\nconst pitch = computePitch(sound, 0.01, 75, 600);\n// ...\npitch.free();\nsound.free();\n\\`\\`\\`\nUse screenshots for UI features:\n## Settings Panel\n\n![Settings panel screenshot](screenshots/settings-panel.png)\n\nClick the gear icon to open settings.",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#best-practices",
    "href": "development/contributing.html#best-practices",
    "title": "Contributing",
    "section": "Best Practices",
    "text": "Best Practices\n\nPerformance\n\nDebounce expensive operations (analysis during zoom)\nFree WASM objects immediately after use\nAvoid unnecessary re-renders (check reactive dependencies)\nUse requestAnimationFrame for smooth animations\n\n\n\nAccessibility\n\nKeyboard navigation for all features\nSemantic HTML where possible\nARIA labels for canvas elements (future)\nFocus indicators visible\n\n\n\nSecurity\n\nNo eval() or unsafe dynamic code\nValidate file inputs (check MIME types)\nSanitize TextGrid import (no code execution)\nUse CORS for remote audio loading\n\n\n\nCompatibility\n\nSupport modern browsers (Chrome 90+, Firefox 88+, Safari 14+)\nGraceful degradation for missing features\nTest on multiple platforms (Windows, Mac, Linux)",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#questions",
    "href": "development/contributing.html#questions",
    "title": "Contributing",
    "section": "Questions?",
    "text": "Questions?\n\nGitHub Discussions: For questions and ideas\nGitHub Issues: For bugs and feature requests\nDocumentation: Check Development section first",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#license",
    "href": "development/contributing.html#license",
    "title": "Contributing",
    "section": "License",
    "text": "License\nBy contributing to Ozen-web, you agree that your contributions will be licensed under the MIT License.\nThank you for contributing to Ozen-web! 🎉",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/contributing.html#see-also",
    "href": "development/contributing.html#see-also",
    "title": "Contributing",
    "section": "See Also",
    "text": "See Also\n\nSetup - Development environment setup\nArchitecture - System design\nStores - State management patterns\nWASM Integration - WASM backend development",
    "crumbs": [
      "Development",
      "Contributing"
    ]
  },
  {
    "objectID": "development/stores.html",
    "href": "development/stores.html",
    "title": "Stores",
    "section": "",
    "text": "Ozen-web uses Svelte stores for all shared state management. Stores are the single source of truth for application state, providing a reactive architecture where components automatically update when relevant data changes.\nLocation: All stores are in src/lib/stores/\nKey principles: - Components subscribe to stores and react to changes - Components never hold local copies of shared state - All mutations go through store functions, not direct assignment - Derived stores compute values from other stores automatically",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#overview",
    "href": "development/stores.html#overview",
    "title": "Stores",
    "section": "",
    "text": "Ozen-web uses Svelte stores for all shared state management. Stores are the single source of truth for application state, providing a reactive architecture where components automatically update when relevant data changes.\nLocation: All stores are in src/lib/stores/\nKey principles: - Components subscribe to stores and react to changes - Components never hold local copies of shared state - All mutations go through store functions, not direct assignment - Derived stores compute values from other stores automatically",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#store-types",
    "href": "development/stores.html#store-types",
    "title": "Stores",
    "section": "Store Types",
    "text": "Store Types\n\nWritable Stores\nDefinition: Can be read and updated by any component.\nimport { writable } from 'svelte/store';\n\nexport const myStore = writable&lt;string&gt;('initial value');\nUsage:\n&lt;script&gt;\n  import { myStore } from '$lib/stores/myStore';\n\n  // Subscribe (auto-managed with $)\n  $: value = $myStore;\n\n  // Update\n  function handleClick() {\n    myStore.set('new value');\n  }\n&lt;/script&gt;\n\n\nDerived Stores\nDefinition: Computed values that automatically update when their dependencies change.\nimport { derived } from 'svelte/store';\n\nexport const fullName = derived(\n  [firstName, lastName],\n  ([$first, $last]) =&gt; `${$first} ${$last}`\n);\nBenefits: - No manual cache invalidation - Always up-to-date - Declarative dependencies\n\n\nReadonly Stores\nDefinition: Can be read but not updated externally (updates only via store functions).\nimport { readable } from 'svelte/store';\n\nexport const time = readable(Date.now(), (set) =&gt; {\n  const interval = setInterval(() =&gt; set(Date.now()), 1000);\n  return () =&gt; clearInterval(interval);\n});",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#core-stores",
    "href": "development/stores.html#core-stores",
    "title": "Stores",
    "section": "Core Stores",
    "text": "Core Stores\n\naudio.ts - Audio Buffer\nPurpose: Stores loaded audio data and metadata.\nState:\n\n\n\n\n\n\n\n\nExport\nType\nDescription\n\n\n\n\naudioBuffer\nFloat64Array \\| null\nRaw audio samples (mono, -1 to 1)\n\n\nsampleRate\nnumber\nOriginal sample rate (Hz)\n\n\nfileName\nstring\nName of loaded file\n\n\nduration\nnumber\nDuration in seconds\n\n\n\nUsage:\nimport { audioBuffer, sampleRate, fileName, duration } from '$lib/stores/audio';\n\n// In component\n$: if ($audioBuffer) {\n  console.log(`Loaded ${$fileName}: ${$duration}s at ${$sampleRate} Hz`);\n}\nKey characteristics: - Audio stored at full resolution (never downsampled) - Mono only (stereo files mixed to mono on load) - Float64Array for WASM compatibility - Set once per file load, immutable afterward\n\n\nview.ts - Viewport State\nPurpose: Manages the visible time window, cursor, and selection.\nState:\n\n\n\n\n\n\n\n\nExport\nType\nDescription\n\n\n\n\ntimeRange\n{ start: number, end: number }\nVisible time window (seconds)\n\n\ncursorPosition\nnumber\nCurrent cursor position (seconds)\n\n\nselection\n{ start, end } \\| null\nSelected region (seconds)\n\n\nhoverPosition\nnumber \\| null\nMouse hover position (seconds)\n\n\nisDragging\nboolean\nWhether cursor is being dragged\n\n\nvisibleDuration\nderived\nComputed: end - start\n\n\n\nUsage:\nimport { timeRange, cursorPosition, selection } from '$lib/stores/view';\n\n// Zoom in around cursor\nfunction zoomIn() {\n  const center = get(cursorPosition);\n  const currentDuration = $timeRange.end - $timeRange.start;\n  const newDuration = currentDuration * 0.5;\n\n  timeRange.set({\n    start: center - newDuration / 2,\n    end: center + newDuration / 2\n  });\n}\n\n// Select a region\nfunction selectRegion(start: number, end: number) {\n  selection.set({ start, end });\n  cursorPosition.set(start);\n}\nReactivity pattern:\n&lt;!-- Waveform component auto-redraws when timeRange changes --&gt;\n&lt;script&gt;\n  $: if ($audioBuffer && $timeRange) {\n    redrawWaveform($audioBuffer, $timeRange);\n  }\n&lt;/script&gt;\n\n\nanalysis.ts - Acoustic Features\nPurpose: Stores computed acoustic analyses and manages analysis state.\nState:\n\n\n\n\n\n\n\n\nExport\nType\nDescription\n\n\n\n\nanalysisResults\nAnalysisResults \\| null\nCached pitch, formants, intensity, etc.\n\n\nisAnalyzing\nboolean\nAnalysis in progress flag\n\n\nanalysisProgress\nnumber\nProgress percentage (0-100)\n\n\nanalysisParams\nobject\nCurrent analysis parameters\n\n\n\nAnalysisResults structure:\ninterface AnalysisResults {\n  pitch: {\n    times: Float64Array;\n    values: Float64Array;\n  };\n  formants: {\n    times: Float64Array;\n    f1: Float64Array;\n    f2: Float64Array;\n    f3: Float64Array;\n    f4: Float64Array;\n    b1: Float64Array;\n    b2: Float64Array;\n    b3: Float64Array;\n    b4: Float64Array;\n  };\n  intensity: { /* ... */ };\n  hnr: { /* ... */ };\n  cog: { /* ... */ };\n  spectralTilt: { /* ... */ };\n  a1p0: { /* ... */ };\n  spectrogram: SpectrogramData;\n}\nFunctions:\nimport { runAnalysis, runAnalysisForRange, analysisResults } from '$lib/stores/analysis';\n\n// Trigger full analysis (for audio ≤60s)\nawait runAnalysis();\n\n// Analyze visible window only (for audio &gt;60s)\nawait runAnalysisForRange(startTime, endTime);\n\n// Access results\n$: if ($analysisResults) {\n  const { pitch, formants } = $analysisResults;\n  // Draw overlays...\n}\nLong audio handling: - Audio &gt;60s: analysisResults starts as null - User zooms in: runAnalysisForRange() called automatically (debounced 300ms) - Results cached for current window - Components check if ($analysisResults) before drawing overlays\n\n\nannotations.ts - Annotation Tiers\nPurpose: Manages annotation tiers, intervals, and boundaries.\nState:\n\n\n\n\n\n\n\n\nExport\nType\nDescription\n\n\n\n\ntiers\nTier[]\nAll annotation tiers\n\n\nselectedTierIndex\nnumber\nCurrently active tier (0-indexed)\n\n\nselectedTier\nderived\nCurrently active tier object\n\n\nselectedIntervalIndex\nnumber \\| null\nSelected interval within tier\n\n\n\nTier structure:\ninterface Tier {\n  name: string;\n  type: 'interval' | 'point';\n  intervals: Interval[];\n}\n\ninterface Interval {\n  start: number;\n  end: number;\n  text: string;\n}\nFunctions:\nimport {\n  tiers,\n  addTier,\n  removeTier,\n  renameTier,\n  addBoundary,\n  removeBoundary,\n  moveBoundary,\n  setIntervalText,\n  loadTextGrid,\n  exportTiers\n} from '$lib/stores/annotations';\n\n// Tier management (not undoable)\naddTier('words', 'interval');\nremoveTier(2);\nrenameTier(0, 'phones');\n\n// Boundary editing (undoable)\naddBoundary(tierIndex, time);       // Double-click on tier\nremoveBoundary(tierIndex, boundaryIndex);  // Right-click menu\nmoveBoundary(tierIndex, boundaryIndex, newTime);  // Drag\n\n// Text editing (undoable)\nsetIntervalText(tierIndex, intervalIndex, 'cat');\n\n// File I/O\nloadTextGrid(textGridString);  // Import from Praat\nconst tgContent = exportTiers();  // Export to TextGrid format\nUndoable operations: - addBoundary() - removeBoundary() - moveBoundary() - setIntervalText()\nNot undoable: - addTier() / removeTier() - structural changes - loadTextGrid() - file operation\n\n\ndataPoints.ts - Data Collection\nPurpose: Manages data collection points on the spectrogram.\nState:\n\n\n\nExport\nType\nDescription\n\n\n\n\ndataPoints\nDataPoint[]\nAll data points\n\n\nhoveredPointId\nnumber \\| null\nCurrently hovered point\n\n\ndraggingPointId\nnumber \\| null\nCurrently dragging point\n\n\n\nDataPoint structure:\ninterface DataPoint {\n  id: number;\n  time: number;          // Position in seconds\n  frequency: number;     // Position in Hz (click location)\n  acousticValues: {      // Measured values at this point\n    pitch?: number;\n    intensity?: number;\n    f1?: number;\n    f2?: number;\n    f3?: number;\n    f4?: number;\n    b1?: number;\n    b2?: number;\n    b3?: number;\n    b4?: number;\n    hnr?: number;\n    cog?: number;\n    spectralTilt?: number;\n    a1p0?: number;\n  };\n  annotationIntervals: { // Text from all tiers at this time\n    [tierName: string]: string;\n  };\n}\nFunctions:\nimport {\n  dataPoints,\n  addDataPoint,\n  removeDataPoint,\n  moveDataPoint,\n  exportToTSV,\n  importFromTSV\n} from '$lib/stores/dataPoints';\n\n// Add point (double-click on spectrogram)\nconst point = addDataPoint(time, frequency);\n// Automatically collects all acoustic values and annotation text\n\n// Remove point (right-click)\nremoveDataPoint(pointId);\n\n// Move point (drag)\nmoveDataPoint(pointId, newTime, newFrequency);\n\n// Export to TSV file\nconst tsvContent = exportToTSV();\n// Includes: time, freq, pitch, intensity, formants, labels, etc.\n\n// Import from TSV\nimportFromTSV(tsvContent);\nAll operations are undoable via the unified undo system.\n\n\nundoManager.ts - Undo/Redo\nPurpose: Unified undo/redo system for all editable state.\nArchitecture: - State-snapshot approach: Captures full state before each change - Single stack: All operations (annotations + data points) in chronological order - JSON deep-copy: Ensures state isolation\nAPI:\nimport {\n  initUndoManager,\n  saveUndo,\n  undo,\n  redo,\n  canUndo,\n  canRedo,\n  clearUndoHistory\n} from '$lib/stores/undoManager';\n\n// Initialize once at app startup\nonMount(() =&gt; {\n  initUndoManager(tiers, dataPoints);\n});\n\n// Before any mutation\nexport function someEditOperation() {\n  saveUndo();  // MUST call before changing state\n  tiers.update(t =&gt; { /* modify */ });\n}\n\n// In component (keyboard shortcut)\nfunction handleKeydown(e: KeyboardEvent) {\n  if ((e.ctrlKey || e.metaKey) && e.key === 'z') {\n    if (!e.shiftKey) {\n      undo();\n    } else {\n      redo();\n    }\n  }\n}\n\n// UI state\n$: undoAvailable = canUndo();\n$: redoAvailable = canRedo();\nCritical usage pattern:\n// ✅ CORRECT: Save before mutation\nexport function addBoundary(tierIndex: number, time: number): void {\n  saveUndo();  // Capture current state first\n  tiers.update(t =&gt; {\n    // Modify tiers...\n    return t;\n  });\n}\n\n// ❌ WRONG: Save after mutation\nexport function addBoundary(tierIndex: number, time: number): void {\n  tiers.update(t =&gt; {\n    // Modify tiers...\n    return t;\n  });\n  saveUndo();  // Too late! Already changed.\n}\nUndoable operations: - All annotation boundary operations - All data point operations\nNon-undoable operations: - Tier add/remove/rename (structural) - File loading (audio, TextGrid)\n\n\nconfig.ts - Application Configuration\nPurpose: Stores visual settings, formant presets, and backend choice.\nState:\ninterface OzenConfig {\n  backend: 'praatfan-local' | 'praatfan' | 'praatfan-gpl';\n  colors: {\n    waveform: { background, line, lineWidth };\n    spectrogram: { colormap: 'grayscale' | 'viridis' };\n    cursor: string;\n    selection: { fill, border };\n    pitch: string;\n    intensity: string;\n    formant: { f1, f2, f3, f4, size };\n    tier: { background, selected, border, text };\n    boundary: string;\n    /* ... */\n  };\n  formantPresets: {\n    female: { maxFormant: 5500, numFormants: 5, ... };\n    male: { maxFormant: 5000, numFormants: 5, ... };\n    child: { maxFormant: 8000, numFormants: 5, ... };\n  };\n  spectrogramSettings: {\n    windowLength: 0.005;\n    dynamicRange: 70;\n    preemphasis: 50;\n  };\n  /* ... */\n}\nFunctions:\nimport {\n  config,\n  loadConfigFromYAML,\n  getCurrentPreset,\n  applyFormantPreset\n} from '$lib/stores/config';\n\n// Load from YAML file\nawait loadConfigFromYAML('/config.yaml');\n\n// Access config\n$: cursorColor = $config.colors.cursor;\n$: pitchColor = $config.colors.pitch;\n\n// Apply formant preset\napplyFormantPreset('female');  // maxFormant = 5500\napplyFormantPreset('male');    // maxFormant = 5000\napplyFormantPreset('child');   // maxFormant = 8000\nYAML format:\nbackend: praatfan-local\n\ncolors:\n  cursor: \"#ff0000\"\n  pitch: \"#0066cc\"\n  formant:\n    f1: \"#ff3333\"\n    f2: \"#ff6666\"\n    f3: \"#ff9999\"\n    f4: \"#ffcccc\"\n\nformantPresets:\n  female:\n    maxFormant: 5500\n    numFormants: 5\n  male:\n    maxFormant: 5000\n    numFormants: 5",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#store-communication-patterns",
    "href": "development/stores.html#store-communication-patterns",
    "title": "Stores",
    "section": "Store Communication Patterns",
    "text": "Store Communication Patterns\n\nDirect Dependencies\nSome stores import and use other stores:\n// analysis.ts imports audio stores\nimport { audioBuffer, sampleRate } from './audio';\n\nexport async function runAnalysis() {\n  const buffer = get(audioBuffer);\n  const sr = get(sampleRate);\n  // Use to create WASM Sound object\n}\n\n\nDerived Stores Across Modules\n// view.ts\nexport const visibleDuration = derived(\n  timeRange,\n  ($tr) =&gt; $tr.end - $tr.start\n);\nComponents can import and use $visibleDuration without knowing how it’s computed.\n\n\nEvent-Driven Updates\n// FileDropZone component loads audio\nasync function handleAudioLoad(file: File) {\n  const decoded = await decodeAudio(file);\n\n  // Update audio stores\n  audioBuffer.set(decoded.samples);\n  sampleRate.set(decoded.sampleRate);\n  fileName.set(file.name);\n  duration.set(decoded.samples.length / decoded.sampleRate);\n\n  // Trigger analysis (if short enough)\n  await runAnalysis();\n}\n\n// Spectrogram component reacts\n$: if ($analysisResults) {\n  renderSpectrogram($analysisResults.spectrogram);\n  drawPitchOverlay($analysisResults.pitch);\n}\n\n\nCross-Store Coordination\nExample: Data points collect annotation text\n// dataPoints.ts\nfunction collectAnnotationIntervals(time: number): Record&lt;string, string&gt; {\n  const allTiers = get(tiers);  // Import from annotations.ts\n  const labels: Record&lt;string, string&gt; = {};\n\n  for (const tier of allTiers) {\n    const interval = tier.intervals.find(\n      (int) =&gt; int.start &lt;= time && time &lt; int.end\n    );\n    if (interval) {\n      labels[tier.name] = interval.text;\n    }\n  }\n\n  return labels;\n}",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#reactive-patterns",
    "href": "development/stores.html#reactive-patterns",
    "title": "Stores",
    "section": "Reactive Patterns",
    "text": "Reactive Patterns\n\nAuto-Subscription in Components\nSvelte’s $ prefix auto-subscribes to stores:\n&lt;script&gt;\n  import { cursorPosition } from '$lib/stores/view';\n\n  // Automatically subscribes when component mounts\n  // Automatically unsubscribes when component unmounts\n  $: console.log('Cursor moved to', $cursorPosition);\n&lt;/script&gt;\n\n&lt;div&gt;Cursor: {$cursorPosition.toFixed(3)}s&lt;/div&gt;\n\n\nReactive Statements\nRe-run code when dependencies change:\n&lt;script&gt;\n  import { audioBuffer, duration } from '$lib/stores/audio';\n  import { timeRange } from '$lib/stores/view';\n\n  // Recalculate when any dependency changes\n  $: visibleSamples = Math.floor(\n    ($timeRange.end - $timeRange.start) * $sampleRate\n  );\n\n  $: pixelsPerSecond = canvasWidth / ($timeRange.end - $timeRange.start);\n&lt;/script&gt;\n\n\nUpdate Methods\nThree ways to update writable stores:\nimport { myStore } from './myStore';\n\n// 1. Set (replace entire value)\nmyStore.set({ x: 10, y: 20 });\n\n// 2. Update (function receives current value)\nmyStore.update(current =&gt; ({ ...current, x: current.x + 1 }));\n\n// 3. Direct assignment in component (compiles to .set())\n$myStore = { x: 10, y: 20 };  // Only in .svelte files\n\n\nCustom Stores\nPattern: Wrap writable with custom logic.\nfunction createCounterStore() {\n  const { subscribe, set, update } = writable(0);\n\n  return {\n    subscribe,\n    increment: () =&gt; update(n =&gt; n + 1),\n    decrement: () =&gt; update(n =&gt; n - 1),\n    reset: () =&gt; set(0)\n  };\n}\n\nexport const counter = createCounterStore();\n\n// Usage\ncounter.increment();  // No need to call .update() with function",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#best-practices",
    "href": "development/stores.html#best-practices",
    "title": "Stores",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Always Use get() for One-Time Reads\nimport { get } from 'svelte/store';\nimport { audioBuffer } from '$lib/stores/audio';\n\n// ✅ CORRECT: One-time read\nfunction processAudio() {\n  const buffer = get(audioBuffer);\n  if (!buffer) return;\n  // Use buffer...\n}\n\n// ❌ WRONG: Creates subscription (memory leak in plain .ts files)\nfunction processAudio() {\n  let buffer;\n  audioBuffer.subscribe(b =&gt; buffer = b)();  // Subscription never cleaned up\n  // Use buffer...\n}\n\n\n2. Use Derived Stores for Computed Values\n// ✅ CORRECT: Derived store\nexport const visibleDuration = derived(\n  timeRange,\n  ($tr) =&gt; $tr.end - $tr.start\n);\n\n// ❌ WRONG: Manual recomputation\nexport const visibleDuration = writable(5);\ntimeRange.subscribe($tr =&gt; {\n  visibleDuration.set($tr.end - $tr.start);\n});\n\n\n3. Call saveUndo() Before Mutations\n// ✅ CORRECT\nexport function addBoundary(tierIndex: number, time: number): void {\n  saveUndo();  // Save BEFORE\n  tiers.update(/* ... */);\n}\n\n// ❌ WRONG\nexport function addBoundary(tierIndex: number, time: number): void {\n  tiers.update(/* ... */);\n  saveUndo();  // Too late\n}\n\n\n4. Keep Stores Focused\nGood: - audio.ts - Only audio data - view.ts - Only viewport state - analysis.ts - Only acoustic results\nBad: - appState.ts - Everything in one giant store\n\n\n5. Use Derived Stores to Avoid Duplication\n// ✅ CORRECT: Single source of truth\nexport const timeRange = writable({ start: 0, end: 5 });\nexport const visibleDuration = derived(timeRange, $tr =&gt; $tr.end - $tr.start);\n\n// ❌ WRONG: Duplicate state (can get out of sync)\nexport const timeRange = writable({ start: 0, end: 5 });\nexport const visibleDuration = writable(5);  // Must manually sync\n\n\n6. Initialize Stores at App Level\n&lt;!-- +layout.svelte or +page.svelte --&gt;\n&lt;script&gt;\n  import { onMount } from 'svelte';\n  import { initUndoManager } from '$lib/stores/undoManager';\n  import { tiers } from '$lib/stores/annotations';\n  import { dataPoints } from '$lib/stores/dataPoints';\n\n  onMount(() =&gt; {\n    initUndoManager(tiers, dataPoints);\n  });\n&lt;/script&gt;\n\n\n7. Handle Null/Undefined Gracefully\n&lt;script&gt;\n  import { audioBuffer, analysisResults } from '$lib/stores/...';\n\n  // ✅ CORRECT: Check before use\n  $: if ($audioBuffer && $analysisResults) {\n    renderVisualization($audioBuffer, $analysisResults);\n  }\n&lt;/script&gt;\n\n&lt;!-- ✅ CORRECT: Conditional rendering --&gt;\n{#if $audioBuffer}\n  &lt;Waveform /&gt;\n{/if}\n\n{#if $analysisResults}\n  &lt;Spectrogram /&gt;\n{/if}",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#store-initialization-order",
    "href": "development/stores.html#store-initialization-order",
    "title": "Stores",
    "section": "Store Initialization Order",
    "text": "Store Initialization Order\nTypical initialization flow:\n1. App loads (+layout.svelte)\n   ↓\n2. Initialize WASM backend (acoustic.ts)\n   ↓\n3. Initialize undo manager\n   ↓\n4. Load config.yaml (if present)\n   ↓\n5. Check URL parameters (viewer route)\n   ↓\n6. Load audio from URL if specified\n   ↓\n7. Run analysis\n   ↓\n8. Render components\nCode:\n&lt;!-- +layout.svelte --&gt;\n&lt;script&gt;\n  import { onMount } from 'svelte';\n  import { initWasm } from '$lib/wasm/acoustic';\n  import { initUndoManager } from '$lib/stores/undoManager';\n  import { loadConfigFromYAML } from '$lib/stores/config';\n\n  onMount(async () =&gt; {\n    // 1. Initialize WASM\n    await initWasm('praatfan-local');\n\n    // 2. Initialize undo\n    initUndoManager(tiers, dataPoints);\n\n    // 3. Load config if present\n    try {\n      await loadConfigFromYAML('/config.yaml');\n    } catch (e) {\n      console.log('No config.yaml, using defaults');\n    }\n  });\n&lt;/script&gt;",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#debugging-stores",
    "href": "development/stores.html#debugging-stores",
    "title": "Stores",
    "section": "Debugging Stores",
    "text": "Debugging Stores\n\nLog Store Changes\n&lt;script&gt;\n  import { cursorPosition } from '$lib/stores/view';\n\n  // Debug: log every change\n  $: console.log('Cursor:', $cursorPosition);\n\n  // Debug: log specific conditions\n  $: if ($cursorPosition &gt; 5) {\n    console.warn('Cursor past 5 seconds');\n  }\n&lt;/script&gt;\n\n\nUse Svelte DevTools\nBrowser extension: Install Svelte DevTools (Chrome/Firefox)\nFeatures: - Inspect current store values - See which components are subscribed - Track state changes over time\n\n\nManual Subscription\nimport { cursorPosition } from '$lib/stores/view';\n\n// Debug: monitor in console\nconst unsubscribe = cursorPosition.subscribe(value =&gt; {\n  console.log('Cursor changed:', value);\n});\n\n// Remember to unsubscribe when done\nunsubscribe();",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#common-patterns",
    "href": "development/stores.html#common-patterns",
    "title": "Stores",
    "section": "Common Patterns",
    "text": "Common Patterns\n\nPattern: Conditional Analysis\n// Run analysis only if audio is short enough\n$: if ($audioBuffer) {\n  if ($duration &lt;= MAX_ANALYSIS_DURATION) {\n    runAnalysis();\n  } else {\n    console.log('Audio too long, waiting for zoom');\n  }\n}\n\n\nPattern: Synchronized Cursors\n// Waveform sets hover position\nfunction handleMouseMove(e: MouseEvent) {\n  const time = pixelToTime(e.offsetX);\n  hoverPosition.set(time);\n}\n\n// Spectrogram reacts to hover\n$: if ($hoverPosition !== null) {\n  drawHoverCursor($hoverPosition);\n}\n\n\nPattern: Debounced Analysis\nimport { debounce } from 'lodash-es';\n\n// Avoid excessive recomputation during smooth zoom\nconst runAnalysisDebounced = debounce(async () =&gt; {\n  await runAnalysisForRange($timeRange.start, $timeRange.end);\n}, 300);\n\n$: if ($timeRange && $duration &gt; MAX_ANALYSIS_DURATION) {\n  const visibleDuration = $timeRange.end - $timeRange.start;\n  if (visibleDuration &lt;= MAX_ANALYSIS_DURATION) {\n    runAnalysisDebounced();\n  }\n}\n\n\nPattern: Store Reset on File Load\nasync function loadNewAudio(file: File) {\n  // Clear old state\n  analysisResults.set(null);\n  tiers.set([]);\n  dataPoints.set([]);\n  clearUndoHistory();\n\n  // Load new audio\n  const decoded = await decodeAudio(file);\n  audioBuffer.set(decoded.samples);\n  // ...\n}",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "development/stores.html#see-also",
    "href": "development/stores.html#see-also",
    "title": "Stores",
    "section": "See Also",
    "text": "See Also\n\nArchitecture - System design overview\nWASM Integration - Analysis backend details\nSvelte Store Tutorial - Official docs\nSvelte Store API - API reference",
    "crumbs": [
      "Development",
      "Stores"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ozen-web",
    "section": "",
    "text": "Browser-based acoustic analysis and annotation\nA powerful, browser-based tool for phonetic research, built with Svelte and WebAssembly. Analyze pitch, formants, intensity, and more, with various supporting backends.\n\nGet Started View Tutorial Try It"
  },
  {
    "objectID": "index.html#key-features",
    "href": "index.html#key-features",
    "title": "Ozen-web",
    "section": "Key Features",
    "text": "Key Features\n\n\n📊 Phonetic Analysis\nPowered by praatfan_rust WebAssembly to provide acoustic measurements: pitch (F0), formants (F1-F4), intensity, HNR, center of gravity, spectral tilt, and more.\n\n\nInteractive Annotation\nFull TextGrid support with multi-tier annotations. Add boundaries, edit labels, snap to higher layer boundaries, with keyboard shortcuts for efficient workflow.\n\n\nMobile-Optimized\nAn alternative touch-friendly viewer-only front end with pinch-to-zoom, drag-to-select, and responsive layout. Aimed for phone-based access to phonetic analysis and embedding in presentations.\n\n\nWorks Offline\nRuns entirely in a browser. No external server required, no data uploaded.\n\n\nData Collection\nDouble-click to add points of intest for data collection with acoustic measurements. Export to TSV with all values and annotation labels for statistical analysis, or ctrl-C / command-C to copy to clipboard.\n\n\nHighly Embeddable\nEmbed the viewer in Quarto documents, R Markdown, Jupyter notebooks, or any web page. URL parameters for pre-configuration and data URL support for self-contained embeds."
  },
  {
    "objectID": "index.html#quick-start",
    "href": "index.html#quick-start",
    "title": "Ozen-web",
    "section": "Quick Start",
    "text": "Quick Start\n\nUse the live version\nOr download the docs/live/ directory to directory on your server, live/\nImportant: Ozen-web will not work if you just open a local html file in your browser, it has to be served from a web server. The web server can be local though (python has python -m   http.server). If this sounds complicated, try the python interface\n\nSee the Getting Started Guide for detailed installation instructions."
  },
  {
    "objectID": "index.html#live-demo",
    "href": "index.html#live-demo",
    "title": "Ozen-web",
    "section": "Live Demo",
    "text": "Live Demo\nTry the interactive viewer below. This example shows a spectrogram with pitch and formant overlays:\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe embedded viewer requires audio to be loaded. Click “Load Audio”, record yourself, or drag a WAV file to explore features."
  },
  {
    "objectID": "index.html#use-cases",
    "href": "index.html#use-cases",
    "title": "Ozen-web",
    "section": "Use Cases",
    "text": "Use Cases\n\nPhonetics Research\n\nMeasure vowel formants, analyze intonation contours, annotate phonetic transcription\n\nSpeech Pathology\n\nVisualize voice quality measures (HNR, spectral tilt), track therapy progress\n\nLanguage Documentation\n\nTranscribe endangered languages, create time-aligned corpora, work offline in the field with laptop or tablet.\n\nTeaching & Presentations\n\nEmbed interactive spectrograms in lecture slides, Quarto documents, or course websites. Students can explore audio without installing software.\n\nPodcast & Audio Production\n\nVisualize speech clarity, identify noise regions, mark edit points with sub-second accuracy."
  },
  {
    "objectID": "index.html#browser-compatibility",
    "href": "index.html#browser-compatibility",
    "title": "Ozen-web",
    "section": "Browser Compatibility",
    "text": "Browser Compatibility\nShould work on most modern web browsers"
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Ozen-web",
    "section": "Next Steps",
    "text": "Next Steps\n\nNew to Ozen-web? → Start with the Getting Started Guide\nWant a guided walkthrough? → Follow the Complete Tutorial\nLooking for specific features? → Browse Feature Documentation\nEmbedding in a website? → Check the Embedding Guide\nContributing to development? → Read Development Setup"
  },
  {
    "objectID": "index.html#related-projects",
    "href": "index.html#related-projects",
    "title": "Ozen-web",
    "section": "Related Projects",
    "text": "Related Projects\n\nozen — Desktop version (Python/PyQt6) with additional export formats and batch processing\npraatfan-core-rs — Rust acoustic analysis library powering Ozen-web"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Ozen-web",
    "section": "License",
    "text": "License\nMIT License — free for research, teaching, and commercial use.\nWASM backend options include MIT/Apache-2.0 (default) and GPL-licensed variants. See WASM Backends Reference for details.\n\n\nBuilt with Svelte • SvelteKit • praatfan • TypeScript"
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html",
    "href": "tutorial/03-acoustic-analysis.html",
    "title": "3. Acoustic Analysis",
    "section": "",
    "text": "Ozen-web can display multiple acoustic features overlaid on the spectrogram:\n\nPitch (F0) — Fundamental frequency (blue line and dots)\nFormants (F1-F4) — Resonance frequencies (red dots)\nIntensity — Loudness over time (green line)\nHNR — Voice quality (harmonicity)\nCoG — Spectral center of gravity\nSpectral Tilt — High/low frequency balance\nA1-P0 — Nasal formant measure\n\nIn this section, you’ll learn to enable these overlays and interpret what they show.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#overview",
    "href": "tutorial/03-acoustic-analysis.html#overview",
    "title": "3. Acoustic Analysis",
    "section": "",
    "text": "Ozen-web can display multiple acoustic features overlaid on the spectrogram:\n\nPitch (F0) — Fundamental frequency (blue line and dots)\nFormants (F1-F4) — Resonance frequencies (red dots)\nIntensity — Loudness over time (green line)\nHNR — Voice quality (harmonicity)\nCoG — Spectral center of gravity\nSpectral Tilt — High/low frequency balance\nA1-P0 — Nasal formant measure\n\nIn this section, you’ll learn to enable these overlays and interpret what they show.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#the-overlay-controls",
    "href": "tutorial/03-acoustic-analysis.html#the-overlay-controls",
    "title": "3. Acoustic Analysis",
    "section": "The Overlay Controls",
    "text": "The Overlay Controls\nLook for the overlay checkboxes in the interface (usually in a sidebar or below the spectrogram).\n\n\n\nOverlay toggle controls",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#enabling-pitch",
    "href": "tutorial/03-acoustic-analysis.html#enabling-pitch",
    "title": "3. Acoustic Analysis",
    "section": "Enabling Pitch",
    "text": "Enabling Pitch\nPitch (fundamental frequency) is the most commonly used overlay.\n\n\nCheck the “Pitch” checkbox\nWait 1-2 seconds for computation (first time only)\nA blue line with dots appears on the spectrogram\n\n\n\nSpectrogram with pitch overlay\n\n\nEach dot represents one pitch measurement\nVertical position = frequency (Hz)\n\n\n\nInterpreting the Pitch Overlay\n\nHigher on screen = higher pitch\nLower on screen = lower pitch\nGaps in the line = unvoiced sounds ([s], [f], [t], [k], etc., or silence) or pitch detection failures\nSmooth contours = steady pitch (often vowels)\nSharp movements = pitch changes (intonation / tone)\n\n\nTry this: Record yourself saying “really?!” and watch the pitch track follow your intonation.\n\n\n\n\n\n\n\nNote\n\n\n\nPitch is only detected in voiced segments. Consonants like [s], [f], [θ] won’t show pitch values.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#enabling-formants",
    "href": "tutorial/03-acoustic-analysis.html#enabling-formants",
    "title": "3. Acoustic Analysis",
    "section": "Enabling Formants",
    "text": "Enabling Formants\nFormants show the resonance frequencies that distinguish vowels.\n\n\nCheck the “Formants” checkbox\nRed dots appear on the spectrogram\n\n\n\nSpectrogram with formants overlay\n\n\nFour vertical series of dots represent F1 (lowest), F2, F3, F4 (highest)\n\n\n\nInterpreting the Formant Overlay\nFormants reveal vowel identity:\n\n\n\nVowel\nAppearance on Spectrogram\n\n\n\n\n\n\n[i] peel\nF1 low, F2 very high\n\n\n\n\n[ʊ] pull\nF1 low (but not as low), F2 low\n\n\n\n\n[ɔ] Paul\nF1 high, F2 low\n\n\n\n\n[æ] pal\nF1 high, F2 high\n\n\n\n\n[ɜ˞] pearl\nF1 mid, F2 high, F3 low\n\n\n\n\n\n\nTry this: Find a vowel in your audio, place the cursor in the middle, and read F1 and F2 values in the values panel. Compare to the table above.\n\n\n\n\n\n\n\nTip\n\n\n\nF1 roughly corresponds to tongue height (high = low F1, low = high F1). F2 roughly corresponds to tongue frontness (front = high F2, back = low F2).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#enabling-intensity",
    "href": "tutorial/03-acoustic-analysis.html#enabling-intensity",
    "title": "3. Acoustic Analysis",
    "section": "Enabling Intensity",
    "text": "Enabling Intensity\nIntensity shows loudness over time.\n\n\nCheck the “Intensity” checkbox\nA green line appears, usually in the upper part of the spectrogram\n\n\n\nSpectrogram with intensity overlay\n\n\nHeight corresponds to decibels (dB)\n\n\n\nInterpreting Intensity\n\nHigher = louder segments (stressed syllables, vowels)\nLower = quieter segments (unstressed syllables, consonants)\nDrops to bottom = silence\n\n\nTry this: Say a sentence with strong stress on one word. The intensity overlay will peak on the stressed syllable.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#enabling-advanced-overlays",
    "href": "tutorial/03-acoustic-analysis.html#enabling-advanced-overlays",
    "title": "3. Acoustic Analysis",
    "section": "Enabling Advanced Overlays",
    "text": "Enabling Advanced Overlays\n\nHNR (Harmonics-to-Noise Ratio)\nMeasures voice quality:\n\nHigh HNR (&gt;10 dB) = clear, modal voice\nLow HNR (&lt;5 dB) = breathy or creaky voice\nVery low HNR = whisper or noise\n\n\n\nCheck the “HNR” checkbox\nOrange/yellow line appears\nHigher = more harmonic (less noisy)\n\n\n\n\nCoG (Center of Gravity)\nMeasures spectral balance:\n\nHigh CoG (&gt;5000 Hz) = sibilants (s, sh), fricatives\nLow CoG (&lt;3000 Hz) = vowels, sonorants\n\nUseful for distinguishing fricatives. To really see how fricatives are different, increase “Max Freq.” to 10,000, and record the word chefs.\n\n\nSpectral Tilt\nMeasures high vs. low frequency emphasis:\n\nPositive tilt = more energy in low frequencies (vowels)\nNegative tilt = more energy in high frequencies (fricatives)\n\n\n\nA1-P0\nA measure related to nasality and open quotient. Useful for advanced research.\n\n\n\n\n\n\nNote\n\n\n\nAdvanced overlays (HNR, CoG, Spectral Tilt, A1-P0) are primarily useful for specialized phonetic research. Most users only need Pitch, Formants, and Intensity.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#viewing-all-overlays-together",
    "href": "tutorial/03-acoustic-analysis.html#viewing-all-overlays-together",
    "title": "3. Acoustic Analysis",
    "section": "Viewing All Overlays Together",
    "text": "Viewing All Overlays Together\nYou can enable multiple overlays simultaneously:\n\n\n\nAll overlays enabled\n\n\n\n\n\n\n\n\nWarning\n\n\n\nEnabling too many overlays at once can make the display cluttered. Start with Pitch + Formants for most tasks.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#reading-values-at-the-cursor",
    "href": "tutorial/03-acoustic-analysis.html#reading-values-at-the-cursor",
    "title": "3. Acoustic Analysis",
    "section": "Reading Values at the Cursor",
    "text": "Reading Values at the Cursor\nThe values panel shows precise measurements at the cursor:\n\n\nEnable the overlays you’re interested in (e.g., Pitch and Formants)\nPlace the cursor in a vowel\nRead the values panel:\n\nTime: 0.452 s\nFreq: 1234 Hz (where you clicked)\nPitch: 234 Hz\nIntensity: 68 dB\nF1: 523 Hz\nF2: 1987 Hz\nF3: 2743 Hz\nF4: 3543 Hz\n\n\n\n\nValues panel with measurements\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can collect these measurements systematically using data points (covered in section 5).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#customizing-analysis-settings",
    "href": "tutorial/03-acoustic-analysis.html#customizing-analysis-settings",
    "title": "3. Acoustic Analysis",
    "section": "Customizing Analysis Settings",
    "text": "Customizing Analysis Settings\nYou can adjust analysis parameters via the settings panel (if available) or config.yaml:\nPitch settings:\n\npitchFloor: Minimum pitch to detect (default: 75 Hz)\npitchCeiling: Maximum pitch to detect (default: 600 Hz)\n\nFormant settings:\n\nmaxFormants: Number of formants to track (default: 5)\nmaxFormantFrequency: Analysis ceiling (default: 5500 Hz for female voices, 5000 Hz for males)\n\nSee Configuration Reference for all options.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#changing-spectrogram-max-frequency",
    "href": "tutorial/03-acoustic-analysis.html#changing-spectrogram-max-frequency",
    "title": "3. Acoustic Analysis",
    "section": "Changing Spectrogram Max Frequency",
    "text": "Changing Spectrogram Max Frequency\nYou may want to adjust the vertical range of the spectrogram:\n\n\nFind the Max Frequency dropdown (usually in toolbar)\nChoose a ceiling:\n\n5 kHz — Good for speech (default)\n7.5 kHz — Includes higher formants\n10 kHz — Full range, useful for fricatives\n\nThe spectrogram y-axis rescales\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor most speech analysis, 5 kHz is sufficient. Use 7.5-10 kHz if studying sibilants or children’s voices.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#practice-exercises",
    "href": "tutorial/03-acoustic-analysis.html#practice-exercises",
    "title": "3. Acoustic Analysis",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nEnable Pitch only\n\nFind a vowel\nPlace cursor in the middle\nNote the pitch value\n\nEnable Formants\n\nIdentify F1 and F2 for the same vowel\nTry to guess the vowel based on formants (use the table above)\n\nEnable Intensity\n\nFind the loudest part of your audio\nCompare intensity values across different sounds\n\nEnable all overlays\n\nObserve how different overlays align\nNotice which features co-vary (e.g., intensity and voicing)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#troubleshooting",
    "href": "tutorial/03-acoustic-analysis.html#troubleshooting",
    "title": "3. Acoustic Analysis",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nPitch overlay doesn’t appear:\n\nEnsure WASM backend is loaded (check backend selector)\nTry enabling/disabling the checkbox\nCheck browser console (F12) for errors\n\nPitch values seem wrong:\n\nAdjust pitchFloor and pitchCeiling for your speaker\nMale voices: Try 50-300 Hz range\nFemale voices: Try 100-500 Hz range\nChildren: Try 150-600 Hz range\n\nFormants are missing or erratic:\n\nEnsure you’re looking at a vowel (not a consonant)\nTry adjusting maxFormantFrequency (5500 for females, 5000 for males)\nZoom in to see individual formant dots more clearly\n\nToo many overlays, display is cluttered:\n\nDisable overlays you’re not actively using\nUse the values panel to read measurements instead of viewing all overlays\n\nOverlays don’t update when zooming:\n\nThis is expected for files &gt;60s — analysis recomputes when zoomed\nWait 1-2 seconds after zooming for the update",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/03-acoustic-analysis.html#whats-next",
    "href": "tutorial/03-acoustic-analysis.html#whats-next",
    "title": "3. Acoustic Analysis",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow that you can visualize acoustic features, let’s learn how to create annotations to mark boundaries and add labels.\nNext: 4. Annotations →\n\nNavigation: ← Previous: Exploring Audio | Tutorial Overview | Next: Annotations →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "3. Acoustic Analysis"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html",
    "href": "tutorial/06-exporting.html",
    "title": "6. Exporting",
    "section": "",
    "text": "After annotating audio and collecting data points, you’ll want to export your work for:\n\nLong-term storage (save your annotations)\nFurther analysis (statistical analysis in R, Python, Praat)\nSharing (collaborate with colleagues)\n\nOzen-web supports three export formats:\n\nTextGrid (Praat format) — For annotations\nTSV (Tab-Separated Values) — For data points + measurements\nWAV (Audio) — For saving recorded or edited audio",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#overview",
    "href": "tutorial/06-exporting.html#overview",
    "title": "6. Exporting",
    "section": "",
    "text": "After annotating audio and collecting data points, you’ll want to export your work for:\n\nLong-term storage (save your annotations)\nFurther analysis (statistical analysis in R, Python, Praat)\nSharing (collaborate with colleagues)\n\nOzen-web supports three export formats:\n\nTextGrid (Praat format) — For annotations\nTSV (Tab-Separated Values) — For data points + measurements\nWAV (Audio) — For saving recorded or edited audio",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#exporting-textgrids",
    "href": "tutorial/06-exporting.html#exporting-textgrids",
    "title": "6. Exporting",
    "section": "Exporting TextGrids",
    "text": "Exporting TextGrids\nTextGrid is the standard format for time-aligned annotations, compatible with Praat.\n\n\nCreate some annotations (section 4) if you haven’t already\nClick “Export TextGrid” button (usually in toolbar or File menu)\n\n\n\nTextGrid export dialog\n\n\nA file download dialog appears (or File System Access API prompt)\nChoose a location and filename (e.g., my-annotation.TextGrid)\nClick “Save”\nThe TextGrid file is saved to your computer\n\n\n\nTextGrid File Contents\nA TextGrid file contains:\n\nAll annotation tiers (interval and point tiers)\nAll boundaries and their time positions\nAll interval labels\n\nExample TextGrid content:\nFile type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\nxmin = 0\nxmax = 2.451\ntiers? &lt;exists&gt;\nsize = 2\n\nitem [1]:\n    class = \"IntervalTier\"\n    name = \"words\"\n    xmin = 0\n    xmax = 2.451\n    intervals: size = 3\n        intervals [1]:\n            xmin = 0\n            xmax = 0.523\n            text = \"hello\"\n        intervals [2]:\n            xmin = 0.523\n            xmax = 1.234\n            text = \"world\"\n        intervals [3]:\n            xmin = 1.234\n            xmax = 2.451\n            text = \"\"\n\n\nOpening TextGrids in Praat\n\n\nOpen Praat\nGo to Read → Read from file…\nSelect your TextGrid file\nOpen the corresponding audio file\nSelect both (Shift+click) and choose View & Edit\nYour annotations appear in Praat’s editor\n\n\n\n\n\n\n\n\nTip\n\n\n\nKeep audio and TextGrid files in the same directory with matching names (e.g., audio.wav and audio.TextGrid) for easy loading in Praat.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#importing-textgrids",
    "href": "tutorial/06-exporting.html#importing-textgrids",
    "title": "6. Exporting",
    "section": "Importing TextGrids",
    "text": "Importing TextGrids\nYou can also import existing TextGrid files into Ozen-web:\n\n\nLoad an audio file first (section 1)\nClick “Import TextGrid” button\nSelect a TextGrid file\nThe tiers and annotations appear in Ozen-web\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe TextGrid must match the audio duration. If durations don’t match, some annotations may be truncated or extended.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#exporting-data-points-tsv",
    "href": "tutorial/06-exporting.html#exporting-data-points-tsv",
    "title": "6. Exporting",
    "section": "Exporting Data Points (TSV)",
    "text": "Exporting Data Points (TSV)\nTSV (Tab-Separated Values) files are perfect for statistical analysis in R, Python, SPSS, or Excel.\n\n\nAdd some data points (section 5) if you haven’t already\nClick “Export Data Points” or “Export TSV” button\n\n\n\nTSV export menu\n\n\nChoose a location and filename (e.g., vowel-data.tsv)\nClick “Save”\nThe TSV file is saved\n\n\n\nTSV File Contents\nA TSV file contains one row per data point with columns for:\n\ntime — Time position (seconds)\nfrequency — Frequency where you clicked (Hz)\npitch — Fundamental frequency (Hz)\nintensity — Intensity (dB)\nf1, f2, f3, f4 — Formant frequencies (Hz)\nb1, b2, b3, b4 — Formant bandwidths (Hz)\nhnr — Harmonics-to-Noise Ratio (dB)\ncog — Center of Gravity (Hz)\nspectral_tilt — Spectral tilt (dB/octave)\na1_p0 — A1-P0 measure\n[tier_name] — One column per annotation tier (labels at that time)\n\nExample TSV:\ntime    frequency   pitch   intensity   f1  f2  f3  f4  phones  words\n0.234   1500    156 72  678 1234    2890    3654    i   see\n0.567   1200    234 68  543 987 2567    3234    u   two\n0.891   1400    198 71  698 1324    2678    3456    æ   cats\n\n\nAnalyzing TSV Data\nIn R:\n# Read data\ndata &lt;- read.delim(\"vowel-data.tsv\")\n\n# Plot F1 vs F2\nlibrary(ggplot2)\nggplot(data, aes(x = f2, y = f1, label = phones)) +\n  geom_text() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  labs(title = \"Vowel Space\", x = \"F2 (Hz)\", y = \"F1 (Hz)\")\nIn Python:\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read data\ndata = pd.read_csv('vowel-data.tsv', sep='\\t')\n\n# Plot F1 vs F2\nplt.figure(figsize=(8, 6))\nfor i, row in data.iterrows():\n    plt.text(row['f2'], row['f1'], row['phones'])\nplt.gca().invert_xaxis()\nplt.gca().invert_yaxis()\nplt.xlabel('F2 (Hz)')\nplt.ylabel('F1 (Hz)')\nplt.title('Vowel Space')\nplt.show()\nIn Excel:\n\nOpen Excel\nFile → Open → Select TSV file\nData imports into columns\nCreate scatter plot (F1 vs F2)\nAdd data labels (vowel symbols)\n\n\nTry this: Export your data points, open in Excel, and create a simple scatter plot of F1 (y-axis) vs F2 (x-axis). Each point represents a vowel!",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#importing-data-points-tsv",
    "href": "tutorial/06-exporting.html#importing-data-points-tsv",
    "title": "6. Exporting",
    "section": "Importing Data Points (TSV)",
    "text": "Importing Data Points (TSV)\nYou can also import previously exported TSV files:\n\n\nLoad the same audio file that you used when creating the data points\nClick “Import Data Points” or “Import TSV”\nSelect your TSV file\nData points appear on the spectrogram\n\n\n\n\n\n\n\n\nWarning\n\n\n\nTSV import requires matching audio files. If the audio is different, time positions may not align correctly.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#exporting-audio",
    "href": "tutorial/06-exporting.html#exporting-audio",
    "title": "6. Exporting",
    "section": "Exporting Audio",
    "text": "Exporting Audio\nSave the current audio (useful after recording or trimming):\n\n\nClick “Export Audio” or “Save Audio” button\nChoose format (usually WAV, 16-bit PCM)\nChoose location and filename\nClick “Save”\nAudio is saved as a WAV file\n\n\n\n\n\n\n\n\nNote\n\n\n\nExported audio is always in WAV format (16-bit PCM, original sample rate). This ensures compatibility with Praat and other analysis software.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#file-system-access-api-vs.-downloads",
    "href": "tutorial/06-exporting.html#file-system-access-api-vs.-downloads",
    "title": "6. Exporting",
    "section": "File System Access API vs. Downloads",
    "text": "File System Access API vs. Downloads\nOzen-web uses two methods for saving files, depending on browser support:\nFile System Access API (Chrome, Edge):\n\nShows native “Save” dialog\nYou choose exact location and filename\nCan overwrite existing files\nFeels like a desktop app\n\nDownload API (Firefox, Safari, older browsers):\n\nFiles save to Downloads folder\nBrowser may add numbers (e.g., data(1).tsv, data(2).tsv)\nYou need to move files manually\n\n\n\n\n\n\n\nTip\n\n\n\nFor best experience, use Chrome or Edge, which support the File System Access API.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#recommended-workflow",
    "href": "tutorial/06-exporting.html#recommended-workflow",
    "title": "6. Exporting",
    "section": "Recommended Workflow",
    "text": "Recommended Workflow\nFor a complete research workflow:\n\n\nLoad audio (section 1)\nCreate annotations with multiple tiers (section 4)\n\nWords tier\nPhones tier\nAny other tiers needed\n\nEnable acoustic overlays (section 3)\n\nPitch, Formants, Intensity\n\nAdd data points at measurement locations (section 5)\n\nVowel midpoints\nConsonant landmarks\nPitch points\n\nExport TextGrid for annotations\n\nSaves all tiers and labels\nCompatible with Praat\n\nExport TSV for acoustic measurements\n\nSaves all data points with values\nReady for statistical analysis\n\nOrganize your files:\nmy-project/\n├── audio.wav\n├── audio.TextGrid\n└── data-points.tsv\n\n\n\n\n\n\n\n\nTip\n\n\n\nExport frequently! Browser tabs can close unexpectedly. Save your work every 10-15 minutes.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#batch-processing-multiple-files",
    "href": "tutorial/06-exporting.html#batch-processing-multiple-files",
    "title": "6. Exporting",
    "section": "Batch Processing Multiple Files",
    "text": "Batch Processing Multiple Files\nFor analyzing many files:\n\n\nAnalyze first file (load, annotate, collect data, export)\nLoad second file\nRepeat annotation and data collection\nExport each file’s TextGrid and TSV\nCombine all TSV files in R or Python:\n\nR:\nfiles &lt;- list.files(pattern = \"*.tsv\")\ndata &lt;- do.call(rbind, lapply(files, read.delim))\nPython:\nimport pandas as pd\nimport glob\n\nfiles = glob.glob(\"*.tsv\")\ndata = pd.concat([pd.read_csv(f, sep='\\t') for f in files])\n\n\n\n\n\n\n\nNote\n\n\n\nOzen-web doesn’t have built-in batch processing. For large corpora, consider using the desktop version ozen or scripting with Praat.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#practice-exercises",
    "href": "tutorial/06-exporting.html#practice-exercises",
    "title": "6. Exporting",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nExport and re-import a TextGrid\n\nCreate annotations\nExport TextGrid\nLoad a new audio file\nLoad the same audio again\nImport the TextGrid\nVerify annotations reappear\n\nExport data points and open in Excel\n\nCreate 3 data points\nExport TSV\nOpen in Excel or Google Sheets\nVerify all columns are present\n\nAnalyze in R or Python (if you know these languages)\n\nExport TSV with formant data\nPlot F1 vs F2\nColor-code by vowel label\n\n\n\nChallenge: Create a complete mini-dataset:\n\nRecord or load 3 short sentences\nAnnotate each with a “words” tier\nAdd data points on all vowels\nExport 3 TextGrids and 3 TSVs\nCombine TSVs in Excel/R/Python\nPlot a vowel space with all data",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#troubleshooting",
    "href": "tutorial/06-exporting.html#troubleshooting",
    "title": "6. Exporting",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n“Save” button doesn’t appear:\n\nCheck browser permissions (allow file system access)\nTry using a different browser (Chrome/Edge recommended)\n\nFile saves to Downloads instead of chosen location:\n\nYour browser doesn’t support File System Access API\nThis is normal for Firefox/Safari; manually move files after download\n\nTextGrid won’t open in Praat:\n\nEnsure the file extension is .TextGrid (not .txt)\nCheck that the file isn’t empty (open in text editor)\nTry re-exporting from Ozen-web\n\nTSV columns are misaligned in Excel:\n\nEnsure file extension is .tsv (not .txt)\nWhen opening in Excel, choose “Tab” as delimiter\nOr use “Text to Columns” feature after opening\n\nImported TextGrid doesn’t match audio:\n\nVerify audio duration matches TextGrid duration\nCheck that you’re importing the correct TextGrid for this audio file\n\nData points export is empty:\n\nEnsure you’ve added data points (section 5)\nCheck that acoustic overlays are enabled\nVerify data points appear visually on the spectrogram",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/06-exporting.html#whats-next",
    "href": "tutorial/06-exporting.html#whats-next",
    "title": "6. Exporting",
    "section": "What’s Next?",
    "text": "What’s Next?\nCongratulations! 🎉 You’ve completed the Ozen-web tutorial. You now know how to:\n\n✅ Load audio files\n✅ Navigate and explore audio\n✅ View acoustic features\n✅ Create multi-tier annotations\n✅ Collect acoustic measurements\n✅ Export data for analysis\n\n\nContinue Learning\n\nFeatures Documentation — Deep dive into all features\nKeyboard Shortcuts Reference — Speed up your workflow\nConfiguration Guide — Customize default settings\nEmbedding Guide — Embed Ozen-web in websites or documents\n\n\n\nGet Involved\n\nReport bugs: GitHub Issues\nRequest features: GitHub Discussions\nContribute: Development Guide\n\n\nNavigation: ← Previous: Data Collection | Tutorial Overview | Features Documentation →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "6. Exporting"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html",
    "href": "tutorial/01-loading-audio.html",
    "title": "1. Loading Audio",
    "section": "",
    "text": "In this section, you’ll learn how to load audio files into Ozen-web using three different methods:\n\nDrag and drop (fastest)\nFile picker (traditional)\nMicrophone recording (create new audio)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#overview",
    "href": "tutorial/01-loading-audio.html#overview",
    "title": "1. Loading Audio",
    "section": "",
    "text": "In this section, you’ll learn how to load audio files into Ozen-web using three different methods:\n\nDrag and drop (fastest)\nFile picker (traditional)\nMicrophone recording (create new audio)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#before-you-start",
    "href": "tutorial/01-loading-audio.html#before-you-start",
    "title": "1. Loading Audio",
    "section": "Before You Start",
    "text": "Before You Start\nMake sure Ozen-web is running in your browser:\n\nHosted version\nLocal server: http://localhost:5173 (change the port to number to match your server)\n\nYou should see an empty interface with a prominent file drop zone.\n\n\n\nEmpty interface with drop zone",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#method-1-drag-and-drop",
    "href": "tutorial/01-loading-audio.html#method-1-drag-and-drop",
    "title": "1. Loading Audio",
    "section": "Method 1: Drag and Drop",
    "text": "Method 1: Drag and Drop\nThe fastest way to load audio is drag-and-drop:\n\n\nFind an audio file on your computer (WAV, MP3, or OGG format)\nDrag the file over the Ozen-web window\nThe drop zone will highlight with a blue border\n\n\n\nDrag-drop highlighted\n\n\nRelease the mouse button\nWait for loading (1-2 seconds)\n\nProgress indicator appears briefly\nWaveform appears in the display\nSpectrogram loads (or prompts to zoom for long files)\n\n\n\n\nAudio loaded successfully\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can drag files from Finder (Mac), File Explorer (Windows), or Nautilus/Dolphin (Linux).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#method-2-file-picker",
    "href": "tutorial/01-loading-audio.html#method-2-file-picker",
    "title": "1. Loading Audio",
    "section": "Method 2: File Picker",
    "text": "Method 2: File Picker\nIf you prefer a traditional file dialog:\n\n\nClick the “Load Audio” button in the interface (usually top-left or center)\nSelect your audio file in the file dialog\nClick “Open”\nAudio loads just like drag-and-drop\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe file picker shows only supported formats (WAV, FLAC, MP3, OGG). If your file doesn’t appear, check the format or try “All Files” filter.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#method-3-microphone-recording",
    "href": "tutorial/01-loading-audio.html#method-3-microphone-recording",
    "title": "1. Loading Audio",
    "section": "Method 3: Microphone Recording",
    "text": "Method 3: Microphone Recording\nCreate new audio directly in the browser:\n\n\nClick the microphone icon 🎤 (usually in the toolbar)\nAllow microphone access when prompted by the browser\n\n\n\nBrowser microphone permission dialog\n\n\nClick the red record button\nSpeak into your microphone\n\nA timer shows recording duration\nMaximum recording time: 60 seconds (configurable)\n\nClick the stop button when finished\nThe recorded audio loads automatically\n\n\n\nTry recording yourself saying a few words that differ only by their vowels (bead, bid, bed, bad) to see clear formant patterns in the spectrogram.\n\n\n\n\n\n\n\nWarning\n\n\n\nMicrophone recording uses your browser’s MediaRecorder API. Quality depends on your microphone and browser settings. For research-grade recordings, use external recording software.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#what-you-should-see",
    "href": "tutorial/01-loading-audio.html#what-you-should-see",
    "title": "1. Loading Audio",
    "section": "What You Should See",
    "text": "What You Should See\nAfter loading audio successfully, you should see:\n\nWaveform display showing amplitude over time\nTime axis at the bottom (0.0s, 0.5s, 1.0s, …)\nFilename displayed in the interface\nSpectrogram (for files ≤60s) or “Zoom in for spectrogram” message\n\n\n\n\nSuccessful audio load",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#supported-audio-formats",
    "href": "tutorial/01-loading-audio.html#supported-audio-formats",
    "title": "1. Loading Audio",
    "section": "Supported Audio Formats",
    "text": "Supported Audio Formats\nOzen-web supports most common audio formats via the Web Audio API:\n\n\n\n\n\n\n\n\nFormat\nExtension\nNotes\n\n\n\n\nWAV\n.wav\nRecommended, lossless, 16/24-bit PCM\n\n\nFLAC\n.flac\nCompressed and lossless\n\n\nMP3\n.mp3\nWidely supported, lossy compression\n\n\nOGG\n.ogg, .oga\nOpen lossy compression format, good quality\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor phonetic research, use 16-bit WAV files at 44.1kHz or 48kHz sampling rate for best compatibility and quality.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#file-size-limits",
    "href": "tutorial/01-loading-audio.html#file-size-limits",
    "title": "1. Loading Audio",
    "section": "File Size Limits",
    "text": "File Size Limits\nWhile there’s no hard limit, practical constraints apply:\n\nSmall files (&lt; 1 MB, &lt; 30s): Load instantly\nMedium files (1-10 MB, 30-300s): Load in 1-5 seconds\nLarge files (&gt; 10 MB, &gt; 300s): May take 10+ seconds; spectrogram computed on-demand when zoomed\n\n\n\n\n\n\n\nNote\n\n\n\nFiles longer than 60 seconds show waveform immediately but require zooming to trigger spectrogram analysis. See Features: Long Audio Handling for details.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#loading-a-different-file",
    "href": "tutorial/01-loading-audio.html#loading-a-different-file",
    "title": "1. Loading Audio",
    "section": "Loading a Different File",
    "text": "Loading a Different File\nTo load a new audio file:\n\nSimply drag another file or click “Load Audio” again\nThe previous audio will be replaced (annotations are lost unless exported)\n\n\n\n\n\n\n\nWarning\n\n\n\nLoading a new file clears all unsaved annotations and data points. Export your work first if needed (see Section 6: Exporting).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#troubleshooting",
    "href": "tutorial/01-loading-audio.html#troubleshooting",
    "title": "1. Loading Audio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nFile won’t load:\n\nCheck the file format (use WAV if unsure)\nEnsure the file isn’t corrupted (try playing in another app)\nCheck browser console (F12) for error messages\n\n“Failed to decode audio” error:\n\nFile may be in an unsupported codec\nConvert to WAV using Audacity, FFmpeg, or Praat\nTry a different browser (Chrome/Edge have best codec support)\n\nMicrophone doesn’t work:\n\nCheck browser permissions (Settings → Privacy → Microphone)\nEnsure microphone is connected and working in other apps\nTry using HTTPS (microphone API requires secure context)\n\nFile loads but no spectrogram:\n\nFor files &gt;60s, zoom in to trigger analysis\nCheck that WASM backend is loaded (see backend selector)\nTry refreshing the page (Ctrl+R)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#practice-exercise",
    "href": "tutorial/01-loading-audio.html#practice-exercise",
    "title": "1. Loading Audio",
    "section": "Practice Exercise",
    "text": "Practice Exercise\nBefore moving to the next section, try:\n\nLoad an audio file using drag-and-drop\nLoad a different file using the file picker\nRecord 5 seconds of speech using the microphone\nObserve the differences in waveform appearance\n\n\nChallenge: Record yourself saying “hello” three times with different intonations (statement, question, excited). Notice how the pitch contours differ in the waveform.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "tutorial/01-loading-audio.html#whats-next",
    "href": "tutorial/01-loading-audio.html#whats-next",
    "title": "1. Loading Audio",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow that you have audio loaded, let’s learn how to navigate and explore it.\nNext: 2. Exploring Audio →\n\nNavigation: ← Back to Tutorial Overview | Next: Exploring Audio →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "1. Loading Audio"
    ]
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html",
    "href": "COMPLETION_CHECKLIST.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#pre-launch-required-for-first-deployment",
    "href": "COMPLETION_CHECKLIST.html#pre-launch-required-for-first-deployment",
    "title": "",
    "section": "🚀 Pre-Launch (Required for First Deployment)",
    "text": "🚀 Pre-Launch (Required for First Deployment)\n\nAdd test audio file\n\nCreate or record a 5-15 second speech sample\nSave as WAV (16-bit PCM) to scripts/screenshots/test-audio/sample.wav\nVerify audio has clear vowels and consonants\n\nUpdate placeholder URLs\n\nFind all instances: grep -r \"ucpresearch\" docs/\nReplace with actual GitHub username\nFiles to check: _quarto.yml, index.html, getting-started.html, tutorial files, etc.\n\nInstall prerequisites\n\nQuarto 1.4.549+ installed\nNode.js 18+ installed\nScreenshot dependencies: cd scripts/screenshots && npm install\n\nTest local build\n\nRun: ./scripts/docs/build-docs.sh\nFix any build errors\nVerify screenshots directory populated: ls docs/screenshots/\n\nTest local preview\n\nRun: ./scripts/docs/serve-docs.sh\nOpen http://localhost:8080\nClick through all navigation links\nVerify no broken internal links\n\nPush to GitHub\n\nCreate GitHub repository (if not exists)\nPush all files\nEnable GitHub Pages in Settings → Pages\nWait for deployment (~5 minutes)\nVerify site live at https://USERNAME.github.io/ozen-web/"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#complete-stub-pages",
    "href": "COMPLETION_CHECKLIST.html#complete-stub-pages",
    "title": "",
    "section": "📄 Complete Stub Pages",
    "text": "📄 Complete Stub Pages\n\nFeatures (7 pages)\n\nfeatures/spectrogram.html\n\nContent: Spectrogram computation, colormap, zoom enhancement\nSources: CLAUDE.md, src/lib/components/Spectrogram.svelte\nScreenshots: Zoomed spectrogram, max frequency selector\n\nfeatures/waveform.html\n\nContent: Waveform display, downsampling, synchronization\nSources: src/lib/components/Waveform.svelte\nScreenshots: Waveform visualization\n\nfeatures/annotations.html\n\nContent: TextGrid support, tier management, undo system\nSources: CLAUDE.md, src/lib/stores/annotations.ts\nScreenshots: Multi-tier annotations, boundary editing\n\nfeatures/acoustic-overlays.html\n\nContent: All overlay types, interpretation guide, settings\nSources: CLAUDE.md, src/lib/wasm/acoustic.ts\nScreenshots: Individual overlays, combined overlays\n\nfeatures/data-points.html\n\nContent: Adding/moving/removing points, export format\nSources: src/lib/stores/dataPoints.ts\nScreenshots: Data points on spectrogram, TSV output\n\nfeatures/audio-playback.html\n\nContent: Playback controls, Web Audio API details\nSources: src/lib/audio/player.ts\nScreenshots: Playback cursor, controls\n\nfeatures/mobile-viewer.html\n\nContent: Touch gestures, mobile layout, URL parameters\nSources: src/routes/viewer/+page.svelte, src/lib/touch/gestures.ts\nScreenshots: Mobile interface (portrait/landscape)\n\n\n\n\nEmbedding (4 pages)\n\nembedding/basic-usage.html\n\nContent: Step-by-step embedding guide, directory structure\nSources: README.md (Embedding section)\nExamples: HTML snippets, helper script usage\n\nembedding/quarto-integration.html\n\nContent: Quarto-specific instructions, embed-resources handling\nSources: README.md, scripts/create-iframe.*\nExamples: Quarto document with embedded viewer\n\nembedding/url-parameters.html\n\nContent: All URL parameters, CORS setup, data URLs\nSources: README.md, src/routes/viewer/+page.svelte\nExamples: Parameter combinations, server configs\n\nembedding/examples.html\n\nContent: Real-world use cases with full code\nExamples: Research paper, course material, blog post, notebook\n\n\n\n\nReference (3 pages)\n\nreference/configuration.html\n\nContent: All config.yaml options with descriptions\nSources: static/config.yaml, src/lib/stores/config.ts\nExamples: Sample configs for different use cases\n\nreference/backends.html\n\nContent: Backend comparison, when to use each, setup\nSources: CLAUDE.md, src/lib/wasm/acoustic.ts\nTable: Feature comparison, license differences\n\nreference/file-formats.html\n\nContent: TextGrid, TSV, WAV format specifications\nSources: src/lib/textgrid/parser.ts\nExamples: File format examples, schemas\n\n\n\n\nDevelopment (5 pages)\n\ndevelopment/setup.html\n\nContent: Detailed dev environment setup, dependencies\nSources: DEVELOPMENT.md\nCommands: Installation, build, test procedures\n\ndevelopment/architecture.html\n\nContent: System design, component architecture\nSources: CLAUDE.md, DEVELOPMENT.md\nDiagrams: Use Mermaid for architecture diagrams\n\ndevelopment/stores.html\n\nContent: Svelte stores pattern, state management\nSources: CLAUDE.md, src/lib/stores/*.ts\nCode: Store implementation examples\n\ndevelopment/wasm-integration.html\n\nContent: WASM backend abstraction, adding new backends\nSources: CLAUDE.md, src/lib/wasm/acoustic.ts\nCode: WASM usage patterns, memory management\n\ndevelopment/contributing.html\n\nContent: Contribution guidelines, code style, PR process\nSources: Standard contributing guide template\nChecklist: PR checklist, testing requirements"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#enhance-screenshots",
    "href": "COMPLETION_CHECKLIST.html#enhance-screenshots",
    "title": "",
    "section": "🖼️ Enhance Screenshots",
    "text": "🖼️ Enhance Screenshots\n\nAdd missing screenshots (not in initial 15)\n\nFile drop zone (highlighted when dragging)\nBackend selector dropdown\nSettings drawer (mobile)\nContext menus (right-click)\nTextGrid export dialog\nTSV export result (spreadsheet view)\nValues panel (close-up)\nBoundary snapping visualization\nAnnotation editing (text input active)\n\nImprove screenshot quality\n\nUse real speech audio (not synthesized tones)\nShow meaningful labels (“hello”, “world” not “a”, “b”)\nCapture at different zoom levels\nInclude UI context (not just cropped regions)"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#add-example-files",
    "href": "COMPLETION_CHECKLIST.html#add-example-files",
    "title": "",
    "section": "📚 Add Example Files",
    "text": "📚 Add Example Files\n\ndocs/examples/demo-audio.wav\n\nShort (10-15 sec) speech sample\nClear vowels for formant demonstration\nVaried intonation for pitch tracking\n\ndocs/examples/demo-textgrid.TextGrid\n\nMatching annotation for demo audio\nMultiple tiers (words, phones)\nExample labels\n\ndocs/examples/demo-data.tsv\n\nSample data points export\nShows typical analysis output\nIncludes annotation labels"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#quality-assurance",
    "href": "COMPLETION_CHECKLIST.html#quality-assurance",
    "title": "",
    "section": "🔍 Quality Assurance",
    "text": "🔍 Quality Assurance\n\nVerify all links\n\nInternal page links work\nScreenshots display correctly\nExternal links open (GitHub, resources)\nNavigation breadcrumbs correct\n\nTest on mobile\n\nSite responsive on phone\nNavigation works (hamburger menu)\nImages scale properly\nNo horizontal scroll\n\nTest code examples\n\nAll code snippets are valid\nBash commands work as written\nPython/R examples are correct\nFile paths are accurate\n\nCheck consistency\n\nTerminology consistent across pages\nScreenshot captions match content\nCross-references up to date\nKeyboard shortcuts match implementation"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#deployment-maintenance",
    "href": "COMPLETION_CHECKLIST.html#deployment-maintenance",
    "title": "",
    "section": "🚀 Deployment & Maintenance",
    "text": "🚀 Deployment & Maintenance\n\nGitHub Actions verification\n\ndeploy-docs.yml runs successfully\nScreenshots generate in CI\nSite deploys to GitHub Pages\nNo workflow errors\n\nWeekly screenshot updates\n\nupdate-screenshots.yml scheduled correctly\nCommits trigger re-deployment\nScreenshots stay current with app changes\n\nSet up custom domain (optional)\n\nConfigure CNAME in repository settings\nUpdate URLs in _quarto.yml\nTest custom domain works"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#progress-tracking",
    "href": "COMPLETION_CHECKLIST.html#progress-tracking",
    "title": "",
    "section": "📊 Progress Tracking",
    "text": "📊 Progress Tracking\nUpdate this table as you complete sections:\n\n\n\nSection\nPages\nComplete\nPercentage\n\n\n\n\nPre-Launch\n-\n0/6\n0%\n\n\nFeatures\n7\n1/7\n14%\n\n\nEmbedding\n4\n1/4\n25%\n\n\nReference\n3\n1/3\n33%\n\n\nDevelopment\n5\n0/5\n0%\n\n\nScreenshots\n15+\n0/15\n0%\n\n\nExamples\n3\n0/3\n0%\n\n\nQA\n-\n0/4\n0%\n\n\nTOTAL\n41+\n3/47\n~6%"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#milestones",
    "href": "COMPLETION_CHECKLIST.html#milestones",
    "title": "",
    "section": "🎯 Milestones",
    "text": "🎯 Milestones\n\nMilestone 1: Deployable (MVP)\n\nPre-launch checklist complete\nSite builds and deploys\nAt least tutorial is complete\nTarget: 1-2 hours\n\n\n\nMilestone 2: Feature Complete\n\nAll stub pages have content\nAll screenshots captured\nExamples added\nTarget: 20-30 hours\n\n\n\nMilestone 3: Production Ready\n\nAll QA checks pass\nCustom domain configured (optional)\nDocumentation mentioned in main README\nAnnounced to users\nTarget: 30-40 hours total"
  },
  {
    "objectID": "COMPLETION_CHECKLIST.html#tips",
    "href": "COMPLETION_CHECKLIST.html#tips",
    "title": "",
    "section": "💡 Tips",
    "text": "💡 Tips\n\nWork incrementally: Complete one page at a time, test, commit\nReuse content: Extract from existing markdown files (README, DEVELOPMENT, CLAUDE)\nUse templates: Copy structure from completed pages\nTest frequently: Run quarto preview to see changes immediately\nAsk for help: Open issues on GitHub if stuck\n\n\nStart date: ____________ Target completion: ____________ Last updated: ____________"
  },
  {
    "objectID": "features/spectrogram.html",
    "href": "features/spectrogram.html",
    "title": "Spectrogram",
    "section": "",
    "text": "The spectrogram displays frequency content over time, computed using Praat-accurate algorithms via WebAssembly. Ozen-web uses a grayscale colormap matching Praat’s default appearance, with intelligent resolution scaling for optimal performance.\n\n\n\nSpectrogram with formant overlays",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#overview",
    "href": "features/spectrogram.html#overview",
    "title": "Spectrogram",
    "section": "",
    "text": "The spectrogram displays frequency content over time, computed using Praat-accurate algorithms via WebAssembly. Ozen-web uses a grayscale colormap matching Praat’s default appearance, with intelligent resolution scaling for optimal performance.\n\n\n\nSpectrogram with formant overlays",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#key-features",
    "href": "features/spectrogram.html#key-features",
    "title": "Spectrogram",
    "section": "Key Features",
    "text": "Key Features\n\nPraat-accurate computation - Uses praatfan WASM for research-grade spectrograms\nDynamic resolution - Automatically enhances detail when zoomed in\nEfficient caching - Stores computed spectrograms for instant redraw\nConfigurable frequency range - Choose 5 kHz, 7.5 kHz, or 10 kHz maximum\nLong audio support - On-demand computation for files over 60 seconds",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#how-it-works",
    "href": "features/spectrogram.html#how-it-works",
    "title": "Spectrogram",
    "section": "How It Works",
    "text": "How It Works\n\nInitial Computation\nWhen audio is loaded, Ozen-web:\n\nComputes full spectrogram via WASM to_spectrogram()\nApplies grayscale colormap → converts to ImageData\nCaches to off-screen canvas for fast redraw\nDisplays visible portion based on current time range\n\n\n\nDynamic Resolution Enhancement\nWhen zoomed beyond 2x magnification, the spectrogram automatically regenerates at higher resolution for the visible window:\n\n\n\n\n\n\nNoteZoom Threshold\n\n\n\n\nBelow 2x zoom: Uses cached full-length spectrogram\nAbove 2x zoom: Regenerates high-res spectrogram for visible region (debounced 300ms)\nLong files (&gt;60s): Only computes spectrogram for visible window\n\n\n\nThis provides optimal detail when examining specific regions while maintaining performance.\n\n\nCaching Strategy\n// Pseudocode\nif (audioLoaded && visibleDuration &lt;= 60s) {\n  if (zoomLevel &gt; 2x) {\n    // High-resolution mode\n    computeSpectrogramForRange(visibleStart, visibleEnd, highResSettings);\n  } else {\n    // Display cached full spectrogram\n    drawCachedSpectrogram(visibleStart, visibleEnd);\n  }\n}",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#settings",
    "href": "features/spectrogram.html#settings",
    "title": "Spectrogram",
    "section": "Settings",
    "text": "Settings\n\nMax Frequency\nControl the frequency range displayed:\n\n5000 Hz - Default for adult male speech\n7500 Hz - Suitable for female/child speech\n10000 Hz - Maximum detail for high-frequency analysis\n\n\n\n\n\n\n\nTipChoosing Max Frequency\n\n\n\nFor most speech analysis, 5000 Hz captures all relevant information. Use 7500-10000 Hz when analyzing:\n\nChildren’s voices\nHigh-pitched vowels\nFricative consonants (s, sh, f)\nBird vocalizations\n\n\n\n\n\nSpectrogram Parameters\nDefault settings (configurable via WASM):\n\n\n\nParameter\nDefault\nDescription\n\n\n\n\nWindow length\n5 ms\nTime window for each FFT\n\n\nTime step\n2 ms\nHop size between windows\n\n\nFrequency step\n20 Hz\nFrequency resolution\n\n\nWindow shape\nGaussian\nSpectral windowing function",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#long-audio-handling",
    "href": "features/spectrogram.html#long-audio-handling",
    "title": "Spectrogram",
    "section": "Long Audio Handling",
    "text": "Long Audio Handling\nFor recordings longer than 60 seconds:\n\nOn load: Waveform displays, spectrogram shows “Zoom in for spectrogram” message\nWhen zoomed to ≤60s visible window: Spectrogram computes for that region\nDebounced: 300ms delay prevents excessive recomputation during zoom/pan\n\nThis allows working with arbitrarily long recordings (hours) without UI freezing.\n\n\n\n\n\n\nTipWorking with Long Files\n\n\n\n\nLoad your multi-hour recording\nUse the waveform to navigate to regions of interest\nZoom in to view detailed spectrogram\nSpectrogram automatically appears when window &lt; 60 seconds",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#overlays",
    "href": "features/spectrogram.html#overlays",
    "title": "Spectrogram",
    "section": "Overlays",
    "text": "Overlays\nThe spectrogram can display multiple acoustic overlays simultaneously:\n\nPitch track (blue line with dots)\nFormants F1-F4 (red dots)\nIntensity (green line)\nHNR (harmonics-to-noise ratio)\nCenter of Gravity (spectral COG)\nData points (yellow dashed lines)\n\nSee Acoustic Overlays for details.",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#interaction",
    "href": "features/spectrogram.html#interaction",
    "title": "Spectrogram",
    "section": "Interaction",
    "text": "Interaction\n\nNavigation\n\nScroll wheel - Zoom in/out (centered on cursor)\nHorizontal scroll - Pan left/right through time\nClick - Place cursor at time position\nClick + drag - Select time region\nDouble-click - Add data point (if in data collection mode)\n\n\n\nVisual Indicators\n\nRed vertical line - Current cursor position\nBlue rectangle - Selected time region\nYellow dashed lines - Data collection points\nColored dots/lines - Acoustic overlay tracks",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#technical-details",
    "href": "features/spectrogram.html#technical-details",
    "title": "Spectrogram",
    "section": "Technical Details",
    "text": "Technical Details\n\nComputation Backend\nSpectrograms are computed using the selected WASM backend:\nimport { computeSpectrogram } from '$lib/wasm/acoustic';\n\nconst spectrogram = computeSpectrogram(\n  sound,           // WASM Sound object\n  0.005,          // windowLength (5 ms)\n  5000,           // maxFrequency (Hz)\n  0.002,          // timeStep (2 ms)\n  20              // frequencyStep (Hz)\n);\n\nconst info = getSpectrogramInfo(spectrogram);\n// { nTimes, nFreqs, values, timeStep, freqStep, ... }\n\n\nColormap\nGrayscale mapping matches Praat conventions:\n\nBlack (0): High energy\nWhite (255): Low energy\nLinear scaling between min and max dB values\n\n\n\nCanvas Rendering\nThe spectrogram uses HTML5 Canvas with ImageData:\n\nCompute spectrogram values (dB) via WASM\nApply grayscale colormap: gray = 255 - normalize(dB, min, max)\nCreate ImageData with RGBA values\nDraw to off-screen canvas\nCopy visible region to display canvas on zoom/pan",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#performance",
    "href": "features/spectrogram.html#performance",
    "title": "Spectrogram",
    "section": "Performance",
    "text": "Performance\n\n\n\nScenario\nComputation Time\nMemory\n\n\n\n\n5s audio, 5 kHz\n~100-200 ms\n~2 MB\n\n\n30s audio, 5 kHz\n~500-800 ms\n~10 MB\n\n\n60s audio, 10 kHz\n~1-2 seconds\n~40 MB\n\n\n3600s audio (on-demand)\n~1 second per 60s window\n~40 MB\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTimes measured on modern desktop (2020+). First computation includes WASM initialization (~200ms).",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#troubleshooting",
    "href": "features/spectrogram.html#troubleshooting",
    "title": "Spectrogram",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nSpectrogram Not Appearing\nProblem: “Zoom in for spectrogram” message displays\nSolution: This is expected for files &gt;60 seconds. Zoom in until visible window is ≤60 seconds.\n\n\nBlurry or Low Resolution\nProblem: Spectrogram looks pixelated when zoomed in\nSolution: Zoom beyond 2x. The system automatically regenerates at higher resolution after 300ms.\n\n\nSlow Performance\nProblem: UI freezes when zooming/panning\nSolution: - Close other browser tabs to free memory - Reduce max frequency (10 kHz → 5 kHz) - Use shorter audio files - Wait for debounced regeneration to complete",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/spectrogram.html#see-also",
    "href": "features/spectrogram.html#see-also",
    "title": "Spectrogram",
    "section": "See Also",
    "text": "See Also\n\nAcoustic Overlays - Pitch, formants, intensity visualization\nWaveform - Amplitude-domain display\nConfiguration - Customizing spectrogram settings\nBackends - WASM backend options",
    "crumbs": [
      "Spectrogram"
    ]
  },
  {
    "objectID": "features/annotations.html",
    "href": "features/annotations.html",
    "title": "Annotations",
    "section": "",
    "text": "Ozen-web provides a complete TextGrid-compatible annotation system for transcription, labeling, and segmentation. Create multi-tiered annotations with interval and point tiers, compatible with Praat.\n\n\n\nMulti-tier annotation example",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#overview",
    "href": "features/annotations.html#overview",
    "title": "Annotations",
    "section": "",
    "text": "Ozen-web provides a complete TextGrid-compatible annotation system for transcription, labeling, and segmentation. Create multi-tiered annotations with interval and point tiers, compatible with Praat.\n\n\n\nMulti-tier annotation example",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#key-features",
    "href": "features/annotations.html#key-features",
    "title": "Annotations",
    "section": "Key Features",
    "text": "Key Features\n\nPraat-compatible TextGrid - Import/export in TextGrid format (short and long)\nMulti-tier support - Unlimited annotation tiers\nInterval tiers - Segment audio into labeled intervals\nBoundary editing - Add, remove, move boundaries\nText editing - Label intervals with text\nUnified undo/redo - Full undo history (Ctrl+Z / Ctrl+Y)\nKeyboard workflow - Efficient shortcuts for rapid annotation",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#tier-management",
    "href": "features/annotations.html#tier-management",
    "title": "Annotations",
    "section": "Tier Management",
    "text": "Tier Management\n\nAdding Tiers\nVia UI: 1. Click “+ Add Tier” button 2. Enter tier name (e.g., “words”, “phones”, “syllables”) 3. Press Enter to create\nTier Naming: - Use descriptive names: “words”, “phones”, “tones”, “events” - Multiple tiers allow parallel annotations at different granularities\n\n\nTier Types\nCurrently supported: - Interval tiers - Continuous segmentation with boundaries\nPlanned: - Point tiers - Single time points with labels (future feature)\n\n\nDeleting Tiers\n\n\n\n\n\n\nWarning\n\n\n\nTier deletion is permanent and cannot be undone. Export your TextGrid before deleting tiers.\n\n\n\nClick tier menu (⋮)\nSelect “Delete tier”\nConfirm deletion",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#boundary-management",
    "href": "features/annotations.html#boundary-management",
    "title": "Annotations",
    "section": "Boundary Management",
    "text": "Boundary Management\n\nAdding Boundaries\nDouble-click method: 1. Double-click on tier at desired time position 2. Boundary appears, splitting the interval\nCursor method: 1. Click on spectrogram to place cursor 2. Double-click on tier at cursor position\n\n\n\n\n\n\nTipQuick Annotation Workflow\n\n\n\n\nPress Space to play audio\nPress Escape when you hear a boundary\nDouble-click tier to add boundary at cursor\nRepeat for all boundaries\n\n\n\n\n\nMoving Boundaries\nDrag and drop: 1. Click and hold boundary marker 2. Drag left or right to new position 3. Release to place\nPrecision: - Zoom in (scroll wheel) for fine-grained adjustment - Cursor position indicator shows exact time\n\n\nBoundary Snapping\nWhen adding boundaries on lower tiers, they can snap to boundaries in upper tiers:\nExample:\nwords:  |  hello  |  world  |\nphones: | h | e | l | o | w | o | r | l | d |\nDouble-clicking near an existing boundary on a different tier will snap to that time, maintaining alignment.\n\n\nRemoving Boundaries\nRight-click menu: 1. Right-click on boundary marker 2. Select “Remove boundary” 3. Adjacent intervals merge\nKeyboard shortcut: - Select boundary (click) - Press Delete or Backspace\n\n\n\n\n\n\nNoteUndo Support\n\n\n\nAll boundary operations (add, move, remove) can be undone with Ctrl+Z (Cmd+Z on Mac).",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#text-editing",
    "href": "features/annotations.html#text-editing",
    "title": "Annotations",
    "section": "Text Editing",
    "text": "Text Editing\n\nLabeling Intervals\nDouble-click to edit: 1. Double-click on interval to activate text editor 2. Type label (IPA, orthography, tags, etc.) 3. Press Enter to save 4. Press Escape to cancel\nClick to select: - Single-click interval to select without editing - Selected interval highlights\n\n\nUnicode and IPA Support\nFull Unicode support for linguistic transcription:\n[ˈhɛloʊ] - IPA transcription\n你好 - Chinese characters\n🔊 - Emoji labels (for prosody annotation)\n\n\nMulti-line Labels\nPress Shift+Enter for multi-line labels (if needed for glosses or notes).",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#keyboard-shortcuts",
    "href": "features/annotations.html#keyboard-shortcuts",
    "title": "Annotations",
    "section": "Keyboard Shortcuts",
    "text": "Keyboard Shortcuts\n\n\n\nShortcut\nAction\n\n\n\n\nDouble-click tier\nAdd boundary at position\n\n\nDouble-click interval\nEdit text label\n\n\nEnter\nSave text edit\n\n\nEscape\nCancel text edit / deselect\n\n\nCtrl+Z / Cmd+Z\nUndo\n\n\nCtrl+Y / Ctrl+Shift+Z\nRedo\n\n\nDelete / Backspace\nRemove selected boundary\n\n\n1-5\nSwitch to tier 1-5\n\n\n\nSee Keyboard Shortcuts Reference for complete list.",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#undo-system",
    "href": "features/annotations.html#undo-system",
    "title": "Annotations",
    "section": "Undo System",
    "text": "Undo System\nOzen-web features a unified undo/redo system for annotation operations:\nUndoable operations: - Adding boundaries - Removing boundaries - Moving boundaries - Editing interval text\nNon-undoable operations: - Adding/removing tiers (permanent) - Loading files\n\n\n\n\n\n\nTipUndo History\n\n\n\nThe undo stack stores the entire annotation state before each operation. You can undo multiple operations sequentially to return to any previous state.\n\n\nKeyboard: - Undo: Ctrl+Z (Windows/Linux), Cmd+Z (Mac) - Redo: Ctrl+Y (Windows/Linux), Ctrl+Shift+Z (Mac)",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#textgrid-importexport",
    "href": "features/annotations.html#textgrid-importexport",
    "title": "Annotations",
    "section": "TextGrid Import/Export",
    "text": "TextGrid Import/Export\n\nImporting TextGrid\nDrag and drop: 1. Drag .TextGrid file onto interface 2. Annotations load automatically 3. Audio must be loaded separately\nFile picker: 1. Click “Load TextGrid” button 2. Select .TextGrid file 3. Annotations appear on tiers\nSupported formats: - Short TextGrid format (quoted strings) - Long TextGrid format (ooTextFile)\n\n\n\n\n\n\nNoteTextGrid Without Audio\n\n\n\nYou can load a TextGrid without audio to view annotation structure. Load the corresponding audio file afterward to see waveform/spectrogram.\n\n\n\n\nExporting TextGrid\nVia UI button: 1. Click “Export TextGrid” button 2. Choose save location (File System Access API) 3. TextGrid saves in short format\nExport format: - Short TextGrid format (Praat-compatible) - UTF-8 encoding - Interval tiers only (current version)\nFilename: - Default: annotations.TextGrid - Recommended: Match audio filename (e.g., recording.wav → recording.TextGrid)\n\n\n\nExport TextGrid interface",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#use-cases",
    "href": "features/annotations.html#use-cases",
    "title": "Annotations",
    "section": "Use Cases",
    "text": "Use Cases\n\nPhonetic Transcription\nCreate detailed phonetic annotations:\nwords:     | the   | cat   | sat   | on    | the   | mat   |\nphones:    | ð | ə | k | æ | t | s | æ | t | ɑ | n | ð | ə | m | æ | t |\nsyllables: | ðə    | kæt   | sæt   | ɑn    | ðə    | mæt   |\n\n\nProsodic Labeling\nAnnotate intonation and prosody:\nwords:    | Hello  | how    | are    | you    |\ntones:    | H*     | L+H*   | H-     | L%     |\nstress:   | 1      | 0      | 1      | 0      |\n\n\nConversation Analysis\nSegment multi-speaker recordings:\nspeaker_a: | Hello           |        | How are you?    |\nspeaker_b: |        | Hi there!       |        | Fine, thanks. |\noverlap:   |                 | ✓      |                 |\n\n\nEvent Annotation\nMark discrete events:\naudio:     |---------------------------------------------------|\nevents:    | cough  |           | door slam |        | phone ring |\nquality:   |        | good      |           | noisy  |            |",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#integration-with-data-points",
    "href": "features/annotations.html#integration-with-data-points",
    "title": "Annotations",
    "section": "Integration with Data Points",
    "text": "Integration with Data Points\nData points automatically capture annotation labels at their time position:\nExample TSV export:\ntime    freq    pitch  F1    label_words  label_phones\n1.234   720     245    720   \"cat\"        \"æ\"\n1.567   1240    250    850   \"sat\"        \"æ\"\nThis enables correlation analysis between acoustic measurements and annotation categories.\nSee Data Points for details.",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#best-practices",
    "href": "features/annotations.html#best-practices",
    "title": "Annotations",
    "section": "Best Practices",
    "text": "Best Practices\n\nTier Organization\nHierarchical structure: 1. Top tier: Largest units (utterances, phrases) 2. Middle tiers: Intermediate units (words, syllables) 3. Bottom tiers: Smallest units (phones, events)\nAlignment: - Keep boundaries aligned across tiers when appropriate - Use boundary snapping for consistent segmentation\n\n\nLabeling Conventions\nBe consistent: - Decide on transcription convention (IPA, ARPABET, orthography) - Use same labels for same sounds - Document conventions in separate file\nCommon systems: - IPA: Universal phonetic transcription - ARPABET: ASCII-friendly phoneme set - X-SAMPA: ASCII IPA alternative - Orthography: Plain text spelling\n\n\nFile Management\nNaming: - Match TextGrid filename to audio: speaker01.wav + speaker01.TextGrid - Use consistent naming across corpus\nBackup: - Export TextGrid frequently during annotation - Version control for large projects (Git) - Keep original audio files separate from annotations",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#troubleshooting",
    "href": "features/annotations.html#troubleshooting",
    "title": "Annotations",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nTextGrid Won’t Load\nProblem: Error loading .TextGrid file\nPossible causes: - File encoding (not UTF-8) - Malformed TextGrid format - Point tiers (not yet supported)\nSolution: - Re-export from Praat - Check file encoding (should be UTF-8) - Convert point tiers to interval tiers in Praat\n\n\nBoundaries Not Snapping\nProblem: Boundaries won’t align to other tiers\nSolution: - Zoom in closer to boundary - Click precisely on boundary location - Manually align by dragging\n\n\nUndo Not Working\nProblem: Ctrl+Z doesn’t undo operation\nNon-undoable operations: - Adding/removing tiers - Loading files\nSolution: - Only annotation edits are undoable - Export TextGrid before tier operations",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/annotations.html#see-also",
    "href": "features/annotations.html#see-also",
    "title": "Annotations",
    "section": "See Also",
    "text": "See Also\n\nTutorial: Annotations - Step-by-step guide\nKeyboard Shortcuts - Full shortcut reference\nFile Formats - TextGrid format details\nData Points - Collecting measurements at annotations",
    "crumbs": [
      "Annotations"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html",
    "href": "features/mobile-viewer.html",
    "title": "Mobile Viewer",
    "section": "",
    "text": "The mobile viewer (/viewer route) provides a touch-optimized, view-only interface designed for smartphones and tablets. It enables audio analysis on mobile devices with touch gestures for navigation and a streamlined UI.\n\n\n\nMobile viewer in landscape mode",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#overview",
    "href": "features/mobile-viewer.html#overview",
    "title": "Mobile Viewer",
    "section": "",
    "text": "The mobile viewer (/viewer route) provides a touch-optimized, view-only interface designed for smartphones and tablets. It enables audio analysis on mobile devices with touch gestures for navigation and a streamlined UI.\n\n\n\nMobile viewer in landscape mode",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#key-features",
    "href": "features/mobile-viewer.html#key-features",
    "title": "Mobile Viewer",
    "section": "Key Features",
    "text": "Key Features\n\nTouch gestures - Tap, drag, pinch, two-finger pan\nCompact UI - Two-row values bar, minimal controls\nView-only mode - No editing (focused on analysis and presentation)\nURL loading - Load audio via ?audio= parameter\nPre-configured overlays - Set via ?overlays= parameter\nResponsive - Works in portrait and landscape\nSafe area support - Handles iPhone notches and rounded corners\nCORS-enabled - Load audio from remote URLs\nData URL support - Embed audio in URL for self-contained shares",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#accessing-the-mobile-viewer",
    "href": "features/mobile-viewer.html#accessing-the-mobile-viewer",
    "title": "Mobile Viewer",
    "section": "Accessing the Mobile Viewer",
    "text": "Accessing the Mobile Viewer\n\nVia URL\nNavigate directly to the /viewer route:\nhttps://ucpresearch.github.io/ozen-web/viewer\n\n\nWith Audio Pre-loaded\nUse URL parameter to load audio automatically:\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/audio.wav\n\n\nWith Overlays Pre-configured\nSpecify which overlays to enable:\nhttps://ucpresearch.github.io/ozen-web/viewer?audio=...&overlays=pitch,formants,intensity\nSee URL Parameters for complete documentation.",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#touch-gestures",
    "href": "features/mobile-viewer.html#touch-gestures",
    "title": "Mobile Viewer",
    "section": "Touch Gestures",
    "text": "Touch Gestures\n\nTap\nSingle tap on spectrogram: - Places cursor at tap position - Shows values at that time/frequency - Clears selection if any\n\n\nDrag (One Finger)\nHorizontal drag: - Creates time selection (blue highlight) - Drag from start to end time - Release to finalize selection\n\n\nPinch (Two Fingers)\nPinch in/out: - Zoom in (pinch out) - Zoom out (pinch in) - Centered on midpoint between fingers\n\n\nTwo-Finger Drag\nPan horizontally: - Scroll left/right through time - Doesn’t create selection - Use when zoomed in\n\n\n\n\n\n\nTipTouch Workflow\n\n\n\n\nPinch to zoom to desired time scale\nTwo-finger drag to navigate to region of interest\nTap to place cursor and view measurements\nDrag to select and play specific region",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#user-interface",
    "href": "features/mobile-viewer.html#user-interface",
    "title": "Mobile Viewer",
    "section": "User Interface",
    "text": "User Interface\n\nCompact Values Bar\nTwo-row display shows key measurements:\nRow 1: - Time (s) - Pitch (Hz) - Intensity (dB) - HNR (dB)\nRow 2: - F1, F2, F3, F4 (Hz) - CoG (Hz)\nValues update as you tap or drag across the spectrogram.\n\n\nSettings Drawer\nSwipe from right edge (or tap settings icon) to open drawer:\n\nOverlay toggles - Enable/disable acoustic overlays\nMax frequency - 5k / 7.5k / 10k Hz\nBackend selector - Choose WASM backend\n\n\n\nFloating Play Button\nLarge play/pause button appears at bottom:\n\nTap to play selection (if selected)\nTap to play visible window (if no selection)\nShows playback state (play/pause icon)",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#orientation-support",
    "href": "features/mobile-viewer.html#orientation-support",
    "title": "Mobile Viewer",
    "section": "Orientation Support",
    "text": "Orientation Support\n\nPortrait Mode\n\n\n\nMobile viewer in portrait mode\n\n\n\nVertical layout\nSpectrogram above waveform\nValues bar at top\nSettings drawer from right\nSuitable for phones\n\n\n\nLandscape Mode\n\n\n\nMobile viewer in landscape mode\n\n\n\nHorizontal layout optimized for viewing\nWider spectrogram view\nSettings drawer from right\nBetter for detailed analysis\nRecommended for tablets\n\n\n\n\n\n\n\nNoteAutomatic Rotation\n\n\n\nThe viewer automatically adapts layout when device is rotated. No refresh required.",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#safe-area-handling",
    "href": "features/mobile-viewer.html#safe-area-handling",
    "title": "Mobile Viewer",
    "section": "Safe Area Handling",
    "text": "Safe Area Handling\nThe viewer respects device safe areas:\n\niPhone notches - Content avoids notch and Dynamic Island\nRounded corners - UI elements stay within safe bounds\nHome indicator - Bottom spacing on iOS devices\n\nThis ensures all controls are accessible on modern smartphones.",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#url-audio-loading",
    "href": "features/mobile-viewer.html#url-audio-loading",
    "title": "Mobile Viewer",
    "section": "URL Audio Loading",
    "text": "URL Audio Loading\nThe mobile viewer can load audio from URLs, enabling sharing and embedding:\n\nDirect WAV/MP3 URLs\n/viewer?audio=https://example.com/recording.wav\nCORS requirements: - Server must allow cross-origin requests - Set Access-Control-Allow-Origin: * header - Works with GitHub Pages, S3, most CDNs\n\n\nData URLs\nEmbed audio directly in URL (self-contained):\n/viewer?audio=data:audio/wav;base64,UklGR...\nUse cases: - Share analysis without hosting files - Embed in QR codes - Create self-contained examples\nSee Embedding Guide for details.",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#embedding-in-web-pages",
    "href": "features/mobile-viewer.html#embedding-in-web-pages",
    "title": "Mobile Viewer",
    "section": "Embedding in Web Pages",
    "text": "Embedding in Web Pages\nEmbed the mobile viewer in responsive web pages:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=...&overlays=pitch,formants\"\n        style=\"width: 100%; height: 600px; border: none;\"&gt;\n&lt;/iframe&gt;\nThe viewer scales appropriately within iframes.",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#limitations",
    "href": "features/mobile-viewer.html#limitations",
    "title": "Mobile Viewer",
    "section": "Limitations",
    "text": "Limitations\nView-only mode - The mobile viewer does not support:\n\n❌ Adding annotation boundaries\n❌ Editing TextGrid labels\n❌ Adding data points\n❌ Exporting files\n❌ Microphone recording\n\nFor full editing features, use the desktop interface.\nWhy view-only? - Touch gestures conflict with editing operations - Prevents accidental edits on mobile - Focuses on analysis and presentation - Simpler, more stable interface",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#performance",
    "href": "features/mobile-viewer.html#performance",
    "title": "Mobile Viewer",
    "section": "Performance",
    "text": "Performance\nThe mobile viewer is optimized for mobile devices:\n\n\n\nFeature\nMobile Optimization\n\n\n\n\nSpectrogram\nOn-demand computation for visible region\n\n\nWaveform\nDownsampled for fast rendering\n\n\nOverlays\nComputed only when toggled on\n\n\nTouch response\nDebounced for smooth interaction\n\n\n\nBattery usage: - Minimal CPU when static - GPU-accelerated canvas rendering where supported - Pauses computation when app backgrounded",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#browser-support",
    "href": "features/mobile-viewer.html#browser-support",
    "title": "Mobile Viewer",
    "section": "Browser Support",
    "text": "Browser Support\nTested on:\n\n\n\nPlatform\nBrowser\nSupport\n\n\n\n\niOS 14+\nSafari\n✅ Full\n\n\nAndroid 10+\nChrome\n✅ Full\n\n\nAndroid 10+\nFirefox\n✅ Full\n\n\niPadOS 14+\nSafari\n✅ Full",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#use-cases",
    "href": "features/mobile-viewer.html#use-cases",
    "title": "Mobile Viewer",
    "section": "Use Cases",
    "text": "Use Cases\n\nField Research\n\nLoad recordings on smartphone at research site\nAnalyze pitch and formants on device\nShare findings via URL with colleagues\n\n\n\nClassroom Demonstrations\n\nProject mobile viewer on screen via HDMI\nTouch gestures visible to students\nInteractive phonetics demonstrations\nNo installation required\n\n\n\nConference Presentations\n\nEmbed viewer in conference poster QR code\nAttendees scan and explore examples\nSelf-contained analysis in URL\nWorks offline after initial load\n\n\n\nRemote Consultation\n\nShare analysis URL via email/chat\nRecipient can explore interactively\nNo file transfers needed\nCORS-compatible hosting",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#troubleshooting",
    "href": "features/mobile-viewer.html#troubleshooting",
    "title": "Mobile Viewer",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nAudio Won’t Load from URL\nProblem: ?audio= parameter doesn’t load audio\nPossible causes: - CORS not enabled on server - Invalid URL - Mixed content (HTTP audio on HTTPS page)\nSolution: - Ensure server sends Access-Control-Allow-Origin header - Use HTTPS for both page and audio URL - Test URL in browser first - Try data URL instead\n\n\nTouch Gestures Not Working\nProblem: Pinch or drag doesn’t respond\nPossible causes: - Browser zoom enabled (conflicts with pinch) - Touch events blocked by CSS - Old browser version\nSolution: - Disable browser zoom (use in-app zoom instead) - Update to latest browser version - Try different browser (Chrome recommended)\n\n\nValues Bar Shows “—”\nProblem: Measurements show dashes instead of values\nExplanation: - Normal for unvoiced regions (pitch undefined) - Overlays may need to be enabled - May be in silent region\nSolution: - Enable desired overlays in settings drawer - Tap on region with audio - Wait for spectrogram to compute",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/mobile-viewer.html#see-also",
    "href": "features/mobile-viewer.html#see-also",
    "title": "Mobile Viewer",
    "section": "See Also",
    "text": "See Also\n\nEmbedding: Basic Usage - How to embed the viewer\nURL Parameters - Complete parameter reference\nEmbedding Examples - Real-world examples",
    "crumbs": [
      "Mobile Viewer"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html",
    "href": "features/acoustic-overlays.html",
    "title": "Acoustic Overlays",
    "section": "",
    "text": "Acoustic overlays display computed measurements directly on the spectrogram, enabling visual analysis of prosody, voice quality, and spectral properties. All computations use Praat-accurate algorithms via WebAssembly.\n\n\n\nAll acoustic overlays enabled",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#overview",
    "href": "features/acoustic-overlays.html#overview",
    "title": "Acoustic Overlays",
    "section": "",
    "text": "Acoustic overlays display computed measurements directly on the spectrogram, enabling visual analysis of prosody, voice quality, and spectral properties. All computations use Praat-accurate algorithms via WebAssembly.\n\n\n\nAll acoustic overlays enabled",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#available-overlays",
    "href": "features/acoustic-overlays.html#available-overlays",
    "title": "Acoustic Overlays",
    "section": "Available Overlays",
    "text": "Available Overlays\n\nPitch (F0)\nDisplay: Blue line with dots\nFundamental frequency (F0) tracking shows vocal fold vibration rate, essential for analyzing intonation, tone, and stress patterns.\nParameters: - Range: 75-600 Hz (default, adjustable for speaker) - Time step: 10 ms - Algorithm: Praat autocorrelation method\nInterpretation: - Higher pitch = higher blue line - Dots mark detected pitch points (unvoiced regions have no dots) - Missing segments indicate unvoiced sounds (like /s/, /t/, /k/)\n\n\n\n\n\n\nTipPitch Range Settings\n\n\n\nAdjust pitch range for different speakers: - Adult male: 75-300 Hz - Adult female: 100-500 Hz - Children: 200-600 Hz\nConfigure via config.yaml or UI settings.\n\n\n\n\nFormants (F1-F4)\nDisplay: Red dots\nFormant frequencies represent vocal tract resonances, critical for vowel identification and acoustic phonetics.\nParameters: - Number of formants: 5 (tracks F1-F4, ignores F5) - Max formant: 5500 Hz (female), 5000 Hz (male) - Time step: 10 ms - Pre-emphasis: 50 Hz\nDisplay: - F1 (lowest red dots) - Vowel height (high vowels = low F1) - F2 (second row) - Vowel frontness/backness - F3, F4 (upper rows) - Rhotic/retroflex indicators\nFormant Presets:\n\n\n\nPreset\nMax Formant\nNum Formants\nUse Case\n\n\n\n\nMale\n5000 Hz\n5\nAdult male speech\n\n\nFemale\n5500 Hz\n5\nAdult female speech\n\n\nChild\n6500 Hz\n5\nChildren’s voices\n\n\n\n\n\n\n\n\n\nNoteFormant Tracking Accuracy\n\n\n\nFormant tracking may be inaccurate for: - Very high-pitched voices - Noisy recordings - Non-modal phonation (creaky voice, breathy voice) - Nasalized vowels\nAlways verify formant values against auditory perception.\n\n\n\n\nIntensity\nDisplay: Green line\nIntensity (loudness) measured in dB SPL (sound pressure level), useful for identifying stressed syllables and amplitude modulation.\nParameters: - Minimum pitch: 75 Hz (for period detection) - Time step: 10 ms - Smoothing: Yes (Praat default)\nInterpretation: - Higher green line = louder sound - Peaks often correspond to vowels - Valleys correspond to consonants or silence\n\n\nHarmonics-to-Noise Ratio (HNR)\nDisplay: Cyan line\nHNR measures voice quality by comparing periodic (harmonic) energy to aperiodic (noise) energy.\nParameters: - Minimum pitch: 75 Hz - Time step: 10 ms - Silence threshold: 0.1\nInterpretation: - High HNR (&gt;20 dB): Clear, modal voice - Medium HNR (10-20 dB): Normal voice with slight breathiness - Low HNR (&lt;10 dB): Breathy, creaky, or pathological voice\n\n\n\n\n\n\nTipVoice Quality Research\n\n\n\nHNR is useful for: - Distinguishing modal vs. non-modal phonation - Detecting voice disorders - Analyzing consonant voicing - Comparing voice quality across speakers\n\n\n\n\nCenter of Gravity (CoG)\nDisplay: Orange line\nSpectral center of gravity indicates where spectral energy is concentrated, useful for fricative and sibilant analysis.\nInterpretation: - High CoG (&gt;6000 Hz): Alveolar fricatives (/s/, /z/) - Medium CoG (3000-6000 Hz): Postalveolar fricatives (/ʃ/, /ʒ/) - Low CoG (&lt;3000 Hz): Dental/interdental fricatives (/θ/, /ð/)\n\n\nSpectral Tilt\nDisplay: Purple line\nSpectral tilt measures the slope of the spectrum, indicating voice quality and phonation type.\nInterpretation: - Negative tilt: More energy in high frequencies (breathy voice, /h/) - Near-zero tilt: Balanced spectrum (modal voice) - Positive tilt: More energy in low frequencies (creaky voice)\n\n\nA1-P0\nDisplay: Pink line\nA1-P0 (amplitude of first harmonic minus amplitude of first formant) measures nasalization and voice source characteristics.\nInterpretation: - Negative values indicate nasal coupling - Useful for detecting nasal consonants and nasalized vowels",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#toggling-overlays",
    "href": "features/acoustic-overlays.html#toggling-overlays",
    "title": "Acoustic Overlays",
    "section": "Toggling Overlays",
    "text": "Toggling Overlays\n\nVia UI Checkboxes\nClick checkboxes in the overlay panel:\n\nPitch\nFormants\nIntensity\nHNR\nCoG\nSpectral Tilt\nA1-P0\n\nMultiple overlays can be enabled simultaneously.\n\n\nVia Keyboard\nUse number keys to quickly toggle overlays:\n\n\n\nKey\nOverlay\n\n\n\n\nP\nPitch\n\n\nF\nFormants\n\n\nI\nIntensity\n\n\nH\nHNR\n\n\n\n\n\nVia URL Parameters\nPre-configure overlays when embedding:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/?overlays=pitch,formants,intensity\"&gt;\n&lt;/iframe&gt;\nSee URL Parameters for details.",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#values-panel",
    "href": "features/acoustic-overlays.html#values-panel",
    "title": "Acoustic Overlays",
    "section": "Values Panel",
    "text": "Values Panel\nHover over the spectrogram to see overlay values at the cursor position:\nTime: 1.234 s\nFreq: 523 Hz\n\nPitch: 245 Hz\nIntensity: 68 dB\nF1: 720 Hz  B1: 80 Hz\nF2: 1240 Hz B2: 110 Hz\nF3: 2650 Hz B3: 150 Hz\nF4: 3500 Hz B4: 200 Hz\nHNR: 15.3 dB\nCoG: 5420 Hz\nSee Values Panel for details.",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#performance",
    "href": "features/acoustic-overlays.html#performance",
    "title": "Acoustic Overlays",
    "section": "Performance",
    "text": "Performance\n\n\n\nOverlay\nComputation Time (30s audio)\n\n\n\n\nPitch\n~100-150 ms\n\n\nFormants\n~200-300 ms\n\n\nIntensity\n~80-120 ms\n\n\nHNR\n~100-150 ms\n\n\nCoG\n~150-200 ms\n\n\nSpectral Tilt\n~150-200 ms\n\n\nA1-P0\n~100-150 ms\n\n\n\nTotal (all overlays): ~1-1.5 seconds for 30s audio\n\n\n\n\n\n\nNote\n\n\n\nOverlays are computed on-demand when toggled. Once computed, values are cached and redrawn instantly.",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#configuration",
    "href": "features/acoustic-overlays.html#configuration",
    "title": "Acoustic Overlays",
    "section": "Configuration",
    "text": "Configuration\nCustomize overlay parameters via config.yaml:\npitch:\n  floor: 75        # Minimum pitch (Hz)\n  ceiling: 600     # Maximum pitch (Hz)\n  timeStep: 0.01   # Analysis step (seconds)\n\nformantPresets:\n  male:\n    maxFormant: 5000\n    numFormants: 5\n  female:\n    maxFormant: 5500\n    numFormants: 5\n  child:\n    maxFormant: 6500\n    numFormants: 5\n\nintensity:\n  minimumPitch: 75\n  timeStep: 0.01\nSee Configuration Reference for all options.",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#troubleshooting",
    "href": "features/acoustic-overlays.html#troubleshooting",
    "title": "Acoustic Overlays",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nPitch Track Missing\nProblem: No blue dots visible\nPossible causes: - Audio is unvoiced (normal for fricatives, stops) - Pitch range set incorrectly for speaker - Very noisy recording\nSolution: - Adjust pitch floor/ceiling for speaker - Check if audio is actually voiced - Try different time region\n\n\nFormants Look Wrong\nProblem: Red dots appear in unexpected locations\nPossible causes: - Wrong formant preset (male/female/child) - Nasalized vowels - Non-modal phonation - Noise or artifacts\nSolution: - Switch formant preset (Settings → Formant Preset) - Check audio quality - Verify with auditory analysis - Manually verify data point values\n\n\nOverlays Not Computing\nProblem: Overlay checkbox enabled but no visualization\nPossible causes: - WASM not initialized - No audio loaded - Audio too long (&gt;60s) and zoomed out\nSolution: - Wait for WASM to load (status indicator) - Load audio file - Zoom in if file is long",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#research-applications",
    "href": "features/acoustic-overlays.html#research-applications",
    "title": "Acoustic Overlays",
    "section": "Research Applications",
    "text": "Research Applications\n\nProsody Analysis\nUse pitch and intensity overlays to study: - Intonation patterns - Lexical tone - Stress and emphasis - Question vs. statement contours\n\n\nVowel Formants\nUse formant overlay to: - Create vowel plots (F1 vs. F2) - Track vowel formant trajectories - Identify vowel targets - Detect vowel coarticulation\n\n\nVoice Quality\nUse HNR, spectral tilt, and A1-P0 to: - Compare modal, breathy, and creaky voice - Detect pathological voice - Analyze phonation types across languages - Study laryngealization\n\n\nFricative Analysis\nUse CoG and spectral tilt to: - Distinguish /s/ vs. /ʃ/ - Measure acoustic contrast - Study lenition processes - Compare speaker-specific productions",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/acoustic-overlays.html#see-also",
    "href": "features/acoustic-overlays.html#see-also",
    "title": "Acoustic Overlays",
    "section": "See Also",
    "text": "See Also\n\nSpectrogram - Base visualization\nData Points - Collecting measurements\nTutorial: Acoustic Analysis - Step-by-step guide\nConfiguration - Customizing parameters",
    "crumbs": [
      "Acoustic Overlays"
    ]
  },
  {
    "objectID": "features/waveform.html",
    "href": "features/waveform.html",
    "title": "Waveform",
    "section": "",
    "text": "The waveform displays audio amplitude over time, providing a complementary view to the spectrogram. It’s useful for identifying silence, amplitude modulation, and overall signal structure.\n\n\n\nWaveform with audio loaded",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#overview",
    "href": "features/waveform.html#overview",
    "title": "Waveform",
    "section": "",
    "text": "The waveform displays audio amplitude over time, providing a complementary view to the spectrogram. It’s useful for identifying silence, amplitude modulation, and overall signal structure.\n\n\n\nWaveform with audio loaded",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#key-features",
    "href": "features/waveform.html#key-features",
    "title": "Waveform",
    "section": "Key Features",
    "text": "Key Features\n\nSynchronized with spectrogram - Both views share the same time axis\nFull file visualization - Works with arbitrarily long recordings\nAmplitude scaling - Auto-scales to maximum amplitude\nDownsampled rendering - Efficient display at any zoom level\nInteractive - Click to place cursor, drag to select",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#display",
    "href": "features/waveform.html#display",
    "title": "Waveform",
    "section": "Display",
    "text": "Display\nThe waveform shows:\n\nPositive amplitude (above centerline)\nNegative amplitude (below centerline)\nFilled area - Shaded region between positive and negative peaks\n\nVisual properties: - Dark gray filled waveform - Light background - Synchronized with time axis",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#downsampling-strategy",
    "href": "features/waveform.html#downsampling-strategy",
    "title": "Waveform",
    "section": "Downsampling Strategy",
    "text": "Downsampling Strategy\nFor efficient rendering at any zoom level, the waveform uses min/max downsampling:\n\nDivide visible time range into pixel columns\nFor each pixel column, find min and max sample values\nDraw vertical line from min to max\nFill area creates solid waveform appearance\n\nThis approach: - Preserves all peaks and zero-crossings - Renders instantly even for hour-long files - Shows full detail when zoomed in - Prevents aliasing artifacts",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#interaction",
    "href": "features/waveform.html#interaction",
    "title": "Waveform",
    "section": "Interaction",
    "text": "Interaction\n\nCursor Placement\nClick anywhere on waveform to place cursor at that time position. Cursor synchronizes across waveform and spectrogram.\n\n\nTime Selection\nClick and drag horizontally to select time region:\n\nClick starting point\nHold and drag to end point\nRelease to finalize selection\nSelected region highlights in blue\n\nKeyboard: - Space - Play selection (if selected) or visible window - Escape - Clear selection\n\n\nZoom and Pan\n\nScroll wheel - Zoom in/out (centered on cursor)\nHorizontal scroll - Pan left/right through time\nWaveform and spectrogram zoom/pan together",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#use-cases",
    "href": "features/waveform.html#use-cases",
    "title": "Waveform",
    "section": "Use Cases",
    "text": "Use Cases\n\nIdentifying Silence\nWaveform clearly shows silent regions where amplitude approaches zero. Useful for:\n\nDetecting recording start/end\nFinding pauses between utterances\nIdentifying segment boundaries\n\n\n\nAmplitude Modulation\nWaveform visualizes amplitude changes over time:\n\nPeaks - Vowels, stressed syllables\nValleys - Consonants, unstressed syllables\nGradual changes - Prosodic phrase structure\n\n\n\nClipping Detection\nWaveform shows if audio clips (reaches maximum amplitude):\n\nFlat tops/bottoms - Signal clipping, indicates distortion\nSmooth peaks - Proper levels, no clipping\n\n\n\n\n\n\n\nWarningAvoiding Clipping\n\n\n\nIf waveform shows flat-topped peaks, the recording clipped during capture. This distortion cannot be removed. Use the original recording with lower gain settings.\n\n\n\n\nPeriod Detection\nWhen zoomed to millisecond scale, waveform shows individual periods of voiced sounds:\n\nCount zero-crossings to estimate fundamental frequency\nVisualize individual pitch periods\nDetect aperiodicity (e.g., creaky voice, vocal fry)",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#configuration",
    "href": "features/waveform.html#configuration",
    "title": "Waveform",
    "section": "Configuration",
    "text": "Configuration\nWaveform appearance can be customized via config.yaml:\ncolors:\n  waveform: \"#333333\"       # Waveform fill color\n  waveformBackground: \"#FFFFFF\"  # Background",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#performance",
    "href": "features/waveform.html#performance",
    "title": "Waveform",
    "section": "Performance",
    "text": "Performance\n\n\n\nFile Duration\nRender Time\n\n\n\n\n5 seconds\n&lt; 10 ms\n\n\n1 minute\n&lt; 20 ms\n\n\n1 hour\n&lt; 50 ms\n\n\n24 hours\n&lt; 100 ms\n\n\n\nWaveform rendering is near-instantaneous for all file lengths due to efficient downsampling.",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#limitations",
    "href": "features/waveform.html#limitations",
    "title": "Waveform",
    "section": "Limitations",
    "text": "Limitations\nCurrent version: - Read-only (no waveform editing) - Single channel (stereo files mix to mono) - No waveform envelope extraction\nFuture enhancements: - Stereo waveform (dual channel display) - Amplitude envelope overlay - Waveform colorization by intensity",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/waveform.html#see-also",
    "href": "features/waveform.html#see-also",
    "title": "Waveform",
    "section": "See Also",
    "text": "See Also\n\nSpectrogram - Frequency-domain visualization\nAudio Playback - Playing audio\nTutorial: Exploring Audio - Navigation guide",
    "crumbs": [
      "Waveform"
    ]
  },
  {
    "objectID": "features/audio-playback.html",
    "href": "features/audio-playback.html",
    "title": "Audio Playback",
    "section": "",
    "text": "Ozen-web provides high-quality audio playback using the Web Audio API, with support for selection playback, real-time cursor tracking, and keyboard controls.",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#overview",
    "href": "features/audio-playback.html#overview",
    "title": "Audio Playback",
    "section": "",
    "text": "Ozen-web provides high-quality audio playback using the Web Audio API, with support for selection playback, real-time cursor tracking, and keyboard controls.",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#key-features",
    "href": "features/audio-playback.html#key-features",
    "title": "Audio Playback",
    "section": "Key Features",
    "text": "Key Features\n\nSelection playback - Play selected time region\nWindow playback - Play currently visible window\nReal-time cursor - Visual cursor follows playback position\nKeyboard shortcuts - Space/Tab/Escape for quick control\nLoop playback - Repeat selected region (future feature)\nPlayback rate - Slow-motion playback (future feature)",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#playback-controls",
    "href": "features/audio-playback.html#playback-controls",
    "title": "Audio Playback",
    "section": "Playback Controls",
    "text": "Playback Controls\n\nKeyboard Shortcuts\n\n\n\nKey\nAction\n\n\n\n\nSpace\nPlay selection (if selected) or toggle play/pause\n\n\nTab\nPlay visible window\n\n\nEscape\nStop playback and deselect\n\n\n\n\n\nMouse Controls\n\nClick play button - Start/pause playback (if UI button present)\nClick on spectrogram during playback - Seek to position",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#playback-modes",
    "href": "features/audio-playback.html#playback-modes",
    "title": "Audio Playback",
    "section": "Playback Modes",
    "text": "Playback Modes\n\nSelection Playback\nWhen time region is selected:\n\nDrag to select time region (blue highlight)\nPress Space to play selected region\nPlayback starts at selection start\nAutomatically stops at selection end\n\nUse cases: - Listen to specific phone or word - Verify annotation boundary placement - Compare different vowel tokens\n\n\nWindow Playback\nWhen no selection:\n\nZoom to desired time window\nPress Tab to play visible window\nPlayback starts at window start\nAutomatically stops at window end\n\nUse cases: - Play phrase or sentence - Listen while zoomed in on detail - Quick audio review without selecting\n\n\nFull File Playback\n\nPress Space with no selection and fully zoomed out\nPlays entire audio file from current cursor position",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#real-time-cursor-tracking",
    "href": "features/audio-playback.html#real-time-cursor-tracking",
    "title": "Audio Playback",
    "section": "Real-Time Cursor Tracking",
    "text": "Real-Time Cursor Tracking\nDuring playback, a red vertical cursor moves in real-time to show the current playback position:\n\nSynchronized with audio sample-accurately\nUpdates at 60 FPS for smooth animation\nStops at end of playback region\nCan be used to identify timing of acoustic events\n\n\n\n\n\n\n\nTipFinding Event Timing\n\n\n\n\nPlay audio (Space)\nWatch cursor move across spectrogram\nPress Escape when cursor reaches target event\nCursor position shows exact time of event",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#web-audio-api",
    "href": "features/audio-playback.html#web-audio-api",
    "title": "Audio Playback",
    "section": "Web Audio API",
    "text": "Web Audio API\nOzen-web uses the Web Audio API for playback:\n// Internal playback system\nconst audioContext = new AudioContext();\nconst source = audioContext.createBufferSource();\nsource.buffer = audioBuffer;\nsource.connect(audioContext.destination);\nsource.start(0, startTime, duration);\nAdvantages: - Sample-accurate timing - Low latency - Cross-platform consistency - No plugin required\nLimitations: - Requires user gesture to start (browser security) - May require AudioContext resume() on first play",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#playback-state",
    "href": "features/audio-playback.html#playback-state",
    "title": "Audio Playback",
    "section": "Playback State",
    "text": "Playback State\nThe app tracks playback state:\n\nStopped - No playback active (initial state)\nPlaying - Audio currently playing\nPaused - Playback paused (can resume)\n\nState transitions: - Space (no selection) → Playing from cursor - Space (selection) → Playing selection - Space (during playback) → Paused - Space (paused) → Resume from pause position - Escape → Stopped (return to start)",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#selection-workflow",
    "href": "features/audio-playback.html#selection-workflow",
    "title": "Audio Playback",
    "section": "Selection Workflow",
    "text": "Selection Workflow\nEfficient annotation workflow:\n\nPlay full window (Tab) to hear context\nPause (Space) when you hear boundary\nZoom in on boundary region\nSelect boundary region (click + drag)\nPlay selection (Space) to verify\nAdd annotation boundary\nRepeat",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#audio-quality",
    "href": "features/audio-playback.html#audio-quality",
    "title": "Audio Playback",
    "section": "Audio Quality",
    "text": "Audio Quality\nAll playback preserves original audio quality:\n\nSample rate - Matches source file (16 kHz, 44.1 kHz, 48 kHz, etc.)\nBit depth - Converted to Float32 for processing (no quality loss)\nResampling - Web Audio API handles resampling if needed\n\nNo compression or downsampling is applied during playback.",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#browser-compatibility",
    "href": "features/audio-playback.html#browser-compatibility",
    "title": "Audio Playback",
    "section": "Browser Compatibility",
    "text": "Browser Compatibility\nWeb Audio API is supported in all modern browsers:\n\n\n\nBrowser\nSupport\nNotes\n\n\n\n\nChrome 90+\n✅ Full\nBest performance\n\n\nFirefox 88+\n✅ Full\n\n\n\nSafari 14+\n✅ Full\nMay require user gesture\n\n\nEdge 90+\n✅ Full\nChromium-based",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#future-features",
    "href": "features/audio-playback.html#future-features",
    "title": "Audio Playback",
    "section": "Future Features",
    "text": "Future Features\nPlanned playback enhancements:\n\nLoop playback - Continuously repeat selection\nPlayback rate control - 0.5x, 0.75x, 1.5x, 2x speed\nPitch-preserving slow-motion - Slow down without changing pitch\nFade in/out - Smooth start/stop transitions\nVolume control - Master volume slider",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#troubleshooting",
    "href": "features/audio-playback.html#troubleshooting",
    "title": "Audio Playback",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nNo Sound\nProblem: Playback starts but no audio heard\nPossible causes: - System volume muted - Browser audio blocked (autoplay policy) - AudioContext not resumed\nSolution: - Check system volume - Click on page first (browser security requirement) - Try pressing Space again to resume AudioContext\n\n\nPlayback Stutters\nProblem: Audio playback is choppy or glitchy\nPossible causes: - High CPU usage (many browser tabs) - Underpowered device - Large buffer size\nSolution: - Close other tabs - Try in Chrome (best performance) - Reload page to reset AudioContext\n\n\nSelection Won’t Play\nProblem: Pressing Space doesn’t play selected region\nPossible causes: - Selection too short (&lt;10ms) - No audio loaded - Playback already active\nSolution: - Ensure selection is &gt;10ms - Load audio file first - Press Escape to stop current playback",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/audio-playback.html#see-also",
    "href": "features/audio-playback.html#see-also",
    "title": "Audio Playback",
    "section": "See Also",
    "text": "See Also\n\nKeyboard Shortcuts - Full shortcut reference\nTutorial: Exploring Audio - Playback workflow\nWaveform - Visual amplitude display",
    "crumbs": [
      "Audio Playback"
    ]
  },
  {
    "objectID": "features/overview.html",
    "href": "features/overview.html",
    "title": "Features Overview",
    "section": "",
    "text": "Ozen-web is a comprehensive browser-based tool for acoustic analysis and annotation. This page provides an overview of all major features with links to detailed documentation.",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#introduction",
    "href": "features/overview.html#introduction",
    "title": "Features Overview",
    "section": "",
    "text": "Ozen-web is a comprehensive browser-based tool for acoustic analysis and annotation. This page provides an overview of all major features with links to detailed documentation.",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#core-features",
    "href": "features/overview.html#core-features",
    "title": "Features Overview",
    "section": "Core Features",
    "text": "Core Features\n\nAudio Loading & Recording\nLoad audio from multiple sources:\n\nDrag & drop audio files (WAV, MP3, OGG, FLAC)\nFile picker dialog\nMicrophone recording with real-time visualization\nRemote URL loading (CORS-enabled)\nData URL embedding for self-contained iframes\n\nSee: Getting Started Guide\n\n\nVisualization\nWaveform Display:\n\nReal-time amplitude visualization\nSynchronized with spectrogram\nResponsive to zoom and pan\nEfficient downsampling for long files\n\nSpectrogram Display:\n\nPraat-style grayscale spectrogram\nAdjustable frequency range (5/7.5/10 kHz)\nDynamic resolution enhancement when zoomed\nCached rendering for smooth performance\n\nSee: Spectrogram Documentation | Waveform Documentation\n\n\n\nMain interface overview\n\n\n\n\nAcoustic Analysis\nOzen-web provides research-grade acoustic measurements using Praat-compatible algorithms:\nAvailable overlays:\n\n\n\n\n\n\n\n\nFeature\nDescription\nUse Cases\n\n\n\n\nPitch (F0)\nFundamental frequency tracking\nIntonation, stress, tone analysis\n\n\nFormants (F1-F4)\nResonance frequencies\nVowel identification, articulation\n\n\nIntensity\nSound pressure level\nProminence, stress patterns\n\n\nHNR\nHarmonics-to-Noise Ratio\nVoice quality, breathiness, creakiness\n\n\nCoG\nSpectral Center of Gravity\nFricative classification\n\n\nSpectral Tilt\nHigh/low frequency balance\nVoice quality, consonant manner\n\n\nA1-P0\nHarmonic amplitude measure\nNasality, open quotient\n\n\n\nAll measurements are computed using praatfan WebAssembly, providing Praat-accurate results in the browser.\nSee: Acoustic Overlays Documentation | Tutorial: Acoustic Analysis\n\n\n\nSpectrogram with all acoustic overlays\n\n\n\n\nAnnotations\nMulti-tier TextGrid-compatible annotation system:\n\nCreate unlimited annotation tiers\nAdd, move, and remove boundaries\nEdit interval labels with keyboard shortcuts\nBoundary snapping between tiers\nFull Praat TextGrid import/export (both short and long formats)\nInterval and point tier support\n\nEfficient workflow:\n\nDouble-click to add boundaries\nDrag to adjust timing\nRight-click context menus\nKeyboard shortcuts for tier selection (1-5 keys)\nUnified undo/redo (Ctrl+Z / Ctrl+Y)\n\nSee: Annotations Documentation | Tutorial: Annotations\n\n\n\nMulti-tier annotation example\n\n\n\n\nData Points\nPoint-and-click acoustic measurement collection:\nData points allow collecting comprehensive acoustic measurements at specific time/frequency locations:\nFeatures:\n\nDouble-click on spectrogram to add points\nAutomatic capture of all acoustic values (F0, F1-F4, intensity, HNR, etc.)\nInherits labels from ALL annotation tiers at that time\nDrag to adjust position\nExport to TSV for statistical analysis\nImport TSV to restore data points\n\nExport format (TSV):\ntime    freq    pitch   f1  f2  f3  f4  intensity   phones  words\n0.234   1500    156 678 1234    2890    3654    72  i   see\n0.567   1200    234 543 987 2567    3234    68  u   two\nPerfect for building datasets for R, Python, SPSS, or Excel analysis.\nSee: Data Points Documentation | Tutorial: Data Collection\n\n\n\nData points on spectrogram\n\n\n\n\nAudio Playback\nFlexible playback controls:\n\nPlay selected region (Space key)\nPlay visible window (Tab key)\nPlay from cursor to end\nReal-time cursor tracking during playback\nPause and resume\nStop and deselect (Escape key)\n\nSee: Audio Playback Documentation\n\n\nMobile Viewer\nTouch-optimized interface at /viewer route:\n\nTouch gestures:\n\nTap → Place cursor\nDrag → Select region\nTwo-finger drag → Pan view\nPinch → Zoom in/out\n\nCompact layout for small screens\nLandscape mode optimization\nSafe area support for notched phones\nSettings drawer for overlay toggles\nFloating play button\nURL parameters for pre-configuration\n\nEmbedding support:\n\nIframe-compatible\nPre-load audio via ?audio= parameter\nConfigure overlays via ?overlays= parameter\nCORS-enabled remote loading\nData URL support for self-contained embeds\n\nSee: Mobile Viewer Documentation | Embedding Guide\n\n\n\nMobile viewer in landscape mode",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#advanced-features",
    "href": "features/overview.html#advanced-features",
    "title": "Features Overview",
    "section": "Advanced Features",
    "text": "Advanced Features\n\nLong Audio Handling\nFor files &gt;60 seconds:\nOzen-web uses an intelligent on-demand analysis strategy to prevent UI freezing:\n\nOn load: Waveform displays immediately, spectrogram shows “Zoom in for spectrogram”\nWhen zoomed: Analysis runs for visible window only (≤60s)\nOn-demand: Results update automatically as you pan/zoom\nDebounced: 300ms delay prevents excessive recomputation\n\nThis allows working with arbitrarily long recordings (hours of audio) while maintaining responsive UI.\nWhy 60 seconds?\n\nBrowser memory constraints\nReal-time user experience (analysis must complete in &lt;1s)\nBalance between convenience and performance\n\nSee: Getting Started: Troubleshooting\n\n\nMultiple WASM Backends\nChoose from multiple acoustic analysis backends:\n\n\n\n\n\n\n\n\n\n\nBackend\nSource\nLicense\nLoad Time\nOffline\n\n\n\n\npraatfan-local\nBundled (static/wasm/)\nMIT/Apache-2.0\nFastest (~100ms)\n✅ Yes\n\n\npraatfan\nGitHub Pages CDN\nMIT/Apache-2.0\nFast (~500ms)\n❌ No\n\n\npraatfan-gpl\nGitHub Pages CDN\nGPL\nFast (~500ms)\n❌ No\n\n\n\nAll backends provide identical Praat-compatible results. Select via dropdown in the app interface.\nSee: WASM Backends Reference\n\n\nConfiguration\nCustomize via config.yaml:\n# Example configuration\nbackend: \"praatfan-local\"\nmaxFrequency: 5000\nshowPitch: true\nshowFormants: true\ntheme: \"dark\"\n\n# Pitch settings\npitchFloor: 75\npitchCeiling: 600\n\n# Formant settings\nmaxFormants: 5\nmaxFormantFrequency: 5500\n\n# Colors\npitchColor: \"#0000FF\"\nformantColor: \"#FF0000\"\nSee: Configuration Reference\n\n\nUnified Undo/Redo\nSingle undo stack for all editable operations:\n\nAdding/removing/moving boundaries\nEditing interval labels\nAdding/removing/moving data points\nChronological order across all operation types\n\nKeyboard shortcuts:\n\nCtrl+Z (Cmd+Z on Mac) — Undo\nCtrl+Y or Ctrl+Shift+Z — Redo\n\nNon-undoable operations (by design):\n\nAdding/removing tiers\nLoading audio/TextGrid files\n\nSee: Tutorial: Annotations",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#platform-features",
    "href": "features/overview.html#platform-features",
    "title": "Features Overview",
    "section": "Platform Features",
    "text": "Platform Features\n\nOffline & PWA Support\nWorks completely offline:\n\nAll processing happens locally in browser\nNo data uploaded to servers\nAfter initial load, works without internet (with local backend)\nPWA-ready with app icons for home screen installation\n\n\n\nBrowser Compatibility\nRecommended browsers:\n\n✅ Chrome/Edge 90+ (best support)\n✅ Firefox 88+\n✅ Safari 15+ (macOS/iOS)\n✅ Mobile browsers (iOS Safari, Chrome Android)\n\nRequirements:\n\nWebAssembly support\nWeb Audio API\nCanvas API\nES6 modules\n\n\n\nPrivacy\n100% local processing:\n\nAudio never leaves your browser\nNo analytics or tracking\nNo server-side processing\nSuitable for sensitive/confidential data",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#file-format-support",
    "href": "features/overview.html#file-format-support",
    "title": "Features Overview",
    "section": "File Format Support",
    "text": "File Format Support\n\nImport Formats\n\n\n\nFormat\nExtension\nNotes\n\n\n\n\nAudio\n.wav, .mp3, .ogg, .flac, .m4a\nBrowser-dependent\n\n\nTextGrid\n.TextGrid\nBoth short and long formats\n\n\nData Points\n.tsv\nTab-separated values\n\n\nConfiguration\n.yaml\nOptional app configuration\n\n\n\n\n\nExport Formats\n\n\n\nFormat\nContent\nUse Case\n\n\n\n\nTextGrid\nAnnotations (all tiers)\nPraat, R, Python\n\n\nTSV\nData points + acoustic values\nStatistical analysis\n\n\nWAV\nAudio (16-bit PCM)\nSave recordings, share clips\n\n\n\nSee: File Formats Reference",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#comparison-with-desktop-tools",
    "href": "features/overview.html#comparison-with-desktop-tools",
    "title": "Features Overview",
    "section": "Comparison with Desktop Tools",
    "text": "Comparison with Desktop Tools\n\n\n\n\n\n\n\n\n\nFeature\nOzen-web\nPraat\nDesktop Ozen\n\n\n\n\nInstallation\nNone (browser)\nRequired\nRequired\n\n\nPlatform\nAny OS with browser\nOS-specific builds\nWindows/Mac/Linux\n\n\nPrivacy\n100% local\n100% local\n100% local\n\n\nOffline\n✅ Yes (with local backend)\n✅ Yes\n✅ Yes\n\n\nMobile\n✅ Optimized\n❌ No\n❌ No\n\n\nEmbedding\n✅ Iframe support\n❌ No\n❌ No\n\n\nAccuracy\nPraat algorithms (WASM)\nPraat native code\nPraat algorithms (Python)\n\n\nScripting\n❌ No\n✅ Yes\n⚠️ Limited\n\n\nBatch Processing\n❌ No\n✅ Yes\n✅ Yes",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#use-cases",
    "href": "features/overview.html#use-cases",
    "title": "Features Overview",
    "section": "Use Cases",
    "text": "Use Cases\nResearch:\n\nVowel formant analysis (F1/F2 plots)\nPitch contour studies (intonation, tone)\nVoice quality assessment (HNR, spectral tilt)\nConsonant acoustics (VOT, CoG, duration)\n\nTeaching:\n\nEmbed interactive spectrograms in lecture slides\nQuarto/R Markdown integration\nNo software installation required for students\nInteractive demonstrations\n\nLanguage Documentation:\n\nField recording transcription\nTime-aligned corpus creation\nOffline capability for field sites\nMobile device support for fieldwork\n\nClinical:\n\nSpeech therapy progress tracking\nVoice quality visualization\nShareable annotated samples\nPatient education\n\nPodcasting & Audio Production:\n\nVisualize speech clarity\nIdentify problem areas\nMark edit points with sub-second precision\nExport annotations for editing software",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/overview.html#next-steps",
    "href": "features/overview.html#next-steps",
    "title": "Features Overview",
    "section": "Next Steps",
    "text": "Next Steps\nNew users:\n\nGetting Started Guide — Installation and first use\nComplete Tutorial — 30-minute guided walkthrough\n\nSpecific features:\n\nSpectrogram — Visualization details\nAnnotations — TextGrid workflow\nData Points — Measurement collection\nMobile Viewer — Touch interface\n\nReference:\n\nKeyboard Shortcuts — All shortcuts\nConfiguration — Customization options\nWASM Backends — Backend comparison\n\nDevelopment:\n\nArchitecture — Technical design\nContributing — How to contribute",
    "crumbs": [
      "Features Overview"
    ]
  },
  {
    "objectID": "features/data-points.html",
    "href": "features/data-points.html",
    "title": "Data Points",
    "section": "",
    "text": "Data points enable systematic collection of acoustic measurements at specific time-frequency locations. Each point automatically captures all available acoustic values and annotation labels, making it ideal for vowel formant collection, prosody research, and quantitative phonetics.\n\n\n\nData points with values panel",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#overview",
    "href": "features/data-points.html#overview",
    "title": "Data Points",
    "section": "",
    "text": "Data points enable systematic collection of acoustic measurements at specific time-frequency locations. Each point automatically captures all available acoustic values and annotation labels, making it ideal for vowel formant collection, prosody research, and quantitative phonetics.\n\n\n\nData points with values panel",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#key-features",
    "href": "features/data-points.html#key-features",
    "title": "Data Points",
    "section": "Key Features",
    "text": "Key Features\n\nQuick collection - Double-click to add measurement points\nComprehensive measurements - Auto-captures all acoustic values\nAnnotation integration - Includes labels from all annotation tiers\nVisual markers - Yellow dashed lines with position indicators\nDrag to move - Reposition points for precise measurement\nTSV export - Export to tab-separated values for analysis\nTSV import - Re-import previous measurements\nUndo support - Full undo/redo for add/move/remove operations",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#adding-data-points",
    "href": "features/data-points.html#adding-data-points",
    "title": "Data Points",
    "section": "Adding Data Points",
    "text": "Adding Data Points\n\nVia Double-Click\n\nEnable desired overlays (pitch, formants, etc.)\nDouble-click on spectrogram at target location\nYellow dashed line appears at that time\nValues panel shows measurements at that point\n\n\n\n\n\n\n\nTipTargeting Specific Frequencies\n\n\n\nThe vertical position of your double-click doesn’t affect measurements — it only sets the time position. All acoustic values (pitch, formants, intensity, etc.) are computed at that time point automatically.\n\n\n\n\nVia Keyboard\n\nClick to place cursor at desired time\nPress D key to add data point at cursor",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#moving-data-points",
    "href": "features/data-points.html#moving-data-points",
    "title": "Data Points",
    "section": "Moving Data Points",
    "text": "Moving Data Points\nDrag horizontally: 1. Click and hold data point marker (yellow line) 2. Drag left or right to new time position 3. Release to place 4. Measurements update automatically\n\n\n\n\n\n\nNoteUndo Support\n\n\n\nAdding, moving, and removing data points are fully undoable with Ctrl+Z / Cmd+Z.",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#removing-data-points",
    "href": "features/data-points.html#removing-data-points",
    "title": "Data Points",
    "section": "Removing Data Points",
    "text": "Removing Data Points\nRight-click menu: 1. Right-click on data point marker 2. Select “Remove data point”\nKeyboard: 1. Click data point to select 2. Press Delete or Backspace",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#collected-measurements",
    "href": "features/data-points.html#collected-measurements",
    "title": "Data Points",
    "section": "Collected Measurements",
    "text": "Collected Measurements\nEach data point automatically captures:\n\nTime-Frequency Position\n\nTime - Exact time position (seconds)\nFrequency - Cursor frequency at time of placement (Hz)\n\n\n\nAcoustic Measurements\n\nPitch (F0) - Fundamental frequency (Hz)\nIntensity - Sound pressure level (dB)\nFormants - F1, F2, F3, F4 (Hz)\nBandwidths - B1, B2, B3, B4 (Hz)\nHNR - Harmonics-to-noise ratio (dB)\nCoG - Center of gravity (Hz)\nSpectral Tilt - Spectral slope\nA1-P0 - Nasal measure (dB)\n\n\n\nAnnotation Labels\n\nAutomatic text from all annotation tiers at that time point\nColumn per tier: label_words, label_phones, etc.",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#values-panel",
    "href": "features/data-points.html#values-panel",
    "title": "Data Points",
    "section": "Values Panel",
    "text": "Values Panel\nHover over any data point to see its measurements in the values panel:\n📍 Data Point #1\n\nTime: 1.234 s\nFreq: 523 Hz\n\nPitch: 245 Hz\nIntensity: 68 dB\nF1:  720 Hz  B1:  80 Hz\nF2: 1240 Hz  B2: 110 Hz\nF3: 2650 Hz  B3: 150 Hz\nF4: 3500 Hz  B4: 200 Hz\nHNR: 15.3 dB\nCoG: 5420 Hz\n\nAnnotations:\n  words: \"cat\"\n  phones: \"æ\"",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#tsv-export",
    "href": "features/data-points.html#tsv-export",
    "title": "Data Points",
    "section": "TSV Export",
    "text": "TSV Export\nExport all data points to tab-separated values format for statistical analysis.\n\nExport Process\n\nClick “Export Data Points” button\nChoose save location\nFile saves as data-points.tsv\n\n\n\nTSV Format\nHeader row:\ntime    freq    pitch   intensity   f1  f2  f3  f4  b1  b2  b3  b4  hnr cog spectral_tilt   a1_p0   label_words label_phones\nData rows:\n1.234   720 245 68  720 1240    2650    3500    80  110 150 200 15.3    5420    -2.1    -5.4    \"cat\"   \"æ\"\n1.567   850 250 70  850 1180    2580    3450    85  105 145 195 16.1    5380    -1.9    -4.8    \"sat\"   \"æ\"\nNotes: - Tab-separated (TSV), not comma - Missing values: empty field - Text labels: quoted strings - Decimal separator: period (.)\n\n\nImport into R\n# Read TSV file\ndata &lt;- read.table(\"data-points.tsv\", header=TRUE, sep=\"\\t\", quote=\"\\\"\")\n\n# Plot F1 vs F2 vowel space\nlibrary(ggplot2)\nggplot(data, aes(x=f2, y=f1, label=label_phones)) +\n  geom_text() +\n  scale_x_reverse() +\n  scale_y_reverse() +\n  labs(title=\"Vowel Space\", x=\"F2 (Hz)\", y=\"F1 (Hz)\")\n\n\nImport into Python/Pandas\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Read TSV file\ndf = pd.read_csv(\"data-points.tsv\", sep='\\t')\n\n# Plot F1 vs F2\nplt.scatter(df['f2'], df['f1'])\nplt.gca().invert_xaxis()\nplt.gca().invert_yaxis()\nplt.xlabel('F2 (Hz)')\nplt.ylabel('F1 (Hz)')\nplt.title('Vowel Space')\nplt.show()\n\n\nImport into Praat\n# Read table\ntable = Read Table from tab-separated file: \"data-points.tsv\"\n\n# Extract columns\nselectObject: table\nf1 = Get column index: \"f1\"\nf2 = Get column index: \"f2\"\n\n# Plot\nScatter plot: \"f2\", 0, 3000, \"f1\", 0, 1000, \"label_phones\", 12, \"yes\", \"+\"",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#tsv-import",
    "href": "features/data-points.html#tsv-import",
    "title": "Data Points",
    "section": "TSV Import",
    "text": "TSV Import\nRe-import previously exported data points:\n\nClick “Load Data Points” button\nSelect .tsv file\nData points appear on spectrogram\n\n\n\n\n\n\n\nNoteRequirements for Import\n\n\n\n\nFile must have header row with column names\nRequired columns: time (other columns optional)\nAudio must be loaded first\nImported points overwrite existing points",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#use-cases",
    "href": "features/data-points.html#use-cases",
    "title": "Data Points",
    "section": "Use Cases",
    "text": "Use Cases\n\nVowel Formant Collection\nGoal: Measure F1/F2 for vowel categories\nWorkflow: 1. Load audio with vowels 2. Enable formants overlay 3. Add annotation tier for vowel labels 4. Double-click vowel midpoint for each token 5. Export TSV 6. Plot F1 vs F2 in R/Python\nResult: Vowel space plot with formant ellipses per category\n\n\nPitch Contour Sampling\nGoal: Sample pitch at specific phrase positions\nWorkflow: 1. Load audio with intonation of interest 2. Enable pitch overlay 3. Add annotation tier for prosodic events 4. Double-click at onset, peak, offset 5. Export TSV 6. Analyze pitch trajectory\nResult: Quantitative intonation patterns\n\n\nFricative CoG Measurements\nGoal: Measure spectral properties of fricatives\nWorkflow: 1. Load audio with /s/, /ʃ/ contrasts 2. Enable CoG overlay 3. Add phone-level annotations 4. Double-click fricative midpoints 5. Export TSV 6. Compare CoG distributions\nResult: Acoustic distinction between sibilant categories\n\n\nVoice Quality Time Series\nGoal: Track HNR across utterance\nWorkflow: 1. Load audio 2. Enable HNR overlay 3. Add data points at regular intervals (every 50ms) 4. Export TSV 5. Plot HNR over time\nResult: Voice quality trajectory showing modal/non-modal regions",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#visual-appearance",
    "href": "features/data-points.html#visual-appearance",
    "title": "Data Points",
    "section": "Visual Appearance",
    "text": "Visual Appearance\nData points are displayed as:\n\nVertical yellow dashed line - Extends full height of spectrogram\nCircle marker - At bottom of spectrogram\nIndex number - Small label (1, 2, 3, …) for identification\n\nColor customization:\n# config.yaml\ncolors:\n  dataPoint: \"#FFFF00\"  # Yellow (default)",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#keyboard-workflow",
    "href": "features/data-points.html#keyboard-workflow",
    "title": "Data Points",
    "section": "Keyboard Workflow",
    "text": "Keyboard Workflow\nEfficient keyboard-driven data collection:\n\n\n\nKey\nAction\n\n\n\n\nDouble-click spectrogram\nAdd data point\n\n\nD\nAdd data point at cursor\n\n\nClick data point\nSelect\n\n\nDelete / Backspace\nRemove selected point\n\n\nCtrl+Z / Cmd+Z\nUndo add/move/remove\n\n\nCtrl+Y / Cmd+Shift+Z\nRedo\n\n\n\nRapid collection: 1. Play audio (Space) 2. Pause at target (Space) 3. Press D to add point 4. Repeat",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#integration-with-annotations",
    "href": "features/data-points.html#integration-with-annotations",
    "title": "Data Points",
    "section": "Integration with Annotations",
    "text": "Integration with Annotations\nData points automatically integrate with annotation tiers:\nExample scenario:\nAnnotations:\n  words:  | the   | cat   | sat   |\n  phones: | ð | ə | k | æ | t | s | æ | t |\n\nData Points:\n  Point 1 at 0.5s → captures \"the\", \"ə\"\n  Point 2 at 1.2s → captures \"cat\", \"æ\"\n  Point 3 at 1.8s → captures \"sat\", \"æ\"\nTSV output:\ntime   f1   f2   label_words  label_phones\n0.5    520  1720  \"the\"        \"ə\"\n1.2    720  1240  \"cat\"        \"æ\"\n1.8    850  1180  \"sat\"        \"æ\"\nThis enables within-category statistical analysis: “Compare F1 for all /æ/ tokens”",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#performance",
    "href": "features/data-points.html#performance",
    "title": "Data Points",
    "section": "Performance",
    "text": "Performance\n\n\n\nOperation\nTime\n\n\n\n\nAdd point\n&lt; 10 ms\n\n\nMove point\n&lt; 10 ms\n\n\nRemove point\n&lt; 10 ms\n\n\nExport 100 points\n&lt; 100 ms\n\n\nImport 100 points\n&lt; 100 ms\n\n\n\nData point operations are near-instantaneous, enabling rapid collection workflows.",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#limitations",
    "href": "features/data-points.html#limitations",
    "title": "Data Points",
    "section": "Limitations",
    "text": "Limitations\nCurrent version: - All data points at time resolution (can’t select specific frequency for formant) - No visual connection between points (no trajectories) - TSV is the only export format (no CSV, JSON, XLSX)\nFuture enhancements: - Click-and-drag formant tracking - Visual trajectories connecting sequential points - Multiple export formats - Statistical summary export (means, SDs per category)",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#troubleshooting",
    "href": "features/data-points.html#troubleshooting",
    "title": "Data Points",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nData Point Not Appearing\nProblem: Double-click doesn’t create point\nPossible causes: - Not clicking on spectrogram (clicking annotation tier instead) - Spectrogram not loaded (for long files, zoom in first)\nSolution: - Ensure spectrogram is visible - Double-click directly on spectrogram canvas - Zoom in if file is &gt;60 seconds\n\n\nMissing Values in Export\nProblem: TSV has empty cells\nExplanation: This is normal when: - Overlay not enabled (pitch values missing if pitch overlay off) - Unvoiced region (pitch undefined for /s/, /t/) - No annotation at that time (label columns empty)\nSolution: - Enable all desired overlays before adding points - Accept missing values as linguistically meaningful - Filter during analysis (e.g., remove unvoiced tokens)\n\n\nCan’t Move Data Point\nProblem: Point won’t drag\nSolution: - Click precisely on yellow line - Ensure not in text edit mode - Try zooming in for better precision",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "features/data-points.html#see-also",
    "href": "features/data-points.html#see-also",
    "title": "Data Points",
    "section": "See Also",
    "text": "See Also\n\nTutorial: Data Collection - Step-by-step guide\nAcoustic Overlays - Understanding measurements\nAnnotations - Creating annotation labels\nExporting - Export workflows",
    "crumbs": [
      "Data Points"
    ]
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started",
    "section": "",
    "text": "Ozen-web is a static web application that runs entirely in your browser. You can:\n\nUse the hosted version (easiest) — No setup required\nRun locally for development — Full control and offline access\nDeploy your own instance — Host on GitHub Pages, Netlify, or any static host"
  },
  {
    "objectID": "getting-started.html#overview",
    "href": "getting-started.html#overview",
    "title": "Getting Started",
    "section": "",
    "text": "Ozen-web is a static web application that runs entirely in your browser. You can:\n\nUse the hosted version (easiest) — No setup required\nRun locally for development — Full control and offline access\nDeploy your own instance — Host on GitHub Pages, Netlify, or any static host"
  },
  {
    "objectID": "getting-started.html#option-1-use-hosted-version",
    "href": "getting-started.html#option-1-use-hosted-version",
    "title": "Getting Started",
    "section": "Option 1: Use Hosted Version",
    "text": "Option 1: Use Hosted Version\nThe fastest way to get started is using the hosted version:\n🌐 https://ucpresearch.github.io/ozen-web/live/\nSimply visit the URL and drag an audio file (WAV, MP3, OGG) to begin analyzing.\n\n\n\n\n\n\nTip\n\n\n\nBookmark the URL for quick access. All processing happens locally in your browser — no data is uploaded to servers.\n\n\n\nLoading Your First Audio File\n\n\nDrag and drop a WAV, MP3, or OGG file onto the interface\nOR\nClick “Load Audio” button and select a file\nWait for the waveform to appear (1-2 seconds for typical files)\nThe spectrogram will load automatically (or zoom in for long files)\nEnable acoustic overlays using checkboxes: Pitch, Formants, Intensity\n\n\n\nDon’t have an audio file handy? Record directly in the browser by clicking the microphone icon 🎤 (requires microphone permission).\n\n\n\nWhat’s Next?\n\nFollow the Complete Tutorial for a guided walkthrough\nExplore Features Documentation for detailed feature guides\nCheck Keyboard Shortcuts for efficient workflow"
  },
  {
    "objectID": "getting-started.html#option-2-run-locally",
    "href": "getting-started.html#option-2-run-locally",
    "title": "Getting Started",
    "section": "Option 2: Run Locally",
    "text": "Option 2: Run Locally\nFor development or offline use, run Ozen-web on your machine.\n\nPrerequisites\n\nAny local web server, for instance python -m http.server 8080, started from one of the parent directories\n\n\n\nDownload\n\ngit clone https://github.com/ucpresearch/ozen-web.git\ncd ozen-web/docs\npython -m http.server 8080 (adapt the port – 8080 – as necessary)\nOpen the following url (adapt the port as necessary): (http://localhost:8080/live/index.html)[http://localhost:8080/live/index.html]\nnotice that this method doesn’t use https, and your browser may complain about it"
  },
  {
    "objectID": "getting-started.html#option-3-deploy-your-own-instance",
    "href": "getting-started.html#option-3-deploy-your-own-instance",
    "title": "Getting Started",
    "section": "Option 3: Deploy Your Own Instance",
    "text": "Option 3: Deploy Your Own Instance\nThe build/ or docs/live (same content) folder will work with any static file host:\n\nAWS S3 + CloudFront\nGoogle Cloud Storage\nAzure Static Web Apps\nCloudflare Pages\nGitLab Pages\n\nJust upload the build/ folder’s contents."
  },
  {
    "objectID": "getting-started.html#configuration",
    "href": "getting-started.html#configuration",
    "title": "Getting Started",
    "section": "Configuration",
    "text": "Configuration\n\nBackend Selection\nOzen-web supports multiple WASM backends:\n\n\n\n\n\n\n\n\n\n\nBackend\nSource\nLicense\nSpeed\nOffline\n\n\n\n\npraatfan-local\nstatic/wasm/\nMIT/Apache-2.0\nFastest\n✅ Yes\n\n\npraatfan\nGitHub Pages CDN\nMIT/Apache-2.0\nFast\n❌ No\n\n\npraatfan-gpl\nGitHub Pages CDN\nGPL\nFast\n❌ No\n\n\n\nSelect the backend using the dropdown in the app interface.\nSee WASM Backends Reference for detailed comparison.\n\n\nCustom Configuration\nCreate static/config.yaml to customize default settings:\n# Example configuration\nbackend: \"praatfan-local\"\nmaxFrequency: 5000\nshowPitch: true\nshowFormants: true\ntheme: \"dark\"\nSee Configuration Reference for all options."
  },
  {
    "objectID": "getting-started.html#troubleshooting",
    "href": "getting-started.html#troubleshooting",
    "title": "Getting Started",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nWASM Fails to Load\n\nCheck backend selection: Try switching between backends in the dropdown\nClear browser cache: Hard refresh with Ctrl+Shift+R (Cmd+Shift+R on Mac)\nCheck browser console: Open DevTools (F12) → Console tab for error messages\n\n\n\nAudio File Won’t Load\n\nSupported formats: WAV, MP3, OGG (16-bit or 24-bit PCM for WAV)\nFile size: Files &gt;100MB may be slow; try shorter clips\nBrowser support: Some browsers have codec limitations (try Chrome/Edge)\n\n\n\nLong Audio Files Show “Zoom in for spectrogram”\nThis is expected behavior for files &gt;60 seconds. Zoom in to a window ≤60s to trigger analysis.\nSee Features: Long Audio Handling for details."
  },
  {
    "objectID": "getting-started.html#getting-help",
    "href": "getting-started.html#getting-help",
    "title": "Getting Started",
    "section": "Getting Help",
    "text": "Getting Help\n\nDocumentation: Browse Features and Reference sections\nTutorial: Follow the step-by-step guide\nIssues: Report bugs at GitHub Issues"
  },
  {
    "objectID": "getting-started.html#whats-next-1",
    "href": "getting-started.html#whats-next-1",
    "title": "Getting Started",
    "section": "What’s Next?",
    "text": "What’s Next?\nReady to dive deeper? Try the:\n\nComplete Tutorial — Guided walkthrough from loading audio to exporting data\nFeatures Overview — Detailed documentation of all features\nEmbedding Guide — Embed Ozen-web in your websites or documents\n\n\nAll set? Head to the Tutorial to learn the complete workflow. →"
  },
  {
    "objectID": "tutorial/04-annotations.html",
    "href": "tutorial/04-annotations.html",
    "title": "4. Annotations",
    "section": "",
    "text": "Annotations let you add time-aligned transcriptions to your audio. In this section, you’ll learn:\n\nCreating annotation tiers\nAdding and removing boundaries\nEditing interval labels\nUsing keyboard shortcuts for efficient annotation\nUnderstanding tier types (interval and point)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#overview",
    "href": "tutorial/04-annotations.html#overview",
    "title": "4. Annotations",
    "section": "",
    "text": "Annotations let you add time-aligned transcriptions to your audio. In this section, you’ll learn:\n\nCreating annotation tiers\nAdding and removing boundaries\nEditing interval labels\nUsing keyboard shortcuts for efficient annotation\nUnderstanding tier types (interval and point)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#what-are-annotation-tiers",
    "href": "tutorial/04-annotations.html#what-are-annotation-tiers",
    "title": "4. Annotations",
    "section": "What Are Annotation Tiers?",
    "text": "What Are Annotation Tiers?\nAnnotation tiers divide your audio into labeled intervals, similar to Praat’s TextGrid format.\nCommon uses:\n\nPhonetic transcription: Mark phones/phonemes\nWord alignment: Segment words with timestamps\nProsody: Label pitch accents and breaks\nEvents: Mark breathing, laughter, noise\n\n\n\n\nMulti-tier annotation example",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#creating-your-first-tier",
    "href": "tutorial/04-annotations.html#creating-your-first-tier",
    "title": "4. Annotations",
    "section": "Creating Your First Tier",
    "text": "Creating Your First Tier\n\n\nClick the “+ Add Tier” button (usually below the spectrogram)\n\n\n\nAdd tier button\n\n\nEnter a tier name in the dialog:\n\nExamples: “phones”, “words”, “syllables”\nNames should be descriptive and lowercase\n\nSelect tier type:\n\nInterval tier (most common) — Divides time into labeled segments\nPoint tier — Marks specific time points (less common)\n\nClick “Create”\nA new tier appears below the spectrogram\n\n\n\n\n\n\n\n\nTip\n\n\n\nCreate multiple tiers for different levels of analysis (e.g., “words” tier above “phones” tier).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#adding-boundaries",
    "href": "tutorial/04-annotations.html#adding-boundaries",
    "title": "4. Annotations",
    "section": "Adding Boundaries",
    "text": "Adding Boundaries\nBoundaries divide a tier into intervals. Each interval can have a text label.\n\n\nDouble-click on a tier at the time position where you want a boundary\n\n\n\nAdding a boundary\n\n\nA vertical boundary line appears\nThis creates two intervals: before and after the boundary\nRepeat to add more boundaries\n\n\nKeyboard shortcut:\n\nDouble-click on tier — Add boundary at that time position\n\n\nTry this: Zoom in on a word and add boundaries at the start and end of each phone. You’ll create a series of intervals corresponding to individual sounds.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#editing-interval-labels",
    "href": "tutorial/04-annotations.html#editing-interval-labels",
    "title": "4. Annotations",
    "section": "Editing Interval Labels",
    "text": "Editing Interval Labels\nEach interval can have a text label (e.g., a phone symbol, word, or comment).\n\n\nDouble-click on an interval (the space between two boundaries)\nA text input field appears\n\n\n\nEditing an interval label\n\n\nType the label (e.g., “a”, “s”, “hello”, “breath”)\nPress Enter to save\nPress Escape to cancel without saving\n\n\nKeyboard shortcuts:\n\nDouble-click interval — Edit label\nEnter — Save label\nEscape — Cancel editing\nTab — Move to next interval (if implemented)\n\n\n\n\n\n\n\nTip\n\n\n\nUse IPA symbols for phonetic transcription. Most browsers support Unicode IPA input. Install an IPA keyboard or use copy-paste from a character palette.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#moving-boundaries",
    "href": "tutorial/04-annotations.html#moving-boundaries",
    "title": "4. Annotations",
    "section": "Moving Boundaries",
    "text": "Moving Boundaries\nYou can adjust boundary positions by dragging:\n\n\nClick on a boundary line\nDrag left or right to adjust its position\nRelease to finalize the position\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen you move a boundary, both adjacent intervals resize accordingly. The boundary to the left and right of the moved boundary stay fixed.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#removing-boundaries",
    "href": "tutorial/04-annotations.html#removing-boundaries",
    "title": "4. Annotations",
    "section": "Removing Boundaries",
    "text": "Removing Boundaries\n\n\nRight-click on a boundary line\nSelect “Remove boundary” from the context menu\n\n\n\nBoundary context menu\n\n\nThe boundary is deleted, merging the two adjacent intervals\nThe merged interval’s label becomes the label of the left interval\n\n\nKeyboard shortcut (if implemented):\n\nSelect boundary + Delete key — Remove selected boundary\n\n\n\n\n\n\n\nWarning\n\n\n\nRemoving a boundary merges intervals permanently (unless you undo). Use Ctrl+Z to undo if needed.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#undo-and-redo",
    "href": "tutorial/04-annotations.html#undo-and-redo",
    "title": "4. Annotations",
    "section": "Undo and Redo",
    "text": "Undo and Redo\nOzen-web has unified undo/redo for all annotation actions:\nKeyboard shortcuts:\n\nCtrl+Z (Windows/Linux) or Cmd+Z (Mac) — Undo last action\nCtrl+Y or Ctrl+Shift+Z — Redo\n\nUndoable actions:\n\nAdding boundaries\nRemoving boundaries\nMoving boundaries\nEditing interval labels\nAdding/removing data points (section 5)\n\nNon-undoable actions (by design):\n\nAdding/removing tiers\nLoading audio files\nLoading TextGrid files\n\n\n\n\n\n\n\nTip\n\n\n\nGet comfortable with Ctrl+Z! It allows you to experiment freely without worrying about mistakes.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#working-with-multiple-tiers",
    "href": "tutorial/04-annotations.html#working-with-multiple-tiers",
    "title": "4. Annotations",
    "section": "Working with Multiple Tiers",
    "text": "Working with Multiple Tiers\nCreate hierarchical annotations with multiple tiers:\n\n\nAdd a “words” tier (click “+ Add Tier”)\nAdd a “phones” tier\nIn the words tier, add boundaries at word boundaries\nIn the phones tier, add boundaries at phone boundaries (within each word)\nLabel each interval appropriately\n\n\n\n\n\nMulti-tier annotation\n\n\n\nTier Ordering\nTiers are displayed bottom-to-top in the order they were created. Some implementations allow reordering (check your version).\nConvention: Place higher-level tiers (words, phrases) above lower-level tiers (phones).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#boundary-snapping",
    "href": "tutorial/04-annotations.html#boundary-snapping",
    "title": "4. Annotations",
    "section": "Boundary Snapping",
    "text": "Boundary Snapping\nOzen-web supports boundary snapping to help align tiers:\n\n\nWhen adding a boundary on a lower tier, move your cursor close to an existing boundary on an upper tier\nThe cursor “snaps” to align with the upper tier’s boundary\nDouble-click to place the boundary at the snapped position\n\n\n\n\n\n\n\n\nTip\n\n\n\nThis feature helps maintain consistent alignment across tiers (e.g., ensuring word boundaries align across “words” and “phones” tiers).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#selecting-annotation-tiers",
    "href": "tutorial/04-annotations.html#selecting-annotation-tiers",
    "title": "4. Annotations",
    "section": "Selecting Annotation Tiers",
    "text": "Selecting Annotation Tiers\nSome operations require selecting which tier to work on:\nKeyboard shortcuts:\n\n1 — Select tier 1\n2 — Select tier 2\n3 — Select tier 3\n4 — Select tier 4\n5 — Select tier 5\n\nThe selected tier is highlighted or indicated visually.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#example-workflow",
    "href": "tutorial/04-annotations.html#example-workflow",
    "title": "4. Annotations",
    "section": "Example Workflow",
    "text": "Example Workflow\nLet’s annotate a short phrase: “hello world”\n\n\nLoad audio (section 1) with someone saying “hello world”\nAdd a “words” tier\nZoom to see the full phrase\nDouble-click at the start of “hello” to add a boundary\nDouble-click between “hello” and “world” to add a boundary\nDouble-click at the end of “world” to add a boundary\nNow you have three intervals:\n\nSilence before “hello”\n“hello”\n“world”\nSilence after (implicit, extends to end)\n\nDouble-click the first interval and type “hello”, press Enter\nDouble-click the second interval and type “world”, press Enter\nAdd a “phones” tier\nZoom in on “hello”\nAdd boundaries and labels for each phone:\n\n/h/ [h]\n/ɛ/ [ɛ] or [e]\n/l/ [l]\n/oʊ/ [oʊ] or [o]\n\nRepeat for “world”:\n\n/w/ [w]\n/ɝ/ [ɝ] or [ər]\n/l/ [l]\n/d/ [d]\n\n\n\n\n\n\nCompleted annotation example",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#practice-exercises",
    "href": "tutorial/04-annotations.html#practice-exercises",
    "title": "4. Annotations",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nCreate a “syllables” tier\n\nMark syllable boundaries in a word\nLabel each syllable\n\nCreate a “phones” tier\n\nTranscribe a short word phonetically\nUse IPA symbols if possible\n\nPractice undo/redo\n\nAdd several boundaries\nUndo them all (Ctrl+Z repeatedly)\nRedo (Ctrl+Y)\n\nMove boundaries\n\nAdd a boundary slightly off\nDrag it to the correct position\nUse playback (Space) to verify alignment\n\n\n\nChallenge: Annotate a complete sentence with two tiers:\n\nWords tier: Segment into words\nPhones tier: Transcribe each word phonetically\n\nUse boundary snapping to align phone boundaries with word boundaries.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#troubleshooting",
    "href": "tutorial/04-annotations.html#troubleshooting",
    "title": "4. Annotations",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nCan’t add boundary:\n\nEnsure you’re double-clicking on the tier, not on an interval or the spectrogram\nCheck that the tier is selected (if your version requires selection)\n\nBoundary appears in wrong location:\n\nZoom in for more precise placement\nYou can drag the boundary to adjust after placing\n\nCan’t edit label:\n\nDouble-click the interval (space between boundaries), not the boundary line itself\nEnsure you’re double-clicking, not single-clicking\n\nUndo doesn’t work:\n\nCheck that you’re using the correct keyboard shortcut (Ctrl+Z, not Alt+Z)\nEnsure the app window has focus (click on it first)\n\nTier names overlap:\n\nTier labels may overlap if many tiers are shown; this is a UI limitation\nUse short tier names (“W” for words, “P” for phones) if needed",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/04-annotations.html#whats-next",
    "href": "tutorial/04-annotations.html#whats-next",
    "title": "4. Annotations",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow that you can create annotations, let’s learn how to collect acoustic measurements at specific time points.\nNext: 5. Data Collection →\n\nNavigation: ← Previous: Acoustic Analysis | Tutorial Overview | Next: Data Collection →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "4. Annotations"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html",
    "href": "tutorial/02-exploring-audio.html",
    "title": "2. Exploring Audio",
    "section": "",
    "text": "In this section, you’ll learn essential navigation techniques:\n\nPlacing the cursor\nSelecting audio regions\nZooming in and out\nPanning the view\nPlaying audio",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#overview",
    "href": "tutorial/02-exploring-audio.html#overview",
    "title": "2. Exploring Audio",
    "section": "",
    "text": "In this section, you’ll learn essential navigation techniques:\n\nPlacing the cursor\nSelecting audio regions\nZooming in and out\nPanning the view\nPlaying audio",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#the-interface-layout",
    "href": "tutorial/02-exploring-audio.html#the-interface-layout",
    "title": "2. Exploring Audio",
    "section": "The Interface Layout",
    "text": "The Interface Layout\nAfter loading audio, you’ll see:\n\nWaveform panel (top) — Shows amplitude over time\nSpectrogram panel (middle) — Shows frequency content over time\nAnnotation tiers (bottom) — For transcription (covered in section 4)\nTime axis — Shows time in seconds\nValues panel (right) — Shows acoustic measurements at cursor",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#placing-the-cursor",
    "href": "tutorial/02-exploring-audio.html#placing-the-cursor",
    "title": "2. Exploring Audio",
    "section": "Placing the Cursor",
    "text": "Placing the Cursor\nThe cursor is a vertical red line that shows the current time position.\n\n\nClick anywhere on the waveform or spectrogram\nThe red cursor line appears at that time position\n\n\n\nCursor placement\n\n\nThe values panel updates to show acoustic measurements at the cursor time",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#selecting-audio-regions",
    "href": "tutorial/02-exploring-audio.html#selecting-audio-regions",
    "title": "2. Exploring Audio",
    "section": "Selecting Audio Regions",
    "text": "Selecting Audio Regions\nSelect a time range to play or analyze a specific section:\n\n\nClick and drag across the waveform or spectrogram\nA blue highlighted region shows your selection\n\n\n\nSelected region\n\n\nRelease the mouse to finalize the selection\nThe selection remains until you click elsewhere or press Escape\n\n\n\nTry selecting a single word or syllable in your audio. The selection will be useful for playback (next section).\n\nKeyboard shortcut:\n\nEscape — Clear selection",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#zooming",
    "href": "tutorial/02-exploring-audio.html#zooming",
    "title": "2. Exploring Audio",
    "section": "Zooming",
    "text": "Zooming\nZooming lets you see fine-grained detail in the audio.\n\n\nUse Up/Down arrow keys:\n\nUp arrow → Zoom in (expands time scale)\nDown arrow → Zoom out (compresses time scale)\nZoom is centered on the visible window center\n\nOR\nUse mouse wheel (centered on mouse position):\n\nScroll up → Zoom in\nScroll down → Zoom out\n\n\n\n\nZoom demonstration\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nArrow keys zoom centered on the visible window center, while mouse wheel zooms centered on the mouse pointer. Use whichever is more convenient!\n\n\n\nZoom Levels\nYou can zoom from:\n\nMaximum zoom out: Entire audio file visible (e.g., 0-300s)\nMaximum zoom in: Individual pitch periods visible (~20-50ms windows)\n\n\n\n\n\n\n\nNote\n\n\n\nFor files &gt;60 seconds, the spectrogram only computes for the visible time window when zoomed. This prevents UI freezing on long recordings.\n\n\nMouse wheel zoom:\n\nMouse wheel — Zoom centered on mouse position",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#panning",
    "href": "tutorial/02-exploring-audio.html#panning",
    "title": "2. Exploring Audio",
    "section": "Panning",
    "text": "Panning\nPan the view to explore different parts of the audio horizontally.\n\n\nPress Left/Right arrow keys to pan left or right (10% of visible window per press)\nOR\nTwo-finger horizontal swipe on trackpad (if available)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#playing-audio",
    "href": "tutorial/02-exploring-audio.html#playing-audio",
    "title": "2. Exploring Audio",
    "section": "Playing Audio",
    "text": "Playing Audio\nPlay your audio or selected regions to hear what you’re analyzing.\n\nPlay Entire Visible Window\n\n\nPress Tab key\nAudio plays from the start to the end of the visible window\nA moving green playback cursor shows the current position\nPress Escape or Space to stop\n\n\n\n\nPlay Selected Region\n\n\nSelect a region (click and drag)\nPress Space key\nOnly the selected audio plays\nPress Space again to pause, or Escape to stop and deselect\n\n\n\n\nPlay from Cursor\n\n\nClick to place the cursor at a time position\nPress Space\nAudio plays from cursor to the end of the visible window\n\n\nKeyboard shortcuts:\n\n\n\nKey\nAction\n\n\n\n\nSpace\nPlay selection (or from cursor if no selection)\n\n\nTab\nPlay entire visible window\n\n\nEscape\nStop playback and clear selection",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#combining-navigation-techniques",
    "href": "tutorial/02-exploring-audio.html#combining-navigation-techniques",
    "title": "2. Exploring Audio",
    "section": "Combining Navigation Techniques",
    "text": "Combining Navigation Techniques\nEfficient workflow combines all these techniques:\n\n\nZoom out to see the full file (scroll wheel down)\nClick on an interesting region (e.g., a word or phrase)\nZoom in on that region (scroll wheel up)\nSelect a portion (click and drag)\nPlay the selection (Space key)\nPlace cursor on a specific point (click)\nRead acoustic values in the values panel",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#the-values-panel",
    "href": "tutorial/02-exploring-audio.html#the-values-panel",
    "title": "2. Exploring Audio",
    "section": "The Values Panel",
    "text": "The Values Panel\nThe values panel (right side) shows acoustic measurements at the cursor position:\n\nTime — Current cursor position (seconds)\nFrequency — Frequency under mouse (Hz)\nPitch (F0) — Fundamental frequency (Hz)\nIntensity — Sound pressure level (dB)\nF1, F2, F3, F4 — Formant frequencies (Hz)\nHNR — Harmonics-to-Noise Ratio (dB)\nCoG — Spectral center of gravity (Hz)\nMore — Additional measurements\n\n\n\n\nValues panel showing measurements\n\n\n\n\n\n\n\n\nNote\n\n\n\nValues are only shown when acoustic overlays are enabled (covered in the next section). If you see “—” for all values, you need to enable overlays first.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#practice-exercises",
    "href": "tutorial/02-exploring-audio.html#practice-exercises",
    "title": "2. Exploring Audio",
    "section": "Practice Exercises",
    "text": "Practice Exercises\nBefore moving to the next section, practice these skills:\n\nZoom in to see a single word clearly\nSelect that word and play it (Space)\nPlace the cursor in the middle of a vowel\nZoom in further to see individual pitch periods\nPan left and right to explore neighboring sounds\nZoom out to see the full audio again\n\n\nChallenge: Find three vowels in your audio. For each one:\n\nZoom to show just that vowel\nPlace cursor in the middle\nNote the approximate pitch (you’ll see this more clearly in the next section)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#troubleshooting",
    "href": "tutorial/02-exploring-audio.html#troubleshooting",
    "title": "2. Exploring Audio",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nZoom isn’t working:\n\nEnsure mouse is over the waveform/spectrogram area\nTry using +/- keys instead of mouse wheel\nCheck that your browser allows scroll events\n\nPlayback doesn’t work:\n\nCheck browser audio isn’t muted\nEnsure audio output device is connected\nTry clicking the page first (browsers require user interaction for audio)\n\nCursor jumps unexpectedly:\n\nThis happens if you click during playback\nPress Escape to stop playback first\n\nSpectrogram shows “Zoom in for spectrogram”:\n\nYour file is &gt;60 seconds\nZoom in to a window ≤60 seconds to trigger analysis\nThis is normal behavior for long files",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/02-exploring-audio.html#whats-next",
    "href": "tutorial/02-exploring-audio.html#whats-next",
    "title": "2. Exploring Audio",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow that you can navigate the audio, let’s enable acoustic overlays to see pitch, formants, and intensity.\nNext: 3. Acoustic Analysis →\n\nNavigation: ← Previous: Loading Audio | Tutorial Overview | Next: Acoustic Analysis →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "2. Exploring Audio"
    ]
  },
  {
    "objectID": "tutorial/index.html",
    "href": "tutorial/index.html",
    "title": "Complete Tutorial",
    "section": "",
    "text": "This tutorial will guide you through the complete workflow of using Ozen-web for acoustic analysis and annotation. By the end, you’ll know how to:\n\nLoad and explore audio files\nView acoustic features (pitch, formants, intensity)\nCreate and edit annotation tiers\nCollect acoustic measurements at specific points\nExport your annotations and data for analysis\n\nPrerequisites:\n\nOzen-web running (Getting Started Guide). The GitHub-hosted server should be enough for most purposes.\nA sample audio file (.wav, .flac, .mp3, or .ogg). You can also record yourself.\nBasic familiarity with spectrograms (helpful but not required)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#overview",
    "href": "tutorial/index.html#overview",
    "title": "Complete Tutorial",
    "section": "",
    "text": "This tutorial will guide you through the complete workflow of using Ozen-web for acoustic analysis and annotation. By the end, you’ll know how to:\n\nLoad and explore audio files\nView acoustic features (pitch, formants, intensity)\nCreate and edit annotation tiers\nCollect acoustic measurements at specific points\nExport your annotations and data for analysis\n\nPrerequisites:\n\nOzen-web running (Getting Started Guide). The GitHub-hosted server should be enough for most purposes.\nA sample audio file (.wav, .flac, .mp3, or .ogg). You can also record yourself.\nBasic familiarity with spectrograms (helpful but not required)",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#sample-files",
    "href": "tutorial/index.html#sample-files",
    "title": "Complete Tutorial",
    "section": "Sample Files",
    "text": "Sample Files\nFor this tutorial, you can use:\n\nYour own audio: Any .wav, .flac, .mp3, .ogg file of speech\nRecord in-browser: Use the microphone icon to record a few sentences\nDownload samples: Example audio files from this documentation\n\n\n\n\n\n\n\nTip\n\n\n\nFor your first time, use a short file (5-15 seconds) to see results quickly. You can try longer files later.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#tutorial-sections",
    "href": "tutorial/index.html#tutorial-sections",
    "title": "Complete Tutorial",
    "section": "Tutorial Sections",
    "text": "Tutorial Sections\n\n\n1. Loading Audio\nLearn three ways to load audio files: drag-and-drop, file picker, and microphone recording.\n\n\n2. Exploring Audio\nNavigate the interface with zoom, pan, cursor placement, and audio playback.\n\n\n3. Acoustic Analysis\nEnable and interpret pitch, formants, intensity, and other acoustic overlays.\n\n\n4. Annotations\nCreate annotation tiers, add boundaries, edit labels, and use keyboard shortcuts.\n\n\n5. Data Collection\nAdd data points to collect acoustic measurements at specific time/frequency locations.\n\n\n6. Exporting\nExport TextGrids, data points (TSV), and audio files for further analysis.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#learning-path",
    "href": "tutorial/index.html#learning-path",
    "title": "Complete Tutorial",
    "section": "Learning Path",
    "text": "Learning Path\nComplete beginner? Follow sections 1-6 in order.\nFamiliar with spectrograms? Skip to section 3 (Acoustic Analysis) or 4 (Annotations).\nJust need annotation? Jump to section 4 (Annotations) and 6 (Exporting).\nJust need measurements? Go to sections 3 (Acoustic Analysis) and 5 (Data Collection).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#what-youll-create",
    "href": "tutorial/index.html#what-youll-create",
    "title": "Complete Tutorial",
    "section": "What You’ll Create",
    "text": "What You’ll Create\nBy the end of this tutorial, you’ll have:\n\n✅ An annotated audio file with multi-tier transcription\n✅ A TextGrid file (Praat format) for further analysis\n✅ A TSV file with acoustic measurements and labels\n✅ Skills to analyze your own research audio",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#tips-for-success",
    "href": "tutorial/index.html#tips-for-success",
    "title": "Complete Tutorial",
    "section": "Tips for Success",
    "text": "Tips for Success\n\n\n\n\n\n\nTip\n\n\n\nUse keyboard shortcuts — They’re listed in each section and dramatically speed up your workflow.\n\n\n\nExperiment freely — The app has undo/redo (Ctrl+Z / Ctrl+Y), so you can’t break anything.\n\n\n\n\n\n\n\nNote\n\n\n\nSave your work — Export your TextGrid and TSV files regularly (section 6) to avoid losing progress.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/index.html#need-help",
    "href": "tutorial/index.html#need-help",
    "title": "Complete Tutorial",
    "section": "Need Help?",
    "text": "Need Help?\n\nStuck on a step? Check the screenshots in each section\nWant more detail? See the Features Documentation\nLooking for shortcuts? See the Keyboard Shortcuts Reference\n\n\nReady to start? Let’s begin with Loading Audio →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "Complete Tutorial"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html",
    "href": "tutorial/05-data-collection.html",
    "title": "5. Data Collection",
    "section": "",
    "text": "Data points allow you to collect acoustic measurements at specific time and frequency locations. Each data point automatically captures:\n\nTime and frequency coordinates\nPitch (F0)\nFormants (F1-F4) and bandwidths (B1-B4)\nIntensity\nHNR, CoG, Spectral Tilt, A1-P0\nLabels from all annotation tiers at that time\n\nThis is ideal for building datasets for statistical analysis (vowel measurements, consonant measurements, etc.).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#overview",
    "href": "tutorial/05-data-collection.html#overview",
    "title": "5. Data Collection",
    "section": "",
    "text": "Data points allow you to collect acoustic measurements at specific time and frequency locations. Each data point automatically captures:\n\nTime and frequency coordinates\nPitch (F0)\nFormants (F1-F4) and bandwidths (B1-B4)\nIntensity\nHNR, CoG, Spectral Tilt, A1-P0\nLabels from all annotation tiers at that time\n\nThis is ideal for building datasets for statistical analysis (vowel measurements, consonant measurements, etc.).",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#why-use-data-points",
    "href": "tutorial/05-data-collection.html#why-use-data-points",
    "title": "5. Data Collection",
    "section": "Why Use Data Points?",
    "text": "Why Use Data Points?\nManual approach (slow):\n\nPlace cursor on vowel\nRead F1 from values panel\nType F1 into spreadsheet\nRead F2 from values panel\nType F2 into spreadsheet\nRepeat for 100+ vowels… 😰\n\nData points approach (fast):\n\nDouble-click on each vowel\nExport all measurements to TSV\nOpen in R, Python, Excel — ready for analysis! 🎉",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#adding-data-points",
    "href": "tutorial/05-data-collection.html#adding-data-points",
    "title": "5. Data Collection",
    "section": "Adding Data Points",
    "text": "Adding Data Points\n\n\nEnable overlays you want to measure (Pitch, Formants, Intensity)\nDouble-click on the spectrogram at the point you want to measure\n\n\n\nAdding a data point\n\n\nA yellow vertical dashed line appears with a marker\nThe point is added to the data points list\nRepeat for each measurement location\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor vowel formant measurements, double-click in the middle of a vowel’s steady state (where F1 and F2 are stable).\n\n\nKeyboard shortcuts:\n\nDouble-click on spectrogram — Add data point\nCtrl+Z — Undo adding point",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#viewing-data-point-values",
    "href": "tutorial/05-data-collection.html#viewing-data-point-values",
    "title": "5. Data Collection",
    "section": "Viewing Data Point Values",
    "text": "Viewing Data Point Values\nEach data point captures all acoustic measurements:\n\n\nClick on a data point line to select it\nThe values panel updates to show measurements at that point:\n\nTime: 0.452 s\nFreq: 1234 Hz (where you clicked)\nPitch: 234 Hz\nF1: 523 Hz\nF2: 1987 Hz\nF3: 2743 Hz\nF4: 3543 Hz\nIntensity: 68 dB\nHNR: 12.3 dB\nCoG: 3456 Hz\nLabels: phones=“a”, words=“cat”\n\n\n\n\nData point values panel\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe “Freq” value is where you clicked, not a measured acoustic property. It’s useful for reference but typically not used in analysis.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#moving-data-points",
    "href": "tutorial/05-data-collection.html#moving-data-points",
    "title": "5. Data Collection",
    "section": "Moving Data Points",
    "text": "Moving Data Points\nAdjust a data point’s position if you placed it slightly off:\n\n\nClick and drag a data point line left or right (time) or up/down (frequency)\nRelease to finalize the position\nValues update automatically based on the new position\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse playback (Space) to verify you’re measuring the correct location. The best measurement location is usually the acoustic midpoint of a vowel or steady state.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#removing-data-points",
    "href": "tutorial/05-data-collection.html#removing-data-points",
    "title": "5. Data Collection",
    "section": "Removing Data Points",
    "text": "Removing Data Points\n\n\nRight-click on a data point line\nSelect “Remove data point” from the context menu\nThe point is deleted\n\n\nKeyboard shortcut:\n\nClick data point + Delete key — Remove selected point (if implemented)\nCtrl+Z — Undo removal",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#example-workflow-measuring-vowels",
    "href": "tutorial/05-data-collection.html#example-workflow-measuring-vowels",
    "title": "5. Data Collection",
    "section": "Example Workflow: Measuring Vowels",
    "text": "Example Workflow: Measuring Vowels\nLet’s collect formant data for all vowels in a sentence:\n\n\nLoad audio with a sentence (e.g., “see two cats”)\nEnable Formants overlay (checkbox)\nOptionally annotate vowels with a “phones” tier (section 4)\nZoom in on the first vowel (“ee” in “see”)\nFind the vowel midpoint (where formants are stable)\nDouble-click at the midpoint\nA data point appears\nRepeat for each vowel:\n\n“ee” in “see” (high F2, low F1)\n“oo” in “two” (low F2, low F1)\n“a” in “cats” (mid F1, mid F2)\n\nExport to TSV (section 6) for analysis\n\n\n\n\n\nMultiple data points on vowels",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#data-points-and-annotations",
    "href": "tutorial/05-data-collection.html#data-points-and-annotations",
    "title": "5. Data Collection",
    "section": "Data Points and Annotations",
    "text": "Data Points and Annotations\nData points automatically capture labels from ALL annotation tiers at that time:\nExample:\nIf you have tiers:\n\nphones tier: interval “a” from 0.4-0.6s\nwords tier: interval “cat” from 0.3-0.7s\nstress tier: interval “1” from 0.3-0.7s\n\nAnd you add a data point at time 0.5s:\nThe exported TSV will include:\ntime    freq    pitch   f1      f2      phones  words   stress\n0.500   1500    234     698     1234    a       cat     1\nThis is extremely powerful for statistical analysis in R or Python!\n\n\n\n\n\n\nTip\n\n\n\nAnnotate your audio first (section 4), then add data points. This way, each data point inherits contextual labels automatically.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#use-cases",
    "href": "tutorial/05-data-collection.html#use-cases",
    "title": "5. Data Collection",
    "section": "Use Cases",
    "text": "Use Cases\nVowel formant analysis:\n\nAdd data points at the midpoint of each vowel\nExport TSV with F1, F2, F3, vowel label\nPlot vowel space in R (F1 vs F2)\n\nVOT (Voice Onset Time) measurement:\n\nAdd data points at burst and voicing onset for stops\nExport TSV with times\nCalculate VOT = voicing_time - burst_time\n\nPitch contour studies:\n\nAdd data points at regular intervals (every 50ms)\nExport pitch values\nAnalyze intonation patterns\n\nConsonant acoustics:\n\nAdd data points at fricative midpoints\nExport CoG, spectral tilt\nCompare sibilants [s] vs [ʃ]",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#practice-exercises",
    "href": "tutorial/05-data-collection.html#practice-exercises",
    "title": "5. Data Collection",
    "section": "Practice Exercises",
    "text": "Practice Exercises\n\nCollect formants for 3 different vowels\n\nAdd one data point per vowel\nCheck the values panel to see F1 and F2\nNote which vowel has highest F2\n\nAnnotate + measure\n\nCreate a “vowels” tier\nAnnotate 3 vowels (e.g., “a”, “i”, “u”)\nAdd data points in the middle of each\nVerify that labels appear in data point info\n\nPractice moving points\n\nAdd a data point slightly off target\nDrag it to the correct position\nObserve how values change\n\nPractice undo/redo\n\nAdd 3 data points\nUndo them all (Ctrl+Z × 3)\nRedo them (Ctrl+Y × 3)\n\n\n\nChallenge: Record yourself saying 5 vowels: [i], [ɛ], [a], [ɔ], [u]. Add data points at the midpoint of each. Export to TSV (next section) and plot F1 vs F2 in Excel or R.",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#troubleshooting",
    "href": "tutorial/05-data-collection.html#troubleshooting",
    "title": "5. Data Collection",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nDouble-click adds boundary instead of data point:\n\nEnsure you’re clicking on the spectrogram, not on an annotation tier\nAnnotation tiers are below the spectrogram; click in the colored frequency display\n\nData point appears but values show “—”:\n\nEnable the relevant overlays (Pitch, Formants, etc.)\nEnsure WASM backend is loaded (check backend selector)\nWait for analysis to compute (1-2 seconds after zoom)\n\nCan’t move data point:\n\nEnsure you’re clicking and dragging the line, not near it\nTry clicking on the marker circle at the top of the line\n\nData point values seem wrong:\n\nCheck you clicked at the correct time and frequency\nVerify the acoustic overlays show reasonable values\nFor formants, ensure you’re clicking in a vowel, not a consonant\n\nToo many data points, display is cluttered:\n\nRemove unnecessary points (right-click → Remove)\nZoom out to see overall distribution\nExport and start fresh if needed",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "tutorial/05-data-collection.html#whats-next",
    "href": "tutorial/05-data-collection.html#whats-next",
    "title": "5. Data Collection",
    "section": "What’s Next?",
    "text": "What’s Next?\nNow that you’ve collected data points and created annotations, let’s learn how to export everything for further analysis.\nNext: 6. Exporting →\n\nNavigation: ← Previous: Annotations | Tutorial Overview | Next: Exporting →",
    "crumbs": [
      "Tutorial",
      "Complete Workflow",
      "5. Data Collection"
    ]
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html",
    "href": "GITHUB_PAGES_SETUP.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#configuration",
    "href": "GITHUB_PAGES_SETUP.html#configuration",
    "title": "",
    "section": "Configuration",
    "text": "Configuration\nDocumentation source files (.qmd) are in docs-src/, and Quarto renders HTML output to docs/:\n# docs-src/_quarto.yml\nproject:\n  type: website\n  output-dir: ../docs  # Output to docs/ directory\nThis keeps source separate from generated files, and allows GitHub Pages to serve from /docs."
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#github-pages-settings",
    "href": "GITHUB_PAGES_SETUP.html#github-pages-settings",
    "title": "",
    "section": "GitHub Pages Settings",
    "text": "GitHub Pages Settings\nAfter pushing to GitHub, configure GitHub Pages:\n\nGo to repository Settings → Pages\nUnder Source, select: Deploy from a branch\nUnder Branch, select:\n\nBranch: master (or main)\nFolder: /docs\n\nClick Save\n\nGitHub will automatically deploy from the /docs folder whenever you push HTML changes."
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#workflow",
    "href": "GITHUB_PAGES_SETUP.html#workflow",
    "title": "",
    "section": "Workflow",
    "text": "Workflow\n\nAutomatic Deployment (via GitHub Actions)\nThe .github/workflows/deploy-docs.yml workflow automatically:\n\nBuilds the Ozen-web app\nCopies build/ → docs/live/ (for embedded examples)\nCaptures screenshots with Playwright\nRenders Quarto documentation to docs/\nCommits generated HTML files back to the repository\nGitHub Pages automatically deploys the updated /docs folder\n\nTriggers: - Push to master/main with changes to documentation or app source - Manual trigger via GitHub Actions UI\n\n\nManual Deployment (local)\nYou can also render and commit locally:\n# Render documentation from source\ncd docs-src\nquarto render\n\n# Commit generated HTML\ngit add ../docs/*.html ../docs/**/*.html ../docs/site_libs/\ngit commit -m \"Update documentation\"\ngit push\nGitHub Pages will pick up the changes automatically."
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#file-structure",
    "href": "GITHUB_PAGES_SETUP.html#file-structure",
    "title": "",
    "section": "File Structure",
    "text": "File Structure\nDocumentation source and output are separated:\nozen-web/\n├── docs-src/                    # Source files (committed)\n│   ├── _quarto.yml              # Quarto configuration\n│   ├── index.qmd                # Landing page source\n│   ├── getting-started.qmd\n│   ├── tutorial/*.qmd\n│   ├── features/*.qmd\n│   ├── embedding/*.qmd\n│   ├── reference/*.qmd\n│   ├── development/*.qmd\n│   ├── assets/                  # CSS, images (source)\n│   └── screenshots/             # Screenshot placeholders\n│\n└── docs/                        # Generated HTML (committed, served by GitHub Pages)\n    ├── index.html\n    ├── getting-started.html\n    ├── tutorial/*.html\n    ├── features/*.html\n    ├── embedding/*.html\n    ├── reference/*.html\n    ├── development/*.html\n    ├── site_libs/               # Bootstrap, jQuery, etc.\n    ├── assets/                  # CSS, images (copied from source)\n    ├── screenshots/             # Generated screenshots\n    └── live/                    # Built Ozen-web app (for embedding examples)\nThis keeps source (.qmd) separate from generated (.html) files."
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#why-this-approach",
    "href": "GITHUB_PAGES_SETUP.html#why-this-approach",
    "title": "",
    "section": "Why This Approach?",
    "text": "Why This Approach?\nPrevious approach (GitHub Actions artifact deployment): - ✅ Cleaner repository (no committed HTML) - ❌ More complex setup - ❌ Requires GitHub Actions permissions\nCurrent approach (serve from /docs): - ✅ Simpler setup - ✅ Works with standard GitHub Pages options - ✅ Easier to debug (HTML files visible in repo) - ❌ Commits HTML files to repository\nFor a documentation site, committing HTML is acceptable and simplifies deployment."
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#verification",
    "href": "GITHUB_PAGES_SETUP.html#verification",
    "title": "",
    "section": "Verification",
    "text": "Verification\nAfter pushing, verify deployment:\n\nCheck GitHub Actions run completed successfully\nVisit: https://ucpresearch.github.io/ozen-web/\nVerify all pages load correctly\nCheck embedded viewer examples work"
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#troubleshooting",
    "href": "GITHUB_PAGES_SETUP.html#troubleshooting",
    "title": "",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n404 errors on GitHub Pages: - Verify /docs folder is committed - Check GitHub Pages settings point to /docs - Ensure index.html exists in docs/\nMissing CSS/JS: - Ensure site_libs/ directory is committed - Check that .gitignore doesn’t exclude site_libs/\nEmbedded viewer not loading: - Verify docs/live/ directory exists and is committed - Check that the build was copied: npm run build:docs"
  },
  {
    "objectID": "GITHUB_PAGES_SETUP.html#see-also",
    "href": "GITHUB_PAGES_SETUP.html#see-also",
    "title": "",
    "section": "See Also",
    "text": "See Also\n\nBUILD_SYNC.md - Syncing build to docs/live\n.github/workflows/deploy-docs.yml - Deployment workflow\n_quarto.yml - Quarto configuration"
  },
  {
    "objectID": "development/architecture.html",
    "href": "development/architecture.html",
    "title": "Architecture",
    "section": "",
    "text": "Ozen-web is a fully client-side web application built with SvelteKit, designed to run entirely in the browser without any backend server. The architecture emphasizes:\n\nReactive state management via Svelte stores\nWASM-powered analysis for Praat-accurate acoustic computation\nCanvas-based rendering for high-performance visualization\nProgressive enhancement for long audio files\nModular design with clear separation of concerns",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#overview",
    "href": "development/architecture.html#overview",
    "title": "Architecture",
    "section": "",
    "text": "Ozen-web is a fully client-side web application built with SvelteKit, designed to run entirely in the browser without any backend server. The architecture emphasizes:\n\nReactive state management via Svelte stores\nWASM-powered analysis for Praat-accurate acoustic computation\nCanvas-based rendering for high-performance visualization\nProgressive enhancement for long audio files\nModular design with clear separation of concerns",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#application-structure",
    "href": "development/architecture.html#application-structure",
    "title": "Architecture",
    "section": "Application Structure",
    "text": "Application Structure\n\nRoutes\nOzen-web uses SvelteKit’s file-based routing with two main routes:\nsrc/routes/\n├── +page.svelte          # Main desktop application\n├── +layout.svelte        # App shell (shared layout)\n├── +layout.ts            # Prerender config\n└── viewer/\n    ├── +page.svelte      # Mobile-optimized viewer\n    └── +layout.ts        # Viewer prerender config\nMain Application (/): - Full-featured desktop interface - Editable annotations and data points - File drop zone, settings panels, toolbar - Keyboard shortcuts for efficient workflow\nMobile Viewer (/viewer): - Touch-optimized, read-only interface - URL-based audio loading (?audio=...) - Compact values display - Gesture support (tap, drag, pinch, pan)\n\n\nBuild System\nStatic Site Generation: - All routes are prerendered at build time (not SPA mode) - Uses relative paths for portable deployment - Post-build script (scripts/fix-relative-paths.js) ensures base path detection works in subdirectories - No server required - deploy to any static host\nBuild output:\nnpm run build    # → build/ directory\nDeployment flexibility:\n# Works at root\nhttps://example.com/\n\n# Works in subdirectory\nhttps://example.com/subfolder/\n\n# Works on GitHub Pages\nhttps://username.github.io/ozen-web/",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#component-hierarchy",
    "href": "development/architecture.html#component-hierarchy",
    "title": "Architecture",
    "section": "Component Hierarchy",
    "text": "Component Hierarchy\ngraph TD\n    A[+layout.svelte] --&gt; B[+page.svelte Main App]\n    A --&gt; C[viewer/+page.svelte Mobile Viewer]\n\n    B --&gt; D[FileDropZone]\n    B --&gt; E[Waveform]\n    B --&gt; F[Spectrogram]\n    B --&gt; G[AnnotationEditor]\n    B --&gt; H[ValuesPanel]\n    B --&gt; I[TimeAxis]\n\n    G --&gt; J[Tier x N]\n\n    F -.overlay.-&gt; K[Pitch overlay]\n    F -.overlay.-&gt; L[Formant overlay]\n    F -.overlay.-&gt; M[Data points]\n\n    C --&gt; N[Compact ValuesPanel]\n    C --&gt; O[Spectrogram read-only]\n    C --&gt; P[Touch gesture layer]\n\nComponent Responsibilities\nLayout Components:\n\n\n\n\n\n\n\n\nComponent\nFile\nPurpose\n\n\n\n\nApp shell\n+layout.svelte\nHTML structure, global styles, WASM initialization\n\n\nMain app\n+page.svelte\nDesktop UI orchestration, toolbar, panels\n\n\nMobile viewer\nviewer/+page.svelte\nTouch-optimized view-only interface\n\n\n\nCore Visualization:\n\n\n\n\n\n\n\n\nComponent\nFile\nResponsibility\n\n\n\n\nWaveform\nWaveform.svelte\nAmplitude display, downsampling, synchronized cursor\n\n\nSpectrogram\nSpectrogram.svelte\nTime-frequency visualization, overlay rendering, interaction\n\n\nTimeAxis\nTimeAxis.svelte\nTime ruler with tick marks and labels\n\n\n\nAnnotation & Data:\n\n\n\n\n\n\n\n\nComponent\nFile\nResponsibility\n\n\n\n\nAnnotationEditor\nAnnotationEditor.svelte\nTier container, add/remove tiers, toolbar\n\n\nTier\nTier.svelte\nIndividual tier display, boundary editing, text input\n\n\nValuesPanel\nValuesPanel.svelte\nReal-time acoustic measurements at cursor\n\n\n\nUtilities:\n\n\n\n\n\n\n\n\nComponent\nFile\nResponsibility\n\n\n\n\nFileDropZone\nFileDropZone.svelte\nDrag-drop and file picker for audio/TextGrid\n\n\nModal\nModal.svelte\nReusable modal dialog container",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#state-management",
    "href": "development/architecture.html#state-management",
    "title": "Architecture",
    "section": "State Management",
    "text": "State Management\nOzen-web uses Svelte stores for all shared state. Stores are the single source of truth; components subscribe and react to changes.\n\nStore Architecture\ngraph LR\n    A[User Action] --&gt; B[Component]\n    B --&gt; C[Store Update]\n    C --&gt; D[Store]\n    D --&gt; E[Reactive $binding]\n    E --&gt; F[Component Re-render]\n\n    D -.derives.-&gt; G[Derived Store]\n    G --&gt; E\n\n\nCore Stores\nsrc/lib/stores/\n\n\n\n\n\n\n\n\nStore\nFile\nState\n\n\n\n\nAudio\naudio.ts\naudioBuffer, sampleRate, fileName, duration\n\n\nView\nview.ts\ntimeRange, cursorPosition, selection, hoverPosition\n\n\nAnalysis\nanalysis.ts\nanalysisResults, isAnalyzing, analysisParams\n\n\nAnnotations\nannotations.ts\ntiers, activeTier, annotation functions\n\n\nData Points\ndataPoints.ts\ndataPoints array with measurements\n\n\nUndo/Redo\nundoManager.ts\nUnified history stack for all edits\n\n\nConfig\nconfig.ts\nColors, formant presets, UI preferences\n\n\n\n\n\nData Flow Example\nLoading audio file:\nsequenceDiagram\n    participant U as User\n    participant D as FileDropZone\n    participant A as audio.ts\n    participant V as view.ts\n    participant An as analysis.ts\n    participant W as Waveform\n    participant S as Spectrogram\n\n    U-&gt;&gt;D: Drop audio file\n    D-&gt;&gt;D: Decode with Web Audio API\n    D-&gt;&gt;A: Set audioBuffer, sampleRate, fileName\n    A-&gt;&gt;V: Reset timeRange to [0, duration]\n    A-&gt;&gt;An: Trigger runAnalysis()\n    An-&gt;&gt;An: Compute pitch, formants, etc.\n    An-&gt;&gt;An: Set analysisResults\n    A--&gt;&gt;W: Reactive update (audioBuffer changed)\n    W-&gt;&gt;W: Redraw waveform\n    An--&gt;&gt;S: Reactive update (analysisResults changed)\n    S-&gt;&gt;S: Render spectrogram + overlays\nEditing annotation:\nsequenceDiagram\n    participant U as User\n    participant T as Tier\n    participant UM as undoManager.ts\n    participant An as annotations.ts\n    participant T2 as Tier (all)\n\n    U-&gt;&gt;T: Double-click to add boundary\n    T-&gt;&gt;UM: saveUndo()\n    UM-&gt;&gt;UM: Capture current state snapshot\n    T-&gt;&gt;An: addBoundary(tierIndex, time)\n    An-&gt;&gt;An: Update tiers array\n    An--&gt;&gt;T2: Reactive update\n    T2-&gt;&gt;T2: Re-render all tiers\n\n\nDerived Stores\nComputed state is implemented with derived stores:\n// src/lib/stores/view.ts\nexport const visibleDuration = derived(\n  timeRange,\n  ($timeRange) =&gt; $timeRange.end - $timeRange.start\n);\nComponents can subscribe to $visibleDuration which automatically updates when timeRange changes.",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#wasm-integration",
    "href": "development/architecture.html#wasm-integration",
    "title": "Architecture",
    "section": "WASM Integration",
    "text": "WASM Integration\n\nBackend Abstraction Layer\nFile: src/lib/wasm/acoustic.ts\nThe abstraction layer provides a unified API across multiple WASM backends:\n\n\n\nBackend\nSource\nLicense\n\n\n\n\npraatfan-local\nstatic/wasm/praatfan/\nMIT/Apache-2.0\n\n\npraatfan\nCDN (GitHub Pages)\nMIT/Apache-2.0\n\n\npraatfan-gpl\nCDN (GitHub Pages)\nGPL\n\n\n\nWhy abstraction matters: - Different backends may have slightly different APIs - Wrapper functions normalize the interface - Easy to add new backends without touching components\n\n\nWASM Call Flow\nsequenceDiagram\n    participant C as Component/Store\n    participant A as acoustic.ts (abstraction)\n    participant W as WASM Module\n    participant M as Memory\n\n    C-&gt;&gt;A: computePitch(sound, params)\n    A-&gt;&gt;W: sound.to_pitch(...)\n    W-&gt;&gt;M: Allocate arrays\n    W-&gt;&gt;M: Run autocorrelation\n    W-&gt;&gt;M: Store results\n    W--&gt;&gt;A: Return Pitch object\n    A-&gt;&gt;W: pitch.ts() → Float64Array\n    A-&gt;&gt;W: pitch.selected_array() → Float64Array\n    W--&gt;&gt;A: Return arrays\n    A-&gt;&gt;W: pitch.free()\n    W-&gt;&gt;M: Deallocate\n    A--&gt;&gt;C: Return { times, values }\nCritical pattern: Always .free() WASM objects to prevent memory leaks.\n\n\nInitialization\nOn app load (+layout.svelte):\n\nCheck for config.yaml or URL parameter for backend choice\nCall initWasm(backend) from acoustic.ts\nSet wasmReady store to true\nComponents react to $wasmReady and enable analysis features\n\nLazy loading for CDN backends:\n// praatfan backend downloads ~5MB on first init\nawait initWasm('praatfan');  // 1-3 second delay\nInstant for local backend:\n// praatfan-local uses bundled WASM\nawait initWasm('praatfan-local');  // ~200ms",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#rendering-pipeline",
    "href": "development/architecture.html#rendering-pipeline",
    "title": "Architecture",
    "section": "Rendering Pipeline",
    "text": "Rendering Pipeline\n\nCanvas Strategy\nOzen-web uses HTML5 Canvas for all visualizations (not SVG/DOM) to handle: - Large datasets (thousands of time points, frequency bins) - Real-time updates (cursor tracking, playback) - Smooth zoom/pan interactions\n\n\nSpectrogram Rendering\nMulti-stage pipeline:\ngraph TD\n    A[Load audio] --&gt; B[Compute full spectrogram via WASM]\n    B --&gt; C[Apply grayscale colormap]\n    C --&gt; D[Create ImageData]\n    D --&gt; E[Draw to off-screen canvas cache]\n    E --&gt; F[On zoom/pan: draw visible region]\n\n    F --&gt; G{Zoom &gt; 2x?}\n    G --&gt;|Yes| H[Debounce 300ms]\n    G --&gt;|No| I[Draw from cache]\n\n    H --&gt; J[Recompute high-res spectrogram for visible window]\n    J --&gt; K[Update cache for this region]\n    K --&gt; I\n\n    I --&gt; L[Draw overlays on top]\n    L --&gt; M[Pitch track]\n    L --&gt; N[Formants]\n    L --&gt; O[Data points]\n    L --&gt; P[Cursor/selection]\nWhy this approach: - Full spectrogram cache: Fast redraw when panning at low zoom - Dynamic resolution: High-quality detail when zoomed in - Debouncing: Prevents excessive recomputation during smooth zoom - Overlay separation: Overlays drawn each frame without recomputing spectrogram\nCode location: src/lib/components/Spectrogram.svelte\n\n\nWaveform Rendering\nDownsampling strategy:\n// Pseudocode from Waveform.svelte\nfunction renderWaveform(audioBuffer, timeRange, canvasWidth) {\n  const samplesPerPixel = Math.ceil(visibleSamples / canvasWidth);\n\n  for (let x = 0; x &lt; canvasWidth; x++) {\n    const startSample = visibleStart + (x * samplesPerPixel);\n    const endSample = startSample + samplesPerPixel;\n\n    // Find min/max in this pixel column\n    let min = Infinity, max = -Infinity;\n    for (let i = startSample; i &lt; endSample; i++) {\n      const sample = audioBuffer[i];\n      if (sample &lt; min) min = sample;\n      if (sample &gt; max) max = sample;\n    }\n\n    // Draw vertical line from min to max\n    drawVerticalLine(x, min, max);\n  }\n}\nResult: One vertical line per pixel showing amplitude range, efficient for any zoom level.\n\n\nOverlay Rendering\nLayers drawn on spectrogram canvas:\n\nBase spectrogram (ImageData from cache)\nSelection (semi-transparent blue rectangle)\nPitch track (blue line with dots)\nFormant tracks (red dots for F1-F4)\nIntensity (green line)\nHNR/CoG/Spectral Tilt (additional colored tracks)\nData points (yellow dashed vertical lines + circles)\nCursor (red vertical line, drawn last)\n\nDrawing order matters: Cursor must be on top to remain visible during playback.",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#touch-handling",
    "href": "development/architecture.html#touch-handling",
    "title": "Architecture",
    "section": "Touch Handling",
    "text": "Touch Handling\nFile: src/lib/touch/gestures.ts\nThe mobile viewer (/viewer route) uses custom touch gesture recognition:\n\nGesture Types\n\n\n\nGesture\nFingers\nAction\nEffect\n\n\n\n\nTap\n1\nQuick touch\nSet cursor position\n\n\nDrag\n1\nTouch + move\nCreate selection\n\n\nPan\n2\nTwo-finger drag\nScroll time axis\n\n\nPinch\n2\nSpread/pinch\nZoom in/out\n\n\n\n\n\nTouch Event Flow\nsequenceDiagram\n    participant U as User\n    participant G as gestures.ts\n    participant V as view.ts\n    participant S as Spectrogram\n\n    U-&gt;&gt;G: touchstart (2 fingers)\n    G-&gt;&gt;G: Detect pinch gesture\n    U-&gt;&gt;G: touchmove\n    G-&gt;&gt;G: Calculate pinch distance\n    G-&gt;&gt;G: Compute zoom delta\n    G-&gt;&gt;V: Update timeRange (zoom)\n    V--&gt;&gt;S: Reactive update\n    S-&gt;&gt;S: Redraw at new zoom level\n    U-&gt;&gt;G: touchend\n    G-&gt;&gt;G: Reset gesture state\nConflict resolution: - 1 finger: Wait 150ms to distinguish tap vs drag - 2 fingers: Immediately enter pan/pinch mode - &gt;2 fingers: Ignore (prevent accidental gestures)\nCode location: viewer/+page.svelte integrates gestures.ts",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#file-io",
    "href": "development/architecture.html#file-io",
    "title": "Architecture",
    "section": "File I/O",
    "text": "File I/O\n\nLoading Files\nAudio files:\ngraph LR\n    A[File source] --&gt; B{Input method}\n    B --&gt;|Drag & drop| C[FileDropZone]\n    B --&gt;|File picker| C\n    B --&gt;|Microphone| D[MediaRecorder API]\n    B --&gt;|URL parameter| E[Fetch with CORS]\n    B --&gt;|Data URL| F[Base64 decode]\n\n    C --&gt; G[Decode with Web Audio API]\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H[Float64Array samples]\n    H --&gt; I[audio.ts store]\nTextGrid files:\ngraph LR\n    A[File picker] --&gt; B[FileReader.readAsText]\n    B --&gt; C[textgrid/parser.ts]\n    C --&gt; D{Format?}\n    D --&gt;|Short| E[Parse short format]\n    D --&gt;|Long| F[Parse long format]\n    E --&gt; G[Tier objects]\n    F --&gt; G\n    G --&gt; H[annotations.ts store]\n\n\nSaving Files\nTwo approaches based on browser support:\nModern browsers (File System Access API):\nconst handle = await window.showSaveFilePicker({\n  suggestedName: 'annotations.TextGrid',\n  types: [{ description: 'TextGrid', accept: { 'text/plain': ['.TextGrid'] } }]\n});\nconst writable = await handle.createWritable();\nawait writable.write(textGridContent);\nawait writable.close();\nFallback (Download link):\nconst blob = new Blob([textGridContent], { type: 'text/plain' });\nconst url = URL.createObjectURL(blob);\nconst a = document.createElement('a');\na.href = url;\na.download = 'annotations.TextGrid';\na.click();\nURL.revokeObjectURL(url);\nFile types supported:\n\n\n\nFormat\nLoad\nSave\nStore\nParser\n\n\n\n\nWAV audio\n✅\n✅\naudio.ts\nWeb Audio API\n\n\nMP3 audio\n✅\n❌\naudio.ts\nWeb Audio API\n\n\nOGG audio\n✅\n❌\naudio.ts\nWeb Audio API\n\n\nTextGrid\n✅\n✅\nannotations.ts\ntextgrid/parser.ts\n\n\nTSV data\n✅\n✅\ndataPoints.ts\nBuilt-in",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#long-audio-handling",
    "href": "development/architecture.html#long-audio-handling",
    "title": "Architecture",
    "section": "Long Audio Handling",
    "text": "Long Audio Handling\nProblem: Analyzing &gt;60 second files upfront causes UI freezing.\nSolution: Progressive, on-demand analysis.\n\nAnalysis Strategy\ngraph TD\n    A[Load audio file] --&gt; B{Duration &gt; 60s?}\n    B --&gt;|No| C[Run full analysis immediately]\n    B --&gt;|Yes| D[Skip analysis, show waveform only]\n\n    C --&gt; E[Display all features]\n\n    D --&gt; F[User zooms in]\n    F --&gt; G{Visible window ≤ 60s?}\n    G --&gt;|No| H[Wait for more zoom]\n    G --&gt;|Yes| I[Debounce 300ms]\n    H --&gt; F\n    I --&gt; J[Compute analysis for visible range only]\n    J --&gt; K[Cache results for this region]\n    K --&gt; E\nImplementation:\n// src/lib/stores/analysis.ts\nexport const MAX_ANALYSIS_DURATION = 60;\n\nexport async function runAnalysis(): Promise&lt;void&gt; {\n  const audioDuration = get(duration);\n\n  if (audioDuration &gt; MAX_ANALYSIS_DURATION) {\n    console.log('Audio too long, skipping upfront analysis');\n    analysisResults.set(null);\n    return;\n  }\n\n  // Run full analysis\n  await computeAllFeatures();\n}\n\n// Called when user zooms (in Spectrogram.svelte)\nexport async function runAnalysisForRange(start: number, end: number): Promise&lt;void&gt; {\n  if (end - start &gt; MAX_ANALYSIS_DURATION) {\n    return; // Still too wide\n  }\n\n  // Compute just for this window\n  await computeFeaturesForRange(start, end);\n}\nUser experience: - File loads instantly (no hanging) - Waveform always visible - Spectrogram shows “Zoom in to see spectrogram” message - Overlays appear once zoomed to analyzable window\nCode location: src/lib/stores/analysis.ts, src/lib/components/Spectrogram.svelte",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#unified-undo-system",
    "href": "development/architecture.html#unified-undo-system",
    "title": "Architecture",
    "section": "Unified Undo System",
    "text": "Unified Undo System\nFile: src/lib/stores/undoManager.ts\n\nArchitecture\nState-snapshot approach: - Before each change, capture full state (all tiers + all data points) - Use JSON deep-copy for isolation - Single history stack ensures chronological order\ngraph LR\n    A[User edits annotation] --&gt; B[saveUndo]\n    B --&gt; C[Capture current state]\n    C --&gt; D[Push to history stack]\n    D --&gt; E[Perform mutation]\n\n    F[User clicks Undo] --&gt; G[Pop from history]\n    G --&gt; H[Restore previous state]\n    H --&gt; I[Update stores]\n\n\nUndoable Operations\nAnnotations: - Add boundary - Remove boundary - Move boundary - Edit interval text\nData Points: - Add data point - Remove data point - Move data point\nNot undoable (by design): - Add/remove/rename tier (structural changes) - Load audio/TextGrid (file operations)\n\n\nUsage Pattern\nimport { saveUndo, undo, redo } from '$lib/stores/undoManager';\n\nexport function addBoundary(tierIndex: number, time: number): void {\n  saveUndo();  // MUST call before mutation\n\n  tiers.update(t =&gt; {\n    // Modify tiers...\n    return t;\n  });\n}\n\n// In component\nfunction handleUndo(event: KeyboardEvent) {\n  if ((event.ctrlKey || event.metaKey) && event.key === 'z') {\n    undo();\n  }\n}\nImportant: Always call saveUndo() before the mutation, not after.",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#audio-playback",
    "href": "development/architecture.html#audio-playback",
    "title": "Architecture",
    "section": "Audio Playback",
    "text": "Audio Playback\nFile: src/lib/audio/player.ts\nUses Web Audio API for precise playback:\ngraph LR\n    A[audioBuffer store] --&gt; B[AudioBufferSourceNode]\n    B --&gt; C[GainNode volume control]\n    C --&gt; D[AudioContext destination]\n    D --&gt; E[System audio output]\n\n    F[cursorPosition store] --&gt; G[requestAnimationFrame loop]\n    G --&gt; H[Update cursor during playback]\n    H --&gt; G\n\nPlayback Modes\nSelection playback:\n// Play only selected region\nplaySelection(selection.start, selection.end);\nVisible window playback:\n// Play what's currently visible on screen\nplayVisibleWindow(timeRange.start, timeRange.end);\nFull file playback:\n// Play from cursor to end\nplayFromCursor(cursorPosition, audioDuration);\n\n\nCursor Synchronization\nDuring playback: 1. AudioBufferSourceNode starts at offset 2. requestAnimationFrame loop checks AudioContext.currentTime 3. Calculate playback position: offset + (currentTime - startTime) 4. Update cursorPosition store (60 FPS) 5. All components with {$cursorPosition} binding re-render\nResult: Smooth cursor tracking across waveform, spectrogram, and annotations.",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#deployment-architecture",
    "href": "development/architecture.html#deployment-architecture",
    "title": "Architecture",
    "section": "Deployment Architecture",
    "text": "Deployment Architecture\n\nStatic Build\nNo server required - entire app is static HTML/CSS/JS:\nbuild/\n├── index.html              # Main app HTML\n├── viewer.html             # Mobile viewer HTML\n├── _app/\n│   ├── immutable/\n│   │   ├── chunks/         # Code-split JS bundles\n│   │   ├── entry/          # Entry points\n│   │   └── nodes/          # Page components\n│   └── version.json        # Build version\n└── ...                     # Static assets\nBase path handling:\nPost-build script (scripts/fix-relative-paths.js) injects runtime base path detection:\n// In generated HTML\n&lt;script&gt;\n  // Detect actual deployment path\n  window.__base = document.currentScript.src.replace(/\\/[^\\/]*$/, '').replace(/\\/_app.*/, '');\n&lt;/script&gt;\nThis allows deploying to: - Root: https://example.com/ - Subdirectory: https://example.com/tools/ozen/ - GitHub Pages: https://user.github.io/ozen-web/\nNo rebuild needed - same build works everywhere.",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#performance-considerations",
    "href": "development/architecture.html#performance-considerations",
    "title": "Architecture",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\nMemory Management\nWASM objects: - Always .free() after use (Pitch, Formant, Spectrogram objects) - Use abstraction layer to enforce cleanup\nCanvas caching: - Off-screen canvas for spectrogram (reuse without recomputing) - Debounce zoom to avoid excessive regeneration\nAudio buffer: - Stored as Float64Array (full resolution, never downsampled) - Mono conversion done once on load (stereo → mono mix)\n\n\nRendering Optimization\nWaveform: - Downsample to one vertical line per pixel - O(canvas width) complexity, not O(sample count)\nSpectrogram: - Cache full spectrogram as ImageData - Draw visible region only (viewport clipping) - Regenerate high-res only when zoomed &gt;2x\nOverlays: - Skip rendering points outside visible time range - Use requestAnimationFrame for smooth cursor updates\n\n\nLazy Analysis\nFor audio &gt;60s: - Skip upfront computation (prevents hanging) - Compute only visible window when zoomed - Cache computed regions (avoid re-analysis on pan)",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#testing-strategy",
    "href": "development/architecture.html#testing-strategy",
    "title": "Architecture",
    "section": "Testing Strategy",
    "text": "Testing Strategy\nCurrently: Manual testing workflow (see Setup)\nFuture: Automated tests could include: - Unit tests for stores (Vitest) - WASM integration tests (mock acoustic.ts) - Component tests (Svelte Testing Library) - E2E tests (Playwright - already used for screenshots)",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/architecture.html#see-also",
    "href": "development/architecture.html#see-also",
    "title": "Architecture",
    "section": "See Also",
    "text": "See Also\n\nSetup - Development environment\nStores - Detailed store documentation\nWASM Integration - Backend development\nContributing - How to contribute",
    "crumbs": [
      "Development",
      "Architecture"
    ]
  },
  {
    "objectID": "development/wasm-integration.html",
    "href": "development/wasm-integration.html",
    "title": "WASM Integration",
    "section": "",
    "text": "Ozen-web uses WebAssembly (WASM) for acoustic analysis, powered by the praatfan library. WASM provides near-native performance for computationally intensive analysis while running entirely in the browser.\nKey file: src/lib/wasm/acoustic.ts - Abstraction layer for all WASM operations\nWhy WASM: - Performance: 10-100x faster than pure JavaScript - Praat-accurate: Implements Praat algorithms identically - Browser-native: No plugins or servers required - Portable: Same binary runs on all platforms",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#overview",
    "href": "development/wasm-integration.html#overview",
    "title": "WASM Integration",
    "section": "",
    "text": "Ozen-web uses WebAssembly (WASM) for acoustic analysis, powered by the praatfan library. WASM provides near-native performance for computationally intensive analysis while running entirely in the browser.\nKey file: src/lib/wasm/acoustic.ts - Abstraction layer for all WASM operations\nWhy WASM: - Performance: 10-100x faster than pure JavaScript - Praat-accurate: Implements Praat algorithms identically - Browser-native: No plugins or servers required - Portable: Same binary runs on all platforms",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#backend-options",
    "href": "development/wasm-integration.html#backend-options",
    "title": "WASM Integration",
    "section": "Backend Options",
    "text": "Backend Options\nOzen-web supports three WASM backends with different licenses and delivery methods:\n\n\n\n\n\n\n\n\n\n\nBackend\nSource\nLicense\nLoad Time\nUse Case\n\n\n\n\npraatfan-local\nstatic/wasm/praatfan/ (bundled)\nMIT/Apache-2.0\nInstant (~200ms)\nProduction deployments, offline use\n\n\npraatfan\nGitHub Pages CDN\nMIT/Apache-2.0\n1-3 seconds\nHosted viewer, minimal bundle size\n\n\npraatfan-gpl\nGitHub Pages CDN\nGPL\n1-3 seconds\nGPL-compatible projects, GPL features\n\n\n\n\nBackend Selection Priority\nRuntime detection order:\n\nURL parameter: ?backend=praatfan-local\nConfig file: backend: praatfan in static/config.yaml\nDefault: praatfan-local\n\n\n\nAPI Differences\nWhile all backends compute the same results, they have slight API differences:\npraatfan-gpl:\n// Direct accessor properties\npitch.start_time\npitch.time_step\npitch.num_frames\n\n// Methods with interpolation parameter\nintensity.get_value_at_time(time, 'cubic')\nformant.get_value_at_time(formantNum, time, 'hertz', 'linear')\npraatfan & praatfan-local:\n// Accessor methods (not properties)\npitch.start_time()\npitch.time_step()\npitch.num_frames()\n\n// Array-based access (manual interpolation)\nconst times = pitch.times();\nconst values = pitch.values();\n// Interpolate manually\nThe abstraction layer handles these differences transparently.",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#abstraction-layer-architecture",
    "href": "development/wasm-integration.html#abstraction-layer-architecture",
    "title": "WASM Integration",
    "section": "Abstraction Layer Architecture",
    "text": "Abstraction Layer Architecture\n\nPurpose\nThe abstraction layer (src/lib/wasm/acoustic.ts) provides a unified API across all backends, isolating components from WASM implementation details.\ngraph LR\n    A[Component/Store] --&gt; B[acoustic.ts abstraction]\n    B --&gt; C{Backend?}\n    C --&gt;|praatfan-gpl| D[praatfan-gpl.wasm]\n    C --&gt;|praatfan| E[praatfan.wasm]\n    C --&gt;|praatfan-local| F[praatfan.wasm local]\n\n    B -.normalize API.-&gt; D\n    B -.normalize API.-&gt; E\n    B -.normalize API.-&gt; F\n\n\nCore Functions\nInitialization:\nimport { initWasm, wasmReady, currentBackend } from '$lib/wasm/acoustic';\n\n// Initialize specific backend\nawait initWasm('praatfan-local');\n\n// Check readiness (store)\nif (get(wasmReady)) {\n  // WASM ready to use\n}\n\n// Get active backend\nconst backend = get(currentBackend);  // 'praatfan-local'\nCreating Sound objects:\nimport { createSound, getWasm } from '$lib/wasm/acoustic';\n\nconst wasm = getWasm();\nconst sound = new wasm.Sound(audioBuffer, sampleRate);\n\n// Or use helper\nconst sound = createSound(audioBuffer, sampleRate);\n\n// CRITICAL: Free when done\nsound.free();\nAnalysis computation:\nimport {\n  computePitch,\n  computeFormant,\n  computeIntensity,\n  computeHarmonicity,\n  computeSpectrogram\n} from '$lib/wasm/acoustic';\n\n// All functions handle backend differences internally\nconst pitch = computePitch(sound, timeStep, pitchFloor, pitchCeiling);\nconst formant = computeFormant(sound, timeStep, numFormants, maxFormant, windowLength, preEmphasis);\nconst intensity = computeIntensity(sound, minPitch, timeStep);\nconst harmonicity = computeHarmonicity(sound, timeStep, minPitch, silenceThreshold, periodsPerWindow);\nconst spectrogram = computeSpectrogram(sound, windowLength, maxFreq, timeStep, freqStep);\nExtracting values:\nimport {\n  getPitchTimes,\n  getPitchValues,\n  getIntensityAtTime,\n  getFormantAtTime,\n  getBandwidthAtTime,\n  getHarmonicityAtTime,\n  getSpectrogramInfo\n} from '$lib/wasm/acoustic';\n\n// Get pitch track\nconst times = getPitchTimes(pitch);    // Float64Array\nconst values = getPitchValues(pitch);  // Float64Array\n\n// Get value at specific time (with interpolation)\nconst f0 = getIntensityAtTime(intensity, 1.5);  // At 1.5 seconds\nconst f1 = getFormantAtTime(formant, 1, 1.5);   // F1 at 1.5 seconds\nconst b1 = getBandwidthAtTime(formant, 1, 1.5); // B1 at 1.5 seconds\nconst hnr = getHarmonicityAtTime(harmonicity, 1.5);\n\n// Get spectrogram metadata\nconst info = getSpectrogramInfo(spectrogram);\nconsole.log(info.nTimes, info.nFreqs);  // Dimensions\nconsole.log(info.timeMin, info.timeMax);  // Time range\nconsole.log(info.freqMin, info.freqMax);  // Frequency range\nconsole.log(info.values);  // Float64Array of power values",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#memory-management",
    "href": "development/wasm-integration.html#memory-management",
    "title": "WASM Integration",
    "section": "Memory Management",
    "text": "Memory Management\nCRITICAL: WASM objects must be manually freed to avoid memory leaks.\n\nThe Problem\nJavaScript has automatic garbage collection, but WASM memory is not tracked by JavaScript’s GC. Unreleased WASM objects accumulate until the browser tab runs out of memory.\n\n\nThe Solution\nAlways call .free() on WASM objects when done:\n// ✅ CORRECT: Free all objects\nconst sound = createSound(samples, sampleRate);\nconst pitch = computePitch(sound, 0.01, 75, 600);\n\n// Use pitch...\nconst times = getPitchTimes(pitch);\nconst values = getPitchValues(pitch);\n\n// Free when done\npitch.free();\nsound.free();\n// ❌ WRONG: Memory leak\nconst sound = createSound(samples, sampleRate);\nconst pitch = computePitch(sound, 0.01, 75, 600);\n\n// Use pitch...\nconst times = getPitchTimes(pitch);\nconst values = getPitchValues(pitch);\n\n// Objects never freed - memory leak!\n\n\nUsing try/finally\nBest practice: Use try/finally to ensure cleanup even on errors:\nconst sound = createSound(samples, sampleRate);\ntry {\n  const pitch = computePitch(sound, 0.01, 75, 600);\n  try {\n    // Use pitch...\n    const times = getPitchTimes(pitch);\n  } finally {\n    pitch.free();  // Always freed\n  }\n} finally {\n  sound.free();  // Always freed\n}\n\n\nAbstraction Layer Cleanup\nThe computeAcoustics() helper handles cleanup automatically:\nimport { computeAcoustics } from '$lib/wasm/acoustic';\n\n// All objects freed internally\nconst results = await computeAcoustics(samples, sampleRate, {\n  timeStep: 0.01,\n  pitchFloor: 75,\n  pitchCeiling: 600,\n  maxFormant: 5500\n});\n\n// Safe to use results (JS arrays, not WASM objects)\nconsole.log(results.pitch);\nconsole.log(results.formants.f1);\nImplementation:\nexport async function computeAcoustics(...) {\n  const sound = createSound(samples, sampleRate);\n\n  try {\n    const pitch = computePitch(...);\n    const intensity = computeIntensity(...);\n    // ... more analyses\n\n    // Extract to JS arrays\n    const results = {\n      times: Array.from(getPitchTimes(pitch)),\n      pitch: Array.from(getPitchValues(pitch)),\n      // ...\n    };\n\n    // Free WASM objects\n    pitch.free();\n    intensity.free();\n    // ... more frees\n\n    return results;  // Pure JS data\n  } finally {\n    sound.free();  // Always freed\n  }\n}",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#complete-usage-examples",
    "href": "development/wasm-integration.html#complete-usage-examples",
    "title": "WASM Integration",
    "section": "Complete Usage Examples",
    "text": "Complete Usage Examples\n\nExample 1: Basic Pitch Analysis\nimport { get } from 'svelte/store';\nimport { audioBuffer, sampleRate } from '$lib/stores/audio';\nimport {\n  createSound,\n  computePitch,\n  getPitchTimes,\n  getPitchValues\n} from '$lib/wasm/acoustic';\n\nasync function analyzePitch() {\n  const samples = get(audioBuffer);\n  const sr = get(sampleRate);\n\n  if (!samples) return;\n\n  const sound = createSound(samples, sr);\n  try {\n    const pitch = computePitch(sound, 0.01, 75, 600);\n    try {\n      const times = getPitchTimes(pitch);\n      const values = getPitchValues(pitch);\n\n      console.log(`Computed ${times.length} pitch frames`);\n      console.log(`Mean F0: ${values.reduce((a,b) =&gt; a+b) / values.length} Hz`);\n\n      return { times, values };\n    } finally {\n      pitch.free();\n    }\n  } finally {\n    sound.free();\n  }\n}\n\n\nExample 2: Formant Tracking\nimport { getFormantAtTime, getBandwidthAtTime } from '$lib/wasm/acoustic';\n\n// Given: formant object from computeFormant()\nfunction getFormantValuesAtTime(formant: any, time: number) {\n  const f1 = getFormantAtTime(formant, 1, time);\n  const f2 = getFormantAtTime(formant, 2, time);\n  const f3 = getFormantAtTime(formant, 3, time);\n  const f4 = getFormantAtTime(formant, 4, time);\n\n  const b1 = getBandwidthAtTime(formant, 1, time);\n  const b2 = getBandwidthAtTime(formant, 2, time);\n  const b3 = getBandwidthAtTime(formant, 3, time);\n  const b4 = getBandwidthAtTime(formant, 4, time);\n\n  return {\n    formants: [f1, f2, f3, f4],\n    bandwidths: [b1, b2, b3, b4]\n  };\n}\n\n// Usage\nconst sound = createSound(samples, sampleRate);\ntry {\n  const formant = computeFormant(sound, 0.01, 5, 5500, 0.025, 50);\n  try {\n    const vowelFormants = getFormantValuesAtTime(formant, 1.5);\n    console.log('F1:', vowelFormants.formants[0], 'Hz');\n    console.log('F2:', vowelFormants.formants[1], 'Hz');\n  } finally {\n    formant.free();\n  }\n} finally {\n  sound.free();\n}\n\n\nExample 3: Spectrogram with Metadata\nimport { computeSpectrogram, getSpectrogramInfo } from '$lib/wasm/acoustic';\n\nfunction computeSpectrogramForVisualization(sound: any) {\n  const spectrogram = computeSpectrogram(sound, 0.005, 5000, 0.005, 20);\n\n  try {\n    const info = getSpectrogramInfo(spectrogram);\n\n    console.log(`Spectrogram: ${info.nTimes} frames × ${info.nFreqs} bins`);\n    console.log(`Time range: ${info.timeMin} - ${info.timeMax} s`);\n    console.log(`Frequency range: ${info.freqMin} - ${info.freqMax} Hz`);\n\n    // Create ImageData for canvas rendering\n    const imageData = createImageData(info.values, info.nFreqs, info.nTimes);\n\n    return imageData;\n  } finally {\n    spectrogram.free();\n  }\n}\n\n\nExample 4: Full Acoustic Analysis\nimport { computeAcoustics } from '$lib/wasm/acoustic';\n\nasync function analyzeAudio(samples: Float64Array, sampleRate: number) {\n  // All WASM objects automatically freed\n  const results = await computeAcoustics(samples, sampleRate, {\n    timeStep: 0.01,\n    pitchFloor: 75,\n    pitchCeiling: 600,\n    maxFormant: 5500\n  });\n\n  // Results are pure JavaScript objects\n  return {\n    times: results.times,\n    pitch: results.pitch,\n    intensity: results.intensity,\n    f1: results.formants.f1,\n    f2: results.formants.f2,\n    f3: results.formants.f3,\n    f4: results.formants.f4,\n    harmonicity: results.harmonicity,\n    spectrogram: results.spectrogram\n  };\n}",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#initialization-process",
    "href": "development/wasm-integration.html#initialization-process",
    "title": "WASM Integration",
    "section": "Initialization Process",
    "text": "Initialization Process\n\nApp Startup\nFile: src/routes/+layout.svelte\n&lt;script lang=\"ts\"&gt;\n  import { onMount } from 'svelte';\n  import { initWasm, wasmReady } from '$lib/wasm/acoustic';\n  import { config } from '$lib/stores/config';\n\n  let loading = true;\n\n  onMount(async () =&gt; {\n    try {\n      // Get backend from config or default\n      const backend = $config.backend || 'praatfan-local';\n\n      // Initialize WASM\n      await initWasm(backend);\n\n      loading = false;\n    } catch (e) {\n      console.error('Failed to initialize WASM:', e);\n      alert('Failed to load analysis module. Try refreshing the page.');\n    }\n  });\n&lt;/script&gt;\n\n{#if loading}\n  &lt;div&gt;Loading acoustic analysis module...&lt;/div&gt;\n{:else if $wasmReady}\n  &lt;slot /&gt;\n{:else}\n  &lt;div&gt;Failed to initialize&lt;/div&gt;\n{/if}\n\n\nLazy Loading Pattern\nFor viewer route with URL-specified backend:\n&lt;script lang=\"ts\"&gt;\n  import { page } from '$app/stores';\n  import { initWasm, wasmReady } from '$lib/wasm/acoustic';\n\n  onMount(async () =&gt; {\n    const backendParam = $page.url.searchParams.get('backend');\n    const backend = backendParam || 'praatfan';\n\n    await initWasm(backend);\n\n    // Now load audio...\n  });\n&lt;/script&gt;\n\n\nSwitching Backends\nUsers can switch backends at runtime:\n&lt;script&gt;\n  import { initWasm, currentBackend } from '$lib/wasm/acoustic';\n\n  async function switchBackend(newBackend: string) {\n    // Re-initialize with different backend\n    await initWasm(newBackend);\n\n    // Clear and recompute analyses\n    analysisResults.set(null);\n    await runAnalysis();\n  }\n&lt;/script&gt;\n\n&lt;select bind:value={$currentBackend} on:change={e =&gt; switchBackend(e.target.value)}&gt;\n  &lt;option value=\"praatfan-local\"&gt;Local (fastest)&lt;/option&gt;\n  &lt;option value=\"praatfan\"&gt;CDN (MIT/Apache)&lt;/option&gt;\n  &lt;option value=\"praatfan-gpl\"&gt;CDN (GPL)&lt;/option&gt;\n&lt;/select&gt;",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#error-handling",
    "href": "development/wasm-integration.html#error-handling",
    "title": "WASM Integration",
    "section": "Error Handling",
    "text": "Error Handling\n\nInitialization Errors\nCommon causes: - Network failure (CDN backends) - CORS issues (CDN backends) - Missing local files (praatfan-local) - Browser doesn’t support WASM\nHandling:\nimport { initWasm } from '$lib/wasm/acoustic';\n\ntry {\n  await initWasm('praatfan');\n} catch (e) {\n  if (e.message.includes('Failed to fetch')) {\n    // Network error - offer fallback\n    console.error('CDN unreachable, trying local backend...');\n    await initWasm('praatfan-local');\n  } else if (e.message.includes('CORS')) {\n    // CORS error - can't fix, inform user\n    alert('Unable to load analysis module due to network restrictions.');\n  } else {\n    // Unknown error\n    console.error('WASM initialization failed:', e);\n    alert('Failed to initialize acoustic analysis.');\n  }\n}\n\n\nAnalysis Errors\nCommon causes: - Invalid parameters (negative frequencies, etc.) - Empty audio buffer - Sample rate = 0\nHandling:\nimport { computePitch } from '$lib/wasm/acoustic';\n\nfunction safePitchAnalysis(sound: any, timeStep: number, floor: number, ceiling: number) {\n  // Validate parameters\n  if (timeStep &lt;= 0 || timeStep &gt; 1) {\n    throw new Error(`Invalid timeStep: ${timeStep}`);\n  }\n  if (floor &lt;= 0 || ceiling &lt;= floor) {\n    throw new Error(`Invalid pitch range: ${floor}-${ceiling} Hz`);\n  }\n\n  try {\n    return computePitch(sound, timeStep, floor, ceiling);\n  } catch (e) {\n    console.error('Pitch analysis failed:', e);\n    throw new Error('Acoustic analysis failed. Check audio quality.');\n  }\n}",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#performance-considerations",
    "href": "development/wasm-integration.html#performance-considerations",
    "title": "WASM Integration",
    "section": "Performance Considerations",
    "text": "Performance Considerations\n\nComputation Cost\nRelative computational cost:\n\n\n\nAnalysis\nCost\nNotes\n\n\n\n\nWaveform\nMinimal\nNo WASM, just downsampling\n\n\nIntensity\nLow\nSimple RMS calculation\n\n\nPitch\nMedium\nAutocorrelation per frame\n\n\nFormants\nHigh\nLPC analysis per frame\n\n\nSpectrogram\nVery High\nFFT for all time-frequency bins\n\n\nHarmonicity\nMedium\nAutocorrelation-based\n\n\n\n\n\nOptimization Strategies\n1. Analyze only visible window (long audio):\n// Instead of analyzing full 5-minute file:\nawait runAnalysisForRange(0, 300);  // All 5 minutes\n\n// Analyze only visible 10 seconds:\nawait runAnalysisForRange(cursorTime - 5, cursorTime + 5);\n2. Debounce during zoom:\nimport { debounce } from 'lodash-es';\n\nconst debouncedAnalysis = debounce(async () =&gt; {\n  await runAnalysisForRange($timeRange.start, $timeRange.end);\n}, 300);  // Wait 300ms after zoom stops\n\n$: if ($timeRange) {\n  debouncedAnalysis();\n}\n3. Reuse Sound objects:\n// ❌ BAD: Create Sound repeatedly\nfor (let i = 0; i &lt; 100; i++) {\n  const sound = createSound(samples, sampleRate);\n  const pitch = computePitch(sound, ...);\n  pitch.free();\n  sound.free();\n}\n\n// ✅ GOOD: Reuse Sound object\nconst sound = createSound(samples, sampleRate);\ntry {\n  for (let i = 0; i &lt; 100; i++) {\n    const pitch = computePitch(sound, ...);\n    // Use pitch...\n    pitch.free();\n  }\n} finally {\n  sound.free();\n}\n4. Extract only needed formants:\n// ❌ BAD: Extract all formants when only F1/F2 needed\nconst formant = computeFormant(sound, 0.01, 5, 5500, 0.025, 50);\nconst f1 = formant.formant_values(1);\nconst f2 = formant.formant_values(2);\nformant.free();\n\n// ✅ GOOD: Use numFormants=2 for faster computation\nconst formant = computeFormant(sound, 0.01, 2, 5500, 0.025, 50);\nconst f1 = formant.formant_values(1);\nconst f2 = formant.formant_values(2);\nformant.free();",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#debugging-wasm-integration",
    "href": "development/wasm-integration.html#debugging-wasm-integration",
    "title": "WASM Integration",
    "section": "Debugging WASM Integration",
    "text": "Debugging WASM Integration\n\nCheck Backend Status\n&lt;script&gt;\n  import { wasmReady, currentBackend } from '$lib/wasm/acoustic';\n  import { getBackendType } from '$lib/wasm/acoustic';\n\n  $: console.log('WASM ready:', $wasmReady);\n  $: console.log('Current backend:', $currentBackend);\n  $: if ($wasmReady) {\n    console.log('Backend type:', getBackendType());\n  }\n&lt;/script&gt;\n\n\nVerify Memory Cleanup\n// Track Sound object count\nlet soundCount = 0;\n\nconst originalCreateSound = createSound;\ncreateSound = function(...args) {\n  soundCount++;\n  console.log(`Created Sound #${soundCount}`);\n  const sound = originalCreateSound(...args);\n\n  const originalFree = sound.free;\n  sound.free = function() {\n    soundCount--;\n    console.log(`Freed Sound, ${soundCount} remaining`);\n    originalFree.call(sound);\n  };\n\n  return sound;\n};\n\n\nTest Analysis Results\nimport { computePitch } from '$lib/wasm/acoustic';\n\n// Verify pitch range\nconst sound = createSound(samples, sampleRate);\nconst pitch = computePitch(sound, 0.01, 75, 600);\n\nconst values = getPitchValues(pitch);\nconst validValues = values.filter(v =&gt; !isNaN(v));\nconst min = Math.min(...validValues);\nconst max = Math.max(...validValues);\n\nconsole.log(`Pitch range: ${min.toFixed(1)} - ${max.toFixed(1)} Hz`);\nconsole.assert(min &gt;= 70 && max &lt;= 610, 'Pitch out of expected range!');\n\npitch.free();\nsound.free();",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#adding-new-backends",
    "href": "development/wasm-integration.html#adding-new-backends",
    "title": "WASM Integration",
    "section": "Adding New Backends",
    "text": "Adding New Backends\n\nSteps to Add a Backend\n1. Add backend to type definition:\n// src/lib/stores/config.ts\nexport type AcousticBackend = 'praatfan-gpl' | 'praatfan' | 'praatfan-local' | 'my-new-backend';\n2. Add backend URL:\n// src/lib/wasm/acoustic.ts\nconst REMOTE_BACKEND_URLS: Record&lt;string, string&gt; = {\n  'praatfan-gpl': 'https://...',\n  'praatfan': 'https://...',\n  'my-new-backend': 'https://example.com/my-backend.js'\n};\n3. Map to backend type:\nconst BACKEND_TYPE: Record&lt;AcousticBackend, 'praatfan-gpl' | 'praatfan' | 'my-type'&gt; = {\n  'praatfan-gpl': 'praatfan-gpl',\n  'praatfan': 'praatfan',\n  'praatfan-local': 'praatfan',\n  'my-new-backend': 'my-type'  // Define API type\n};\n4. Update abstraction functions if API differs:\nexport function computePitch(sound: any, timeStep: number, floor: number, ceiling: number) {\n  const backend = getBackendType();\n\n  if (backend === 'my-type') {\n    // Handle new backend API\n    return sound.calculate_pitch(timeStep, floor, ceiling);\n  } else if (backend === 'praatfan-gpl') {\n    return sound.to_pitch(timeStep, floor, ceiling);\n  } else {\n    return sound.to_pitch_ac(timeStep, floor, ceiling);\n  }\n}\n5. Test all analysis functions:\n# Load new backend\nnpm run dev\n# Open browser console\n&gt; import { initWasm } from '$lib/wasm/acoustic';\n&gt; await initWasm('my-new-backend');\n&gt; // Run test analyses...",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "development/wasm-integration.html#see-also",
    "href": "development/wasm-integration.html#see-also",
    "title": "WASM Integration",
    "section": "See Also",
    "text": "See Also\n\nArchitecture - Overall system design\nStores - State management (analysis store uses WASM)\nBackends Reference - User-facing backend documentation\npraatfan-core-rs - GPL backend source\npraatfan-core-clean - MIT/Apache backend source",
    "crumbs": [
      "Development",
      "WASM Integration"
    ]
  },
  {
    "objectID": "BUILD_SYNC.html",
    "href": "BUILD_SYNC.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "BUILD_SYNC.html#why",
    "href": "BUILD_SYNC.html#why",
    "title": "",
    "section": "Why?",
    "text": "Why?\nThe embedded examples in the documentation (especially in embedding/ section) demonstrate the viewer with live, working examples. These need the actual built app."
  },
  {
    "objectID": "BUILD_SYNC.html#methods",
    "href": "BUILD_SYNC.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n1. Manual: npm Script (Simple)\nRun when needed:\nnpm run build:docs\nThis builds the app and copies build/ → docs/live/.\nThen commit:\ngit add docs/live\ngit commit -m \"Update live viewer\"\n\n\n2. Automatic: Git Hook (Local Development)\nInstall the post-commit hook to automatically sync on every commit:\n# From repository root\nln -sf ../../scripts/hooks/post-commit .git/hooks/post-commit\nchmod +x .git/hooks/post-commit\nBehavior: - After each commit, if build/ exists, it’s automatically copied to docs/live/ - The commit is amended to include the updated docs/live/\nDisable temporarily:\ngit commit --no-verify -m \"message\"\nUninstall:\nrm .git/hooks/post-commit\nSee scripts/hooks/README.md for details.\n\n\n3. Automatic: GitHub Actions (Production)\nThe deploy-docs.yml workflow automatically:\n\nBuilds the Ozen-web app\nCopies build/ → docs/live/\nCaptures screenshots\nRenders Quarto documentation\nDeploys to GitHub Pages\n\nTriggers: - Push to master/main with changes to: - docs/** (documentation content) - src/** (app source code) - static/** (static assets) - Config files (package.json, svelte.config.js, vite.config.ts) - Manual trigger via GitHub Actions UI\nThis ensures the documentation site always includes the latest build."
  },
  {
    "objectID": "BUILD_SYNC.html#recommended-workflow",
    "href": "BUILD_SYNC.html#recommended-workflow",
    "title": "",
    "section": "Recommended Workflow",
    "text": "Recommended Workflow\nFor active development:\n# 1. Install git hook for convenience\nln -sf ../../scripts/hooks/post-commit .git/hooks/post-commit\nchmod +x .git/hooks/post-commit\n\n# 2. Build the app\nnpm run build\n\n# 3. Commit changes (hook auto-updates docs/live/)\ngit add src/\ngit commit -m \"Add new feature\"\n\n# 4. Push to trigger docs deployment\ngit push\nFor one-off updates:\n# Build and copy manually\nnpm run build:docs\n\n# Commit\ngit add docs/live\ngit commit -m \"Update live viewer\"\ngit push"
  },
  {
    "objectID": "BUILD_SYNC.html#file-size-considerations",
    "href": "BUILD_SYNC.html#file-size-considerations",
    "title": "",
    "section": "File Size Considerations",
    "text": "File Size Considerations\nThe docs/live/ directory contains a full build (~2-5 MB). This is acceptable for git, but if size becomes an issue, consider:\n\nGit LFS for the live directory\nSeparate branch for documentation builds\nCDN hosting of the viewer (reference external URL instead)\n\nFor now, committing docs/live/ directly is the simplest approach."
  },
  {
    "objectID": "BUILD_SYNC.html#verifying-embedded-examples",
    "href": "BUILD_SYNC.html#verifying-embedded-examples",
    "title": "",
    "section": "Verifying Embedded Examples",
    "text": "Verifying Embedded Examples\nAfter updating docs/live/, verify the embedded examples work:\ncd docs\nquarto preview\nNavigate to any embedding example page (e.g., embedding/examples.html) and verify the iframes load correctly."
  },
  {
    "objectID": "BUILD_SYNC.html#see-also",
    "href": "BUILD_SYNC.html#see-also",
    "title": "",
    "section": "See Also",
    "text": "See Also\n\nscripts/hooks/README.md - Git hooks documentation\n.github/workflows/deploy-docs.yml - Deployment workflow\nembedding/ documentation - Pages using embedded viewer"
  },
  {
    "objectID": "reference/file-formats.html",
    "href": "reference/file-formats.html",
    "title": "File Formats",
    "section": "",
    "text": "Ozen-web supports multiple file formats for audio input, annotation import/export, and data export. This page documents format specifications and compatibility.",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#overview",
    "href": "reference/file-formats.html#overview",
    "title": "File Formats",
    "section": "",
    "text": "Ozen-web supports multiple file formats for audio input, annotation import/export, and data export. This page documents format specifications and compatibility.",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#audio-formats",
    "href": "reference/file-formats.html#audio-formats",
    "title": "File Formats",
    "section": "Audio Formats",
    "text": "Audio Formats\n\nWAV (Waveform Audio File Format)\nExtension: .wav MIME type: audio/wav, audio/x-wav Support: ✅ Full (recommended)\nSpecifications: - Encoding: PCM (uncompressed) - Bit depth: 8-bit, 16-bit, 24-bit, 32-bit - Sample rate: Any (8 kHz - 96 kHz typical) - Channels: Mono or stereo (stereo is mixed to mono)\nAdvantages: - Uncompressed (no quality loss) - Universally compatible - Fast to decode\nDisadvantages: - Large file size - Not suitable for web streaming\nExample:\n# Convert audio to WAV\nffmpeg -i input.mp3 -ar 16000 -ac 1 output.wav\n\n\nMP3 (MPEG Audio Layer III)\nExtension: .mp3 MIME type: audio/mpeg Support: ✅ Full\nSpecifications: - Encoding: Lossy compression - Bit rate: 64-320 kbps - Sample rate: 8-48 kHz - Channels: Mono or stereo\nAdvantages: - Small file size - Good for web distribution - Widely supported\nDisadvantages: - Lossy compression artifacts - Not recommended for precise acoustic analysis - Compression can affect formant measurements\nExample:\n# Convert to MP3 (192 kbps)\nffmpeg -i input.wav -b:a 192k output.mp3\n\n\nOGG Vorbis\nExtension: .ogg MIME type: audio/ogg Support: ✅ Full\nSpecifications: - Encoding: Lossy compression (Vorbis codec) - Bit rate: Variable (typically 64-320 kbps) - Sample rate: 8-192 kHz - Channels: Mono or stereo\nAdvantages: - Open format (no licensing fees) - Better quality than MP3 at same bitrate - Smaller than WAV\nDisadvantages: - Lossy compression - Less universal than WAV/MP3\n\n\nFormat Recommendations\n\n\n\nUse Case\nRecommended Format\n\n\n\n\nResearch analysis\nWAV (16-bit, 16-44.1 kHz)\n\n\nLong recordings\nMP3 (192+ kbps)\n\n\nWeb embedding\nMP3 or OGG\n\n\nArchival\nWAV (24-bit, 48 kHz)\n\n\nField recordings\nWAV (16-bit, 16 kHz)",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#annotation-formats",
    "href": "reference/file-formats.html#annotation-formats",
    "title": "File Formats",
    "section": "Annotation Formats",
    "text": "Annotation Formats\n\nTextGrid (Praat Format)\nExtension: .TextGrid Support: ✅ Import and export\nDescription:\nTextGrid is Praat’s native annotation format, widely used in phonetics research. Ozen-web supports both short and long TextGrid formats.\nFormat variants:\n\nShort Format\nFile type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\nxmin = 0\nxmax = 2.5\ntiers? &lt;exists&gt;\nsize = 2\nitem []:\n    item [1]:\n        class = \"IntervalTier\"\n        name = \"words\"\n        xmin = 0\n        xmax = 2.5\n        intervals: size = 3\n        intervals [1]:\n            xmin = 0\n            xmax = 0.8\n            text = \"the\"\n        intervals [2]:\n            xmin = 0.8\n            xmax = 1.5\n            text = \"cat\"\n        intervals [3]:\n            xmin = 1.5\n            xmax = 2.5\n            text = \"\"\n\n\nLong Format\nFile type = \"ooTextFile\"\nObject class = \"TextGrid\"\n\n0\n2.5\n&lt;exists&gt;\n2\n\"IntervalTier\"\n\"words\"\n0\n2.5\n3\n0\n0.8\n\"the\"\n0.8\n1.5\n\"cat\"\n1.5\n2.5\n\"\"\nOzen-web support:\n\n\n\nFeature\nImport\nExport\n\n\n\n\nInterval tiers\n✅ Yes\n✅ Yes\n\n\nPoint tiers\n❌ No\n❌ No\n\n\nMultiple tiers\n✅ Yes\n✅ Yes\n\n\nUTF-8 text\n✅ Yes\n✅ Yes\n\n\nIPA characters\n✅ Yes\n✅ Yes\n\n\nShort format\n✅ Yes\n✅ Yes (default)\n\n\nLong format\n✅ Yes\n❌ No\n\n\n\nCompatibility:\n\n✅ Praat (all versions)\n✅ Montreal Forced Aligner\n✅ Elan (via conversion)\n✅ WebMAUS\n\nExample (programmatic creation):\n# Python (using pympi or textgrid library)\nimport textgrid\n\ntg = textgrid.TextGrid()\ntier = textgrid.IntervalTier(name='words', minTime=0, maxTime=2.5)\ntier.add(0, 0.8, 'the')\ntier.add(0.8, 1.5, 'cat')\ntier.add(1.5, 2.5, '')\ntg.append(tier)\ntg.write('output.TextGrid')\nLimitations:\n\n\n\n\n\n\nWarningPoint Tiers Not Supported\n\n\n\nOzen-web currently does not support point tiers. Convert point tiers to interval tiers in Praat before importing:\n# In Praat\ntextgrid = selected(\"TextGrid\")\nCreate TextGrid... 0 2.5 \"words\" \"\"\n# Manually convert points to intervals",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#data-export-formats",
    "href": "reference/file-formats.html#data-export-formats",
    "title": "File Formats",
    "section": "Data Export Formats",
    "text": "Data Export Formats\n\nTSV (Tab-Separated Values)\nExtension: .tsv Support: ✅ Export only\nDescription:\nData points are exported as tab-separated values for analysis in R, Python, Excel, or Praat.\nFormat:\ntime    freq    pitch   intensity   f1  f2  f3  f4  b1  b2  b3  b4  hnr cog spectral_tilt   a1_p0   label_words label_phones\n1.234   720 245 68  720 1240    2650    3500    80  110 150 200 15.3    5420    -2.1    -5.4    \"cat\"   \"æ\"\n1.567   850 250 70  850 1180    2580    3450    85  105 145 195 16.1    5380    -1.9    -4.8    \"sat\"   \"æ\"\nColumn descriptions:\n\n\n\nColumn\nUnit\nDescription\n\n\n\n\ntime\nseconds\nTime position of data point\n\n\nfreq\nHz\nFrequency at cursor position\n\n\npitch\nHz\nFundamental frequency (F0)\n\n\nintensity\ndB SPL\nSound pressure level\n\n\nf1, f2, f3, f4\nHz\nFormant frequencies\n\n\nb1, b2, b3, b4\nHz\nFormant bandwidths\n\n\nhnr\ndB\nHarmonics-to-noise ratio\n\n\ncog\nHz\nSpectral center of gravity\n\n\nspectral_tilt\ndB/Hz\nSpectral slope\n\n\na1_p0\ndB\nNasal measure\n\n\nlabel_*\ntext\nAnnotation labels (one column per tier)\n\n\n\nMissing values:\n\nEmpty cells indicate measurement not available\nCommon for pitch in unvoiced regions\nCommon for labels when no annotation at that time\n\nImporting into tools:\nR:\ndata &lt;- read.table(\"data-points.tsv\", header=TRUE, sep=\"\\t\", quote=\"\\\"\")\nPython/Pandas:\nimport pandas as pd\ndf = pd.read_csv(\"data-points.tsv\", sep='\\t')\nPraat:\ntable = Read Table from tab-separated file: \"data-points.tsv\"\nExcel: - File → Open → Select TSV file - Choose “Tab” as delimiter",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#audio-save-format",
    "href": "reference/file-formats.html#audio-save-format",
    "title": "File Formats",
    "section": "Audio Save Format",
    "text": "Audio Save Format\n\nWAV Export\nSupport: ✅ Save audio\nOzen-web can save the loaded audio as WAV file:\n\nFormat: 16-bit PCM WAV\nSample rate: Original (preserved from input)\nChannels: Mono (if stereo input, mixed down)\nEncoding: Uncompressed\n\nUse cases: - Save microphone recording - Export processed audio - Convert MP3/OGG to WAV\nLimitations: - Cannot edit audio (read-only) - Cannot apply effects - Save as-is only",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#file-size-guidelines",
    "href": "reference/file-formats.html#file-size-guidelines",
    "title": "File Formats",
    "section": "File Size Guidelines",
    "text": "File Size Guidelines\n\nAudio Files\n\n\n\nDuration\nWAV (16-bit, 16 kHz)\nMP3 (192 kbps)\nOGG (Q5)\n\n\n\n\n10 seconds\n~320 KB\n~240 KB\n~160 KB\n\n\n1 minute\n~1.9 MB\n~1.4 MB\n~960 KB\n\n\n5 minutes\n~9.5 MB\n~7.2 MB\n~4.8 MB\n\n\n30 minutes\n~57 MB\n~43 MB\n~29 MB\n\n\n1 hour\n~115 MB\n~86 MB\n~58 MB\n\n\n\n\n\nTextGrid Files\nTextGrid files are plain text:\n\nSmall: &lt;1 KB (single tier, few intervals)\nMedium: 1-10 KB (multiple tiers, detailed annotation)\nLarge: 10-100 KB (many tiers, long recordings)\n\n\n\nTSV Files\nData point exports:\n\n~100 bytes per data point\n100 data points ≈ 10 KB\n1000 data points ≈ 100 KB",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#character-encoding",
    "href": "reference/file-formats.html#character-encoding",
    "title": "File Formats",
    "section": "Character Encoding",
    "text": "Character Encoding\n\nText Files\nAll text files use UTF-8 encoding:\n\nTextGrid files\nTSV files\nConfiguration files\n\nThis ensures support for: - IPA characters (ɑ, ə, ʃ, etc.) - Unicode symbols - Non-Latin scripts (中文, العربية, etc.)\nEncoding issues:\nIf you see garbled characters, the file may be in a different encoding. Convert to UTF-8:\n# Linux/Mac\niconv -f ISO-8859-1 -t UTF-8 input.TextGrid &gt; output.TextGrid\n\n# Or use Praat (always saves UTF-8)",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#browser-compatibility",
    "href": "reference/file-formats.html#browser-compatibility",
    "title": "File Formats",
    "section": "Browser Compatibility",
    "text": "Browser Compatibility\n\nSupported Audio Formats by Browser\n\n\n\nFormat\nChrome\nFirefox\nSafari\nEdge\n\n\n\n\nWAV\n✅\n✅\n✅\n✅\n\n\nMP3\n✅\n✅\n✅\n✅\n\n\nOGG\n✅\n✅\n❌\n✅\n\n\n\nNote: Safari does not support OGG Vorbis. Use WAV or MP3 for maximum compatibility.\n\n\nFile API Support\nAll modern browsers support: - File picker (input type=“file”) - Drag and drop - File System Access API (save files)\nMinimum versions: - Chrome 90+ - Firefox 88+ - Safari 14+ - Edge 90+",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#conversion-tools",
    "href": "reference/file-formats.html#conversion-tools",
    "title": "File Formats",
    "section": "Conversion Tools",
    "text": "Conversion Tools\n\nffmpeg (Command Line)\nConvert to WAV:\n# From MP3\nffmpeg -i input.mp3 output.wav\n\n# Specify sample rate and channels\nffmpeg -i input.mp3 -ar 16000 -ac 1 output.wav\n\n# From video (extract audio)\nffmpeg -i video.mp4 -vn -acodec pcm_s16le output.wav\nConvert to MP3:\n# High quality\nffmpeg -i input.wav -b:a 192k output.mp3\n\n# Smaller file\nffmpeg -i input.wav -b:a 128k output.mp3\n\n\nSoX (Sound eXchange)\n# Convert format\nsox input.mp3 output.wav\n\n# Resample\nsox input.wav -r 16000 output.wav\n\n# Mono conversion\nsox input.wav -c 1 output.wav\n\n\nPraat\n# Read audio\nsound = Read from file: \"input.mp3\"\n\n# Save as WAV\nSave as WAV file: \"output.wav\"\n\n\nOnline Tools\n\nCloudConvert\nOnline Audio Converter",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#future-format-support",
    "href": "reference/file-formats.html#future-format-support",
    "title": "File Formats",
    "section": "Future Format Support",
    "text": "Future Format Support\nPlanned additions:\n\nFLAC - Lossless compression audio\nWebM - Web-native audio/video\nELAN (.eaf) - Alternative annotation format\nJSON - Machine-readable annotation export\nCSV - Alternative to TSV",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/file-formats.html#see-also",
    "href": "reference/file-formats.html#see-also",
    "title": "File Formats",
    "section": "See Also",
    "text": "See Also\n\nTutorial: Loading Audio - Supported loading methods\nTutorial: Exporting - Export workflows\nAnnotations - TextGrid features\nData Points - TSV export details",
    "crumbs": [
      "File Formats"
    ]
  },
  {
    "objectID": "reference/configuration.html",
    "href": "reference/configuration.html",
    "title": "Configuration",
    "section": "",
    "text": "Ozen-web can be customized using an optional config.yaml file placed in the same directory as the application. This file allows you to override default colors, formant presets, spectrogram settings, and more.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#overview",
    "href": "reference/configuration.html#overview",
    "title": "Configuration",
    "section": "",
    "text": "Ozen-web can be customized using an optional config.yaml file placed in the same directory as the application. This file allows you to override default colors, formant presets, spectrogram settings, and more.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#configuration-file-location",
    "href": "reference/configuration.html#configuration-file-location",
    "title": "Configuration",
    "section": "Configuration File Location",
    "text": "Configuration File Location\nFor local development:\nozen-web/\n├── static/\n│   └── config.yaml    # Place here\nFor deployed sites:\nyour-site/\n├── index.html\n├── _app/\n└── config.yaml        # Place alongside index.html\nThe app automatically loads config.yaml on startup. If the file is not found, built-in defaults are used.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#basic-structure",
    "href": "reference/configuration.html#basic-structure",
    "title": "Configuration",
    "section": "Basic Structure",
    "text": "Basic Structure\n# config.yaml\ncolors:\n  cursor: '#ff0000'\n  pitch: '#0000ff'\n\nformantPresets:\n  female:\n    maxFormant: 5500\n\nspectrogram:\n  maxFrequency: 5000\n\n\n\n\n\n\nNote\n\n\n\nThe config file is optional. Only include settings you want to override — defaults are used for omitted values.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#complete-configuration-reference",
    "href": "reference/configuration.html#complete-configuration-reference",
    "title": "Configuration",
    "section": "Complete Configuration Reference",
    "text": "Complete Configuration Reference\n\nColors\n\nCursor and Selection\ncolors:\n  # Cursor line\n  cursor: '#ff0000'           # Red cursor\n  cursorWidth: 1              # Line width in pixels\n\n  # Selection highlight\n  selection:\n    fill: 'rgba(255, 192, 203, 0.4)'  # Semi-transparent pink\n    border: '#ff0080'                  # Pink border\nDefaults: - Cursor: Red (#ff0000) - Selection fill: Semi-transparent blue - Selection border: Solid blue\n\n\nWaveform\ncolors:\n  waveform:\n    background: '#ffffff'     # White background\n    line: '#000000'           # Black waveform\n    lineWidth: 1              # Line width\nDefaults: - Background: White - Line: Black - Line width: 1px\n\n\nAcoustic Overlays\ncolors:\n  # Pitch (F0)\n  pitch: '#0000ff'            # Blue\n  pitchWidth: 2               # Line width\n\n  # Intensity\n  intensity: '#008000'        # Green\n  intensityWidth: 2\n\n  # HNR\n  hnr: '#ff8000'              # Orange\n  hnrWidth: 2\n\n  # CoG (Center of Gravity)\n  cog: '#800080'              # Purple\n  cogWidth: 2\n\n  # Spectral Tilt\n  spectralTilt: '#00ffff'     # Cyan\n  spectralTiltWidth: 2\nDefaults: - Pitch: Blue (#0000ff) - Intensity: Green (#008000) - HNR: Orange - CoG: Purple - All widths: 2px\n\n\nFormants\ncolors:\n  formant:\n    f1: '#ff0000'             # Bright red (F1)\n    f2: '#ff8080'             # Light red (F2)\n    f3: '#ff4040'             # Medium red (F3)\n    f4: '#ffc0c0'             # Very light red (F4)\n    size: 3                   # Dot size in pixels\nDefaults: - F1-F4: Red gradient (dark to light) - Dot size: 3px\n\n\n\n\n\n\nTip\n\n\n\nUse different shades of the same color for formants (F1-F4) to keep them visually grouped while distinguishing each formant.\n\n\n\n\nAnnotations\ncolors:\n  # Tier backgrounds\n  tier:\n    background: '#f5f5f5'     # Light gray\n    selected: '#dcdcff'       # Light blue (when selected)\n    border: '#808080'         # Gray border\n    text: '#000000'           # Black text\n\n  # Boundary lines\n  boundary: '#0000ff'         # Blue\n  boundaryHover: '#ff0000'    # Red (on mouse hover)\n  boundaryWidth: 2            # Line width\nDefaults: - Tier background: Light gray - Selected tier: Light blue - Boundaries: Blue, red on hover - Boundary width: 2px\n\n\n\nFormant Presets\nDefine formant analysis parameters for different speaker types:\nformantPresets:\n  female:\n    maxFormant: 5500          # Hz - analysis ceiling\n    numFormants: 5            # Number of formants to track\n\n  male:\n    maxFormant: 5000\n    numFormants: 5\n\n  child:\n    maxFormant: 8000          # Higher ceiling for children's voices\n    numFormants: 5\nUsage: Select the preset in the app’s formant settings dropdown.\nDefaults: - Female: 5500 Hz, 5 formants - Male: 5000 Hz, 5 formants - Child: 8000 Hz, 5 formants\n\n\n\n\n\n\nTip\n\n\n\nChoosing maxFormant:\n\nToo low: Missing high formants (especially F3, F4 in female/child speech)\nToo high: Spurious formants, tracking errors\nRule of thumb: Female/child ~5500-8000 Hz, Male ~5000-5500 Hz\n\n\n\n\n\nSpectrogram Settings\nspectrogram:\n  dynamicRange: 70.0          # dB - contrast range\n  maxFrequency: 5000          # Hz - vertical axis maximum\n  windowLength: 0.005         # seconds - analysis window (5ms)\n  timeStep: 0.002             # seconds - time resolution (2ms)\nParameters:\n\n\n\n\n\n\n\n\n\nParameter\nDescription\nDefault\nRange\n\n\n\n\ndynamicRange\nContrast between dark and light (dB)\n70.0\n30-100\n\n\nmaxFrequency\nVertical axis ceiling (Hz)\n5000\n1000-22050\n\n\nwindowLength\nAnalysis window duration (s)\n0.005\n0.001-0.05\n\n\ntimeStep\nTime between analyses (s)\n0.002\n0.0005-0.01\n\n\n\nEffect of parameters:\n\ndynamicRange ↑ → More contrast, darker background\nwindowLength ↑ → Better frequency resolution, worse time resolution\nwindowLength ↓ → Better time resolution, worse frequency resolution\ntimeStep ↓ → Smoother spectrogram, slower computation\n\n\n\n\n\n\n\nNote\n\n\n\nFor most speech analysis, the defaults (5ms window, 2ms step) provide a good balance.\n\n\n\n\nPitch Settings\npitch:\n  displayFloor: 75            # Hz - minimum pitch to display\n  displayCeiling: 500         # Hz - maximum pitch to display\n  floor: 75                   # Hz - detection floor\n  ceiling: 600                # Hz - detection ceiling\nDisplay vs. Detection:\n\ndisplayFloor/displayCeiling: Visual y-axis range on spectrogram\nfloor/ceiling: Pitch detection algorithm range\n\nTypical values:\n\n\n\nSpeaker Type\nFloor\nCeiling\n\n\n\n\nMale\n50-75 Hz\n300-400 Hz\n\n\nFemale\n100-150 Hz\n400-600 Hz\n\n\nChild\n150-200 Hz\n600-800 Hz\n\n\n\n\n\nAnnotation Settings\nannotation:\n  defaultTiers:               # Tiers created when loading audio\n    - words\n    - phones\n    - syllables\nWhen you load an audio file, these tiers are automatically created.\nDefault: words, phones",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#example-configurations",
    "href": "reference/configuration.html#example-configurations",
    "title": "Configuration",
    "section": "Example Configurations",
    "text": "Example Configurations\n\nHigh-Contrast Theme\nFor presentations or low-light environments:\ncolors:\n  cursor: '#ffff00'           # Yellow cursor\n  pitch: '#00ffff'            # Cyan pitch\n  formant:\n    f1: '#ff00ff'             # Magenta\n    f2: '#ffff00'             # Yellow\n    f3: '#00ffff'             # Cyan\n    f4: '#ff8080'             # Light red\n  waveform:\n    background: '#000000'     # Black background\n    line: '#00ff00'           # Green waveform\n\n\nFemale Speaker Analysis\nformantPresets:\n  female:\n    maxFormant: 5800          # Slightly higher for accurate F4\n    numFormants: 5\n\npitch:\n  floor: 120\n  ceiling: 500\n  displayFloor: 120\n  displayCeiling: 500\n\nspectrogram:\n  maxFrequency: 7500          # Show higher frequencies\n\n\nMale Speaker Analysis\nformantPresets:\n  male:\n    maxFormant: 5000\n    numFormants: 5\n\npitch:\n  floor: 60\n  ceiling: 300\n  displayFloor: 60\n  displayCeiling: 300\n\nspectrogram:\n  maxFrequency: 5000          # Standard range sufficient\n\n\nConsonant Analysis\nFor analyzing fricatives and stops:\nspectrogram:\n  maxFrequency: 10000         # Show high-frequency content\n  windowLength: 0.003         # Shorter window for better time resolution\n  timeStep: 0.001             # Finer time steps\n\ncolors:\n  cog: '#ff00ff'              # Highlight CoG for fricative analysis\n  cogWidth: 3\n\n\nTeaching/Presentation\nLarge, visible overlays:\ncolors:\n  pitch: '#0000ff'\n  pitchWidth: 4               # Thicker lines\n\n  formant:\n    f1: '#ff0000'\n    f2: '#00ff00'\n    f3: '#0000ff'\n    f4: '#ff00ff'\n    size: 5                   # Larger dots\n\n  boundary: '#ff0000'\n  boundaryWidth: 4",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#loading-custom-configurations",
    "href": "reference/configuration.html#loading-custom-configurations",
    "title": "Configuration",
    "section": "Loading Custom Configurations",
    "text": "Loading Custom Configurations\n\nAt Startup\nPlace config.yaml in the app directory. The app loads it automatically on startup.\n\n\nRuntime Loading\nSome implementations support loading custom configs via UI:\n\nClick “Settings” or “⚙️” icon\nSelect “Load Configuration”\nChoose your .yaml file\n\nCheck your app version for runtime config loading support.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#color-format-reference",
    "href": "reference/configuration.html#color-format-reference",
    "title": "Configuration",
    "section": "Color Format Reference",
    "text": "Color Format Reference\nColors can be specified in multiple formats:\ncolors:\n  pitch: '#0000ff'                    # Hex (6-digit)\n  intensity: '#008000'                # Hex (6-digit)\n  selection:\n    fill: 'rgba(255, 192, 203, 0.4)' # RGBA (with alpha)\n    border: 'rgb(255, 0, 128)'       # RGB\nSupported formats:\n\nHex: #rrggbb (e.g., #ff0000)\nRGB: rgb(r, g, b) (e.g., rgb(255, 0, 0))\nRGBA: rgba(r, g, b, a) (e.g., rgba(255, 0, 0, 0.5))\nNamed colors: red, blue, green (limited palette)",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#validation",
    "href": "reference/configuration.html#validation",
    "title": "Configuration",
    "section": "Validation",
    "text": "Validation\nInvalid config files will show errors in the browser console:\nConfig validation error: Invalid color format '#xyz'\nConfig validation error: maxFormant must be between 1000 and 22050\nIf validation fails, defaults are used for that setting.",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#troubleshooting",
    "href": "reference/configuration.html#troubleshooting",
    "title": "Configuration",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nConfig not loading:\n\nCheck file is named exactly config.yaml (not config.yml)\nEnsure YAML syntax is valid (use YAML Lint)\nCheck browser console for errors (F12 → Console)\nEnsure file is in correct location (same directory as index.html)\n\nColors not changing:\n\nVerify color format is valid (#rrggbb or rgba(...))\nClear browser cache (Ctrl+Shift+R)\nCheck console for validation errors\n\nFormant preset not working:\n\nEnsure maxFormant is reasonable (1000-22050 Hz)\nVerify you’ve selected the preset in the app UI\nCheck numFormants is between 3-5",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#default-configuration",
    "href": "reference/configuration.html#default-configuration",
    "title": "Configuration",
    "section": "Default Configuration",
    "text": "Default Configuration\nIf you need to reset to defaults, simply delete or rename config.yaml. The complete default configuration is:\ncolors:\n  cursor: '#ff0000'\n  cursorWidth: 1\n  selection:\n    fill: 'rgba(173, 216, 230, 0.4)'\n    border: '#0080ff'\n  waveform:\n    background: '#ffffff'\n    line: '#000000'\n    lineWidth: 1\n  pitch: '#0000ff'\n  pitchWidth: 2\n  intensity: '#008000'\n  intensityWidth: 2\n  formant:\n    f1: '#ff0000'\n    f2: '#ff8080'\n    f3: '#ff4040'\n    f4: '#ffc0c0'\n    size: 3\n  tier:\n    background: '#f5f5f5'\n    selected: '#dcdcff'\n    border: '#808080'\n    text: '#000000'\n  boundary: '#0000ff'\n  boundaryHover: '#ff0000'\n  boundaryWidth: 2\n\nformantPresets:\n  female:\n    maxFormant: 5500\n    numFormants: 5\n  male:\n    maxFormant: 5000\n    numFormants: 5\n  child:\n    maxFormant: 8000\n    numFormants: 5\n\nspectrogram:\n  dynamicRange: 70.0\n  maxFrequency: 5000\n  windowLength: 0.005\n  timeStep: 0.002\n\npitch:\n  displayFloor: 75\n  displayCeiling: 500\n  floor: 75\n  ceiling: 600\n\nannotation:\n  defaultTiers:\n    - words\n    - phones",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "reference/configuration.html#see-also",
    "href": "reference/configuration.html#see-also",
    "title": "Configuration",
    "section": "See Also",
    "text": "See Also\n\nFeatures Overview — What each setting controls\nWASM Backends — Backend-specific configuration\nDevelopment: Stores — How configuration is implemented",
    "crumbs": [
      "Configuration"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html",
    "href": "embedding/basic-usage.html",
    "title": "Basic Embedding",
    "section": "",
    "text": "Ozen-web can be embedded in web pages, documentation, and presentations using standard HTML iframes. The viewer supports URL parameters for pre-loading audio and configuring overlays, making it ideal for interactive examples and educational materials.",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#overview",
    "href": "embedding/basic-usage.html#overview",
    "title": "Basic Embedding",
    "section": "",
    "text": "Ozen-web can be embedded in web pages, documentation, and presentations using standard HTML iframes. The viewer supports URL parameters for pre-loading audio and configuring overlays, making it ideal for interactive examples and educational materials.",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#simple-iframe-embedding",
    "href": "embedding/basic-usage.html#simple-iframe-embedding",
    "title": "Basic Embedding",
    "section": "Simple iframe Embedding",
    "text": "Simple iframe Embedding\n\nBasic Example\nEmbed the hosted viewer directly:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer\"\n        width=\"800\"\n        height=\"600\"\n        style=\"border: 1px solid #ccc;\"&gt;\n&lt;/iframe&gt;\nThis embeds the mobile viewer interface with an empty file drop zone.\n\n\nWith Audio Pre-loaded\nLoad audio automatically via URL parameter:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/audio.wav\"\n        width=\"800\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\nRequirements: - Audio file must be accessible via HTTPS - Server must enable CORS (see below) - Supported formats: WAV, MP3, OGG\n\n\nWith Overlays Configured\nPre-enable acoustic overlays:\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/audio.wav&overlays=pitch,formants,intensity\"\n        width=\"800\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\nOverlay options: - pitch - Fundamental frequency track - formants - F1-F4 formants - intensity - Intensity track - hnr - Harmonics-to-noise ratio - cog - Center of gravity - spectral_tilt - Spectral tilt - a1_p0 - A1-P0 measure\nMultiple overlays separated by commas.",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#cors-requirements",
    "href": "embedding/basic-usage.html#cors-requirements",
    "title": "Basic Embedding",
    "section": "CORS Requirements",
    "text": "CORS Requirements\nFor the viewer to load audio from your server, the server must send CORS headers:\n\nRequired Headers\nAccess-Control-Allow-Origin: *\nAccess-Control-Allow-Methods: GET\n\n\nApache (.htaccess)\n&lt;IfModule mod_headers.c&gt;\n    Header set Access-Control-Allow-Origin \"*\"\n&lt;/IfModule&gt;\n\n\nNginx\nlocation ~ \\.(wav|mp3|ogg)$ {\n    add_header Access-Control-Allow-Origin *;\n}\n\n\nPython HTTP Server\n# Simple CORS-enabled server\npython3 -m http.server 8000 --bind 0.0.0.0\nNote: Python’s built-in server doesn’t add CORS headers by default. Use a production server for deployment.\n\n\nGitHub Pages\nGitHub Pages automatically enables CORS for all static files — no configuration needed.\n\n\nAmazon S3\nWhen uploading audio files, set: - Public access: Enabled - CORS configuration:\n[\n    {\n        \"AllowedHeaders\": [\"*\"],\n        \"AllowedMethods\": [\"GET\"],\n        \"AllowedOrigins\": [\"*\"],\n        \"ExposeHeaders\": []\n    }\n]",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#responsive-embedding",
    "href": "embedding/basic-usage.html#responsive-embedding",
    "title": "Basic Embedding",
    "section": "Responsive Embedding",
    "text": "Responsive Embedding\nMake the iframe responsive to container width:\n&lt;div style=\"position: relative; width: 100%; padding-bottom: 75%; /* 4:3 aspect ratio */\"&gt;\n  &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=...\"\n          style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%; border: none;\"&gt;\n  &lt;/iframe&gt;\n&lt;/div&gt;\nCommon aspect ratios: - 16:9 (widescreen): padding-bottom: 56.25%; - 4:3 (standard): padding-bottom: 75%; - 3:2: padding-bottom: 66.67%;",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#self-contained-embedding-data-urls",
    "href": "embedding/basic-usage.html#self-contained-embedding-data-urls",
    "title": "Basic Embedding",
    "section": "Self-Contained Embedding (Data URLs)",
    "text": "Self-Contained Embedding (Data URLs)\nFor truly portable embeds without external file dependencies, encode audio as a data URL:\n\nCreating Data URL\nUsing Base64 encoding:\n# Linux/Mac\nbase64 -w 0 audio.wav &gt; audio.base64\n\n# Then construct data URL:\n# data:audio/wav;base64,&lt;paste base64 here&gt;\nIn JavaScript:\n// Convert file to data URL\nasync function fileToDataURL(file) {\n  return new Promise((resolve) =&gt; {\n    const reader = new FileReader();\n    reader.onload = e =&gt; resolve(e.target.result);\n    reader.readAsDataURL(file);\n  });\n}\n\nconst dataURL = await fileToDataURL(audioFile);\n// data:audio/wav;base64,UklGRiQAAABXQVZFZm10...\nIn R:\nlibrary(base64enc)\n\naudio_data &lt;- readBin(\"audio.wav\", \"raw\", file.info(\"audio.wav\")$size)\ndata_url &lt;- paste0(\"data:audio/wav;base64,\", base64encode(audio_data))\n\n\nEmbedding with Data URL\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=data:audio/wav;base64,UklGRiQAAABXQVZFZm10...\"\n        width=\"800\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\nAdvantages: - No external file hosting required - Works offline after initial page load - Self-contained — entire example in one HTML file - No CORS issues\nLimitations: - URL length limits (~2MB practical limit for most browsers) - Not suitable for long audio files - Increases page size\nUse cases: - Short audio examples in documentation - QR code-embeddable demos - Email-friendly examples (single HTML file)",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#helper-scripts",
    "href": "embedding/basic-usage.html#helper-scripts",
    "title": "Basic Embedding",
    "section": "Helper Scripts",
    "text": "Helper Scripts\nOzen-web includes helper scripts for generating embed code:\n\nR Script (create-iframe.R)\nLocated at scripts/create-iframe.R:\nBasic usage:\nsource(\"scripts/create-iframe.R\")\n\n# Generate iframe HTML\nhtml &lt;- create_embedded_viewer(\n  \"data/vowel-example.wav\",\n  overlays = \"pitch,formants,intensity\",\n  height = 600\n)\n\n# Use in R Markdown/Quarto\nhtmltools::HTML(html)\nCommand-line usage:\nRscript scripts/create-iframe.R audio.wav \"pitch,formants\" \"./ozen-web/viewer.html\"\nFeatures: - Automatically calculates relative paths - Handles data URL encoding - Customizable viewer URL (for local builds) - Height can be numeric (pixels) or percentage\nSee Quarto Integration for detailed R Markdown examples.\n\n\nPython Script (create-iframe.py)\nLocated at scripts/create-iframe.py:\nBasic usage:\nfrom create_iframe import create_embedded_viewer\n\n# Generate iframe HTML\nhtml = create_embedded_viewer(\n    audio_path=\"data/vowel-example.wav\",\n    overlays=\"pitch,formants,intensity\",\n    height=600\n)\n\nprint(html)\nCommand-line usage:\npython scripts/create-iframe.py audio.wav --overlays pitch,formants --height 600\nFeatures: - Same functionality as R version - Base64 encoding for data URLs - Relative path calculation - Jupyter notebook compatible",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#local-development",
    "href": "embedding/basic-usage.html#local-development",
    "title": "Basic Embedding",
    "section": "Local Development",
    "text": "Local Development\nWhen embedding in local HTML files:\n\nFile Protocol Limitation\nModern browsers block file:// iframes for security:\n&lt;!-- ❌ Won't work when opened as file:// --&gt;\n&lt;iframe src=\"file:///path/to/viewer.html?audio=file:///audio.wav\"&gt;\n&lt;/iframe&gt;\n\n\nSolution: Local HTTP Server\nServe your HTML files over HTTP:\nPython 3:\npython3 -m http.server 8000\n# Open http://localhost:8000/your-page.html\nNode.js (http-server):\nnpx http-server -p 8000\n# Open http://localhost:8000/your-page.html\nPHP:\nphp -S localhost:8000\nR (servr package):\nservr::httd(port = 8000)\nNow iframes will work properly.",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#embedding-best-practices",
    "href": "embedding/basic-usage.html#embedding-best-practices",
    "title": "Basic Embedding",
    "section": "Embedding Best Practices",
    "text": "Embedding Best Practices\n\n1. Choose Appropriate Dimensions\nDesktop viewers: - Minimum width: 800px - Minimum height: 600px - Recommended: 1000px × 700px or larger\nMobile-optimized: - Width: 100% (responsive) - Height: 500-700px\n\n\n2. Set Meaningful Defaults\nPre-configure overlays to highlight the acoustic features relevant to your example:\n&lt;!-- For pitch analysis example --&gt;\n?overlays=pitch,intensity\n\n&lt;!-- For vowel formants example --&gt;\n?overlays=formants\n\n&lt;!-- For voice quality example --&gt;\n?overlays=pitch,hnr,spectral_tilt\n\n\n3. Provide Context\nAlways explain what the viewer shows:\n&lt;p&gt;Figure 1: Pitch contour for question intonation.\n   Notice the final rise at the end of the utterance.&lt;/p&gt;\n\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=...&overlays=pitch,intensity\"&gt;\n&lt;/iframe&gt;\n\n\n4. Test on Mobile\nIf your page is mobile-accessible, test the embedded viewer on phones:\n\nTouch gestures work correctly\nViewer is responsive\nSettings drawer accessible\nPlay button visible\n\n\n\n5. Consider Loading Time\nFor large audio files (&gt;5 MB): - Warn users about loading time - Consider hosting files on CDN - Use compressed formats (MP3 instead of WAV)",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#troubleshooting",
    "href": "embedding/basic-usage.html#troubleshooting",
    "title": "Basic Embedding",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nViewer Not Loading\nProblem: Blank iframe or error message\nPossible causes: - CORS not enabled on audio server - Invalid audio URL - Mixed content (HTTP audio on HTTPS page)\nSolution: - Check browser console for CORS errors - Test audio URL directly in browser - Ensure both page and audio use HTTPS\n\n\nAudio Not Playing\nProblem: Viewer loads but audio doesn’t play\nPossible causes: - Browser autoplay policy - Invalid audio format - File corrupted\nSolution: - User must click play button (autoplay restricted) - Verify audio file plays in browser directly - Re-encode audio file\n\n\niframe Too Small\nProblem: Viewer UI is cramped or cut off\nSolution: - Increase iframe dimensions - Use responsive embedding with aspect ratio - Minimum 800×600 recommended",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#security-considerations",
    "href": "embedding/basic-usage.html#security-considerations",
    "title": "Basic Embedding",
    "section": "Security Considerations",
    "text": "Security Considerations\n\nContent Security Policy (CSP)\nIf your site uses CSP headers, allow iframes:\nContent-Security-Policy: frame-src https://ucpresearch.github.io\n\n\nData URL Size Limits\nDifferent browsers have different URL length limits:\n\n\n\nBrowser\nMax URL Length\n\n\n\n\nChrome\n~2 MB (practical)\n\n\nFirefox\n~65 KB (strict)\n\n\nSafari\n~2 MB\n\n\nEdge\n~2 MB\n\n\n\nFor cross-browser compatibility, keep data URLs under 64 KB or use external file hosting.",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/basic-usage.html#see-also",
    "href": "embedding/basic-usage.html#see-also",
    "title": "Basic Embedding",
    "section": "See Also",
    "text": "See Also\n\nQuarto Integration - Using in Quarto documents\nURL Parameters - Complete parameter reference\nExamples - Real-world embedding examples\nMobile Viewer - Touch interface details",
    "crumbs": [
      "Embedding",
      "Basic Embedding"
    ]
  },
  {
    "objectID": "embedding/examples.html",
    "href": "embedding/examples.html",
    "title": "Embedding Examples",
    "section": "",
    "text": "This page provides complete, copy-paste ready examples for common embedding scenarios. All examples use the hosted viewer at https://ucpresearch.github.io/ozen-web/viewer.",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#overview",
    "href": "embedding/examples.html#overview",
    "title": "Embedding Examples",
    "section": "",
    "text": "This page provides complete, copy-paste ready examples for common embedding scenarios. All examples use the hosted viewer at https://ucpresearch.github.io/ozen-web/viewer.",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#phonetics-linguistics",
    "href": "embedding/examples.html#phonetics-linguistics",
    "title": "Embedding Examples",
    "section": "Phonetics & Linguistics",
    "text": "Phonetics & Linguistics\n\nExample 1: Vowel Formants\nUse case: Demonstrate F1/F2 formant values for different vowels\n&lt;h3&gt;High Front Vowel /i/ (\"beet\")&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/vowels/i.wav&overlays=formants&maxFreq=7500\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n\n&lt;p&gt;Notice: Low F1 (~300 Hz), high F2 (~2500 Hz)&lt;/p&gt;\n\n&lt;h3&gt;Low Back Vowel /ɑ/ (\"bot\")&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/vowels/a.wav&overlays=formants&maxFreq=7500\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n\n&lt;p&gt;Notice: High F1 (~700 Hz), low F2 (~1100 Hz)&lt;/p&gt;\n\n\nExample 2: Pitch Contours\nUse case: Compare declarative vs. question intonation\n&lt;h3&gt;Declarative: \"He's leaving.\"&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/intonation/statement.wav&overlays=pitch,intensity\"\n        width=\"100%\"\n        height=\"500\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n&lt;p&gt;Falling pitch at end of utterance.&lt;/p&gt;\n\n&lt;h3&gt;Question: \"He's leaving?\"&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/intonation/question.wav&overlays=pitch,intensity\"\n        width=\"100%\"\n        height=\"500\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n&lt;p&gt;Rising pitch at end of utterance.&lt;/p&gt;\n\n\nExample 3: Voice Quality Analysis\nUse case: Demonstrate HNR for voice quality research\n&lt;h2&gt;Voice Quality Comparison&lt;/h2&gt;\n\n&lt;h3&gt;Modal Phonation&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/voice/modal.wav&overlays=pitch,hnr,spectral_tilt\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n&lt;p&gt;High HNR (&gt;15 dB) indicates clear, modal voice quality.&lt;/p&gt;\n\n&lt;h3&gt;Creaky Voice&lt;/h3&gt;\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/voice/creaky.wav&overlays=pitch,hnr,spectral_tilt\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n&lt;p&gt;Low HNR (&lt;10 dB) and irregular pitch indicate vocal fry.&lt;/p&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#teaching-education",
    "href": "embedding/examples.html#teaching-education",
    "title": "Embedding Examples",
    "section": "Teaching & Education",
    "text": "Teaching & Education\n\nExample 4: Interactive Lab Exercise\nUse case: Self-paced phonetics lab with questions\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Phonetics Lab 3: Fricatives&lt;/title&gt;\n  &lt;style&gt;\n    body { font-family: sans-serif; max-width: 900px; margin: 0 auto; padding: 20px; }\n    .exercise { margin: 40px 0; padding: 20px; background: #f5f5f5; border-radius: 8px; }\n    iframe { margin: 20px 0; }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Lab 3: Fricative Acoustics&lt;/h1&gt;\n\n  &lt;div class=\"exercise\"&gt;\n    &lt;h2&gt;Exercise 1: Sibilant Contrast&lt;/h2&gt;\n    &lt;p&gt;Compare the spectral properties of /s/ and /ʃ/:&lt;/p&gt;\n\n    &lt;h3&gt;Alveolar /s/ (as in \"sip\")&lt;/h3&gt;\n    &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/fricatives/s.wav&overlays=cog&maxFreq=10000\"\n            width=\"100%\"\n            height=\"500\"\n            style=\"border: none;\"&gt;\n    &lt;/iframe&gt;\n\n    &lt;h3&gt;Postalveolar /ʃ/ (as in \"ship\")&lt;/h3&gt;\n    &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/fricatives/sh.wav&overlays=cog&maxFreq=10000\"\n            width=\"100%\"\n            height=\"500\"\n            style=\"border: none;\"&gt;\n    &lt;/iframe&gt;\n\n    &lt;h3&gt;Questions:&lt;/h3&gt;\n    &lt;ol&gt;\n      &lt;li&gt;What is the approximate Center of Gravity (CoG) for /s/? For /ʃ/?&lt;/li&gt;\n      &lt;li&gt;Which fricative has higher-frequency energy?&lt;/li&gt;\n      &lt;li&gt;Can you identify the fricative burst vs. following vowel?&lt;/li&gt;\n    &lt;/ol&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nExample 5: Minimal Embedding in Blog Post\nUse case: Simple blog post with one example\n&lt;p&gt;Here's what the /a/ vowel sounds and looks like acoustically:&lt;/p&gt;\n\n&lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://myblog.com/audio/vowel-a.wav&overlays=formants\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;\n\n&lt;p&gt;Notice the low second formant (F2) around 1200 Hz, typical of back vowels.&lt;/p&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#research-presentations",
    "href": "embedding/examples.html#research-presentations",
    "title": "Embedding Examples",
    "section": "Research & Presentations",
    "text": "Research & Presentations\n\nExample 6: Conference Poster (QR Code)\nUse case: Interactive figure accessible via QR code\n&lt;!-- Host this page and generate QR code to it --&gt;\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n  &lt;title&gt;Interactive Figure: Vocal Fry Analysis&lt;/title&gt;\n  &lt;style&gt;\n    body { margin: 0; font-family: sans-serif; }\n    .header { background: #003366; color: white; padding: 20px; text-align: center; }\n    .container { max-width: 1000px; margin: 0 auto; padding: 20px; }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;div class=\"header\"&gt;\n    &lt;h1&gt;Vocal Fry in Final Position&lt;/h1&gt;\n    &lt;p&gt;Smith & Jones (2024) — Interactive Figure 3&lt;/p&gt;\n  &lt;/div&gt;\n\n  &lt;div class=\"container\"&gt;\n    &lt;p&gt;&lt;strong&gt;Example:&lt;/strong&gt; Creaky phonation in utterance-final position&lt;/p&gt;\n\n    &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/research/vocal-fry.wav&overlays=pitch,hnr\"\n            width=\"100%\"\n            height=\"700\"\n            style=\"border: none;\"&gt;\n    &lt;/iframe&gt;\n\n    &lt;p&gt;&lt;strong&gt;Observations:&lt;/strong&gt;&lt;/p&gt;\n    &lt;ul&gt;\n      &lt;li&gt;HNR drops below 10 dB in final syllable (indicates aperiodicity)&lt;/li&gt;\n      &lt;li&gt;F0 decreases and becomes irregular&lt;/li&gt;\n      &lt;li&gt;Typical of American English discourse-final position&lt;/li&gt;\n    &lt;/ul&gt;\n  &lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n\nExample 7: Data URL (Self-Contained)\nUse case: Email-friendly single HTML file\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Example: Self-Contained Audio&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Embedded Example&lt;/h1&gt;\n  &lt;p&gt;This entire example is in one HTML file — no external dependencies!&lt;/p&gt;\n\n  &lt;!-- Short audio file encoded as data URL --&gt;\n  &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACABAAZGF0YQAAAAA=&overlays=pitch\"\n          width=\"100%\"\n          height=\"600\"\n          style=\"border: none;\"&gt;\n  &lt;/iframe&gt;\n\n  &lt;p&gt;Share this HTML file via email, and recipients can view it without any setup.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#language-learning",
    "href": "embedding/examples.html#language-learning",
    "title": "Embedding Examples",
    "section": "Language Learning",
    "text": "Language Learning\n\nExample 8: Pronunciation Practice\nUse case: Show target pronunciation with model audio\n&lt;h2&gt;Lesson 5: French Nasal Vowels&lt;/h2&gt;\n\n&lt;div style=\"display: grid; grid-template-columns: 1fr 1fr; gap: 20px;\"&gt;\n  &lt;div&gt;\n    &lt;h3&gt;Target: /ɑ̃/ as in \"temps\"&lt;/h3&gt;\n    &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/french/nasal-a.wav&overlays=formants,a1_p0&maxFreq=7500\"\n            width=\"100%\"\n            height=\"500\"\n            style=\"border: 1px solid #ccc;\"&gt;\n    &lt;/iframe&gt;\n    &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt; Notice A1-P0 negative values indicating nasal coupling.&lt;/p&gt;\n  &lt;/div&gt;\n\n  &lt;div&gt;\n    &lt;h3&gt;Your recording:&lt;/h3&gt;\n    &lt;p&gt;Record yourself saying \"temps\" and compare the formant patterns and A1-P0 values.&lt;/p&gt;\n    &lt;p&gt;&lt;em&gt;(Users would upload their own recording to compare)&lt;/em&gt;&lt;/p&gt;\n  &lt;/div&gt;\n&lt;/div&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#responsive-multi-device",
    "href": "embedding/examples.html#responsive-multi-device",
    "title": "Embedding Examples",
    "section": "Responsive Multi-Device",
    "text": "Responsive Multi-Device\n\nExample 9: Mobile-Friendly Embedding\nUse case: Works on desktop, tablet, phone\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n  &lt;style&gt;\n    body {\n      font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", sans-serif;\n      margin: 0;\n      padding: 20px;\n      max-width: 1200px;\n      margin: 0 auto;\n    }\n\n    .viewer-container {\n      position: relative;\n      width: 100%;\n      padding-bottom: 75%; /* 4:3 aspect ratio */\n      margin: 20px 0;\n    }\n\n    iframe {\n      position: absolute;\n      top: 0;\n      left: 0;\n      width: 100%;\n      height: 100%;\n      border: 1px solid #ddd;\n      border-radius: 8px;\n    }\n\n    @media (max-width: 768px) {\n      .viewer-container { padding-bottom: 100%; } /* More square on mobile */\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Acoustic Analysis Example&lt;/h1&gt;\n\n  &lt;div class=\"viewer-container\"&gt;\n    &lt;iframe src=\"https://ucpresearch.github.io/ozen-web/viewer?audio=https://example.com/audio.wav&overlays=pitch,formants\"&gt;\n    &lt;/iframe&gt;\n  &lt;/div&gt;\n\n  &lt;p&gt;This viewer adapts to your screen size. Try viewing on phone, tablet, and desktop.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#advanced-javascript-integration",
    "href": "embedding/examples.html#advanced-javascript-integration",
    "title": "Embedding Examples",
    "section": "Advanced JavaScript Integration",
    "text": "Advanced JavaScript Integration\n\nExample 10: Dynamic URL Generation\nUse case: Let users select overlays and generate embed code\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n  &lt;title&gt;Embed Code Generator&lt;/title&gt;\n  &lt;style&gt;\n    body { font-family: sans-serif; max-width: 800px; margin: 40px auto; padding: 20px; }\n    .controls { background: #f5f5f5; padding: 20px; border-radius: 8px; margin: 20px 0; }\n    label { display: block; margin: 10px 0; }\n    input[type=\"text\"] { width: 100%; padding: 8px; }\n    button { padding: 10px 20px; background: #007bff; color: white; border: none; border-radius: 4px; cursor: pointer; }\n    #embedCode { width: 100%; height: 150px; font-family: monospace; font-size: 12px; }\n    #preview { margin: 40px 0; }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n  &lt;h1&gt;Ozen-web Embed Code Generator&lt;/h1&gt;\n\n  &lt;div class=\"controls\"&gt;\n    &lt;label&gt;\n      Audio URL:\n      &lt;input type=\"text\" id=\"audioURL\" value=\"https://example.com/audio.wav\"&gt;\n    &lt;/label&gt;\n\n    &lt;label&gt;\n      &lt;input type=\"checkbox\" id=\"pitch\" checked&gt; Pitch\n    &lt;/label&gt;\n    &lt;label&gt;\n      &lt;input type=\"checkbox\" id=\"formants\"&gt; Formants\n    &lt;/label&gt;\n    &lt;label&gt;\n      &lt;input type=\"checkbox\" id=\"intensity\"&gt; Intensity\n    &lt;/label&gt;\n\n    &lt;button onclick=\"generateEmbed()\"&gt;Generate Embed Code&lt;/button&gt;\n  &lt;/div&gt;\n\n  &lt;h3&gt;Embed Code:&lt;/h3&gt;\n  &lt;textarea id=\"embedCode\" readonly&gt;&lt;/textarea&gt;\n\n  &lt;h3&gt;Preview:&lt;/h3&gt;\n  &lt;div id=\"preview\"&gt;&lt;/div&gt;\n\n  &lt;script&gt;\n    function generateEmbed() {\n      const audioURL = document.getElementById('audioURL').value;\n      const overlays = [];\n\n      if (document.getElementById('pitch').checked) overlays.push('pitch');\n      if (document.getElementById('formants').checked) overlays.push('formants');\n      if (document.getElementById('intensity').checked) overlays.push('intensity');\n\n      const params = new URLSearchParams({\n        audio: audioURL,\n        overlays: overlays.join(',')\n      });\n\n      const viewerURL = `https://ucpresearch.github.io/ozen-web/viewer?${params}`;\n\n      const embedCode = `&lt;iframe src=\"${viewerURL}\"\n        width=\"100%\"\n        height=\"600\"\n        style=\"border: none;\"&gt;\n&lt;/iframe&gt;`;\n\n      document.getElementById('embedCode').value = embedCode;\n      document.getElementById('preview').innerHTML = embedCode;\n    }\n\n    // Generate initial embed\n    generateEmbed();\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#best-practices-summary",
    "href": "embedding/examples.html#best-practices-summary",
    "title": "Embedding Examples",
    "section": "Best Practices Summary",
    "text": "Best Practices Summary\n\nChoose the Right Approach\n\n\n\nScenario\nRecommendation\n\n\n\n\nBlog post, single example\nHosted viewer + remote audio URL\n\n\nResearch paper (HTML)\nHosted viewer + GitHub Pages audio\n\n\nTeaching materials (Quarto)\nHelper script + local audio\n\n\nConference poster\nData URL for self-contained QR code\n\n\nEmail distribution\nData URL in single HTML file\n\n\nOffline access required\nLocal viewer build + local audio\n\n\n\n\n\nOptimize for Performance\n\nUse MP3 for large files (vs. WAV)\nHost audio on CDN for faster loading\nPreload only necessary overlays\nUse appropriate maxFreq (don’t default to 10kHz)\n\n\n\nAccessibility\n\nProvide text alternative describing what audio shows\nInclude captions or transcripts\nEnsure responsive design for mobile viewers\nTest with screen readers (provide ARIA labels)",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  },
  {
    "objectID": "embedding/examples.html#see-also",
    "href": "embedding/examples.html#see-also",
    "title": "Embedding Examples",
    "section": "See Also",
    "text": "See Also\n\nBasic Embedding - Fundamentals\nURL Parameters - Customization reference\nQuarto Integration - Academic publishing\nMobile Viewer - Touch interface",
    "crumbs": [
      "Embedding",
      "Embedding Examples"
    ]
  }
]